{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971},{"sourceId":4747505,"sourceType":"datasetVersion","datasetId":2747345},{"sourceId":7101872,"sourceType":"datasetVersion","datasetId":4069519}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**<h1>WLASL Dataset Sign Language Model**\n\n**<h3> This notebook aims to use MediaPipe landmarks detection as the starting point** \n    \n**<h3> for building an ASL Translation Model**\n    \n**<h3> Sections:**\n* [Importing Libraries](#Importing)\n* [Data Preparation](#Preparation)\n* [MediaPipe Implementation](#MediaPipe)\n* [Visualizing landmarks](#Visualizing)\n* [Data Encoding](#Encoding)\n* [Data Loading](#Loading)\n* [Data Augmentation](#Augmentation)\n* [Data Preprocessing](#Preprocessing)\n* [Models](#Models)","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Importing\">\n    \n**<h2> Importing Libraries** ","metadata":{}},{"cell_type":"code","source":"!pip install -q mediapipe==0.10.7","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-05T02:59:20.464145Z","iopub.execute_input":"2023-12-05T02:59:20.464512Z","iopub.status.idle":"2023-12-05T02:59:37.048732Z","shell.execute_reply.started":"2023-12-05T02:59:20.464479Z","shell.execute_reply":"2023-12-05T02:59:37.047398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport json\nimport time\nimport spacy\nimport shutil\nimport threading\nimport numpy as np\nfrom tqdm import tqdm\nimport mediapipe as mp\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom IPython.display import clear_output, FileLink","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T02:59:37.051041Z","iopub.execute_input":"2023-12-05T02:59:37.051372Z","iopub.status.idle":"2023-12-05T02:59:54.445236Z","shell.execute_reply.started":"2023-12-05T02:59:37.05134Z","shell.execute_reply":"2023-12-05T02:59:54.444251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -q gdown\n\n# # ALL Encoded Lamdmarks V1\n# # !gdown 10NXf2LPIBMlI-zrAPIJU7OIsjeXQA3UG\n\n# # ALL Encoded Lamdmarks V2\n# !gdown 1tsvPlCJW7sCafqCkESFkgwHl59DQaRrc\n\n# # WLASL Parsed JSON (labels)\n# !gdown 1mETzoM47Fo7ggpG-wEYYXOs_rahHfSIK","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:11:31.193785Z","iopub.execute_input":"2023-12-01T23:11:31.194244Z","iopub.status.idle":"2023-12-01T23:12:02.351541Z","shell.execute_reply.started":"2023-12-01T23:11:31.194211Z","shell.execute_reply":"2023-12-01T23:12:02.350319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Preparation\">\n    \n**<h2>Data Preparation   (Done Once)**","metadata":{}},{"cell_type":"markdown","source":"<h3> We first load the data","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/wlasl-processed/WLASL_v0.3.json', 'r') as json_file:\n    all_data = json.load(json_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:40:04.462788Z","iopub.execute_input":"2023-12-01T21:40:04.463388Z","iopub.status.idle":"2023-12-01T21:40:04.715599Z","shell.execute_reply.started":"2023-12-01T21:40:04.463358Z","shell.execute_reply":"2023-12-01T21:40:04.714639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Then we extracting the needed info from the data\n<h4>    \n    \n* gloss : the word being expressed\n    \n* video_path : the path to the video in the datasets\n    \n* start_frame : the frame number where the word starts\n    \n* end_frame : the frame number where each word ends\n   \n* split : the type of data when modeling (train, val, test)","metadata":{}},{"cell_type":"code","source":"video_dir = '/kaggle/input/wlasl-processed/videos'\nbackup_dir = '/kaggle/input/wlasl2000-resized/wlasl-complete/videos'\ndata = [] # formatted data\n\nfor i in tqdm(range(len(all_data)), ncols=100):\n    gloss = all_data[i]['gloss']\n    instances = all_data[i]['instances']\n    for instance in instances:\n        video_id = instance['video_id']\n        if os.path.exists(os.path.join(video_dir, f'{video_id}.mp4')):\n            video_path = os.path.join(video_dir, f'{video_id}.mp4')\n        elif os.path.exists(os.path.join(backup_dir, f'{video_id}.mp4')):\n            video_path = os.path.join(backup_dir, f'{video_id}.mp4')\n        else:\n            continue\n            \n        frame_start = instance['frame_start']\n        frame_end = instance['frame_end']\n        split = instance['split']\n        data.append({\n            'gloss': gloss,\n            'video_path': video_path,\n            'frame_start': frame_start,\n            'frame_end': frame_end,\n            'split': split\n        })","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:40:04.71712Z","iopub.execute_input":"2023-12-01T21:40:04.717996Z","iopub.status.idle":"2023-12-01T21:41:13.368202Z","shell.execute_reply.started":"2023-12-01T21:40:04.717956Z","shell.execute_reply":"2023-12-01T21:41:13.367082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:41:13.371024Z","iopub.execute_input":"2023-12-01T21:41:13.371452Z","iopub.status.idle":"2023-12-01T21:41:13.37917Z","shell.execute_reply.started":"2023-12-01T21:41:13.371414Z","shell.execute_reply":"2023-12-01T21:41:13.378032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> We then save the organized dictionary for future uses","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/WLASL_parsed_data.json', 'w') as json_file:\n    json.dump(data, json_file, indent=4)\n    \nFileLink(r'WLASL_parsed_data.json')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:41:13.380775Z","iopub.execute_input":"2023-12-01T21:41:13.381358Z","iopub.status.idle":"2023-12-01T21:41:13.607781Z","shell.execute_reply.started":"2023-12-01T21:41:13.381322Z","shell.execute_reply":"2023-12-01T21:41:13.60672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"MediaPipe\">\n    \n**<h2>MediaPipe Implementation (Not important when training)**","metadata":{}},{"cell_type":"markdown","source":"<h3> We first choose the landmarks\n<h4>    \n    \n* Hands : we'll keep all **42** of them, as they are the most important part.\n    \n* Pose : **6** landmarks for the upper body excluding the face, as we have dedicated process for it.\n    \n* Face : out of the **478** landmarks, we'll choose **132**, focusing on the lips, eyes, eyebrows, and the outline of the face.\n    \n* This brings the total number of landmarks to **180**, each with coordinates (x, y, z).   ","metadata":{}},{"cell_type":"code","source":"filtered_face = [0, 4, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58,\n                 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105,\n                 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154,\n                 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191,\n                 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291,\n                 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324,\n                 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380,\n                 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409,\n                 415, 454, 466, 468, 473]\n\nfiltered_pose = [11, 12, 13, 14, 15, 16]\n\nFACE_NUM = len(filtered_face)\nPOSE_NUM = len(filtered_pose)\nHAND_NUM = 21 # per hand","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:20:27.303398Z","iopub.execute_input":"2023-12-04T01:20:27.303881Z","iopub.status.idle":"2023-12-04T01:20:27.31363Z","shell.execute_reply.started":"2023-12-04T01:20:27.303855Z","shell.execute_reply":"2023-12-04T01:20:27.312768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Extracting landmarks from a frame","metadata":{}},{"cell_type":"code","source":"hands = mp.solutions.hands.Hands()\npose = mp.solutions.pose.Pose()\nface_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n\ndef get_frame_landmarks(frame):\n    \n    all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM + FACE_NUM, 3))\n    \n    def get_hands(frame):\n        results_hands = hands.process(frame)\n        if results_hands.multi_hand_landmarks:\n            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n                if results_hands.multi_handedness[i].classification[0].index == 0: \n                    all_landmarks[:HAND_NUM, :] = np.array(\n                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark])\n                else:\n                    all_landmarks[HAND_NUM:HAND_NUM * 2, :] = np.array(\n                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark])\n\n    def get_pose(frame):\n        results_pose = pose.process(frame)\n        if results_pose.pose_landmarks:\n            all_landmarks[HAND_NUM * 2:HAND_NUM * 2 + POSE_NUM, :] = np.array(\n                [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])[filtered_pose]\n        \n    def get_face(frame):\n        results_face = face_mesh.process(frame)\n        if results_face.multi_face_landmarks:\n            all_landmarks[HAND_NUM * 2 + POSE_NUM:, :] = np.array(\n                [(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark])[filtered_face]\n        \n    threads = []\n    threads.append(threading.Thread(target=get_hands, args=(frame,)))\n    threads.append(threading.Thread(target=get_pose, args=(frame,)))\n    threads.append(threading.Thread(target=get_face, args=(frame,)))\n    \n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n\n    return all_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:20:27.315335Z","iopub.execute_input":"2023-12-04T01:20:27.316086Z","iopub.status.idle":"2023-12-04T01:20:27.3845Z","shell.execute_reply.started":"2023-12-04T01:20:27.316048Z","shell.execute_reply":"2023-12-04T01:20:27.383527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Extracting landmarks from a video","metadata":{}},{"cell_type":"code","source":"def get_video_landmarks(video_path, start_frame=1, end_frame=-1):\n    cap = cv2.VideoCapture(video_path)\n    \n    # if the starting is 0\n    if start_frame <= 1:\n        start_frame = 1\n        \n    # if the video is precropped\n    elif start_frame > int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n        start_frame = 1\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n    # if the final frame was not given (-1)    \n    if end_frame < 0: \n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    num_landmarks = HAND_NUM * 2 + POSE_NUM + FACE_NUM\n    all_frame_landmarks = np.zeros((end_frame - start_frame + 1, num_landmarks, 3))\n    frame_index = 1\n    \n    while cap.isOpened() and frame_index <= end_frame:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_index >= start_frame:\n            frame.flags.writeable = False\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_landmarks = get_frame_landmarks(frame)\n            all_frame_landmarks[frame_index - start_frame] = frame_landmarks\n\n        frame_index += 1\n\n    cap.release()\n    hands.reset()\n    pose.reset()\n    face_mesh.reset()\n    return all_frame_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:22:01.711268Z","iopub.execute_input":"2023-12-04T01:22:01.712064Z","iopub.status.idle":"2023-12-04T01:22:01.720899Z","shell.execute_reply.started":"2023-12-04T01:22:01.712021Z","shell.execute_reply":"2023-12-04T01:22:01.719966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_landmarks(input_path, output_path, video_landmarks, start_frame=1, end_frame=-1):\n    cap = cv2.VideoCapture(input_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    if start_frame <= 1:\n        start_frame = 1\n    elif start_frame > int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n        start_frame = 1\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if end_frame < 0:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n    frame_index = 1\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_index >= start_frame and frame_index <= end_frame:\n            frame_landmarks = video_landmarks[frame_index - start_frame]\n            landmarks = [(int(x * width), int(y * height)) for x, y, _ in frame_landmarks]\n            for x, y in landmarks:\n                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n            out.write(frame)\n        else:\n            # out.write(frame) # Enable if you want the full video\n            pass\n        frame_index += 1\n\n    cap.release()\n    out.release()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:22:03.339614Z","iopub.execute_input":"2023-12-04T01:22:03.33995Z","iopub.status.idle":"2023-12-04T01:22:03.350058Z","shell.execute_reply.started":"2023-12-04T01:22:03.339924Z","shell.execute_reply":"2023-12-04T01:22:03.349082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Visualizing\">\n    \n**<h2>Visualizing landmarks  (Done Once)**","metadata":{}},{"cell_type":"markdown","source":"<h3>Test on a frame","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom io import BytesIO\n\nimage_url = 'https://images.unsplash.com/photo-1515294898968-a408405d7674'\nresponse = requests.get(image_url)\nimg = Image.open(BytesIO(response.content))\nimg = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(img[:,:,::-1])\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:22:05.89049Z","iopub.execute_input":"2023-12-04T01:22:05.891163Z","iopub.status.idle":"2023-12-04T01:22:09.647672Z","shell.execute_reply.started":"2023-12-04T01:22:05.891127Z","shell.execute_reply":"2023-12-04T01:22:09.646609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height, width, _ = img.shape\n\nframe_landmarks = get_frame_landmarks(img[:,:,::-1])\nfor landmark in frame_landmarks:\n    x = int(landmark[0] * width)\n    y = int(landmark[1] * height)\n    cv2.circle(img, (x, y), 10, (0, 255, 0), -1)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(img[:,:,::-1])\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:22:09.6491Z","iopub.execute_input":"2023-12-04T01:22:09.64946Z","iopub.status.idle":"2023-12-04T01:22:13.158164Z","shell.execute_reply.started":"2023-12-04T01:22:09.649427Z","shell.execute_reply":"2023-12-04T01:22:13.157174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Test on video","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/mutemotion-output/WLASL_parsed_data.json', 'r') as json_file:\n    data = json.load(json_file)\n    \ntest = data[40]\nvideo_landmarks = get_video_landmarks(test['video_path'],test['frame_start'],test['frame_end'])\n\noutput_path = '/kaggle/working/landmarks_test.mp4'\ndraw_landmarks(test['video_path'], output_path, video_landmarks, test['frame_start'],test['frame_end'])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:24:48.400665Z","iopub.execute_input":"2023-12-04T01:24:48.401704Z","iopub.status.idle":"2023-12-04T01:24:54.142409Z","shell.execute_reply.started":"2023-12-04T01:24:48.401668Z","shell.execute_reply":"2023-12-04T01:24:54.141382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.remove('/kaggle/working/landmarks_test.mp4')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:25:47.74273Z","iopub.execute_input":"2023-12-04T01:25:47.743116Z","iopub.status.idle":"2023-12-04T01:25:47.748078Z","shell.execute_reply.started":"2023-12-04T01:25:47.743085Z","shell.execute_reply":"2023-12-04T01:25:47.746875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Encoding\">\n    \n**<h2>Data Encoding   (Done Once)**","metadata":{}},{"cell_type":"code","source":"npy_dir = '/kaggle/working/landmarks'\nos.makedirs(npy_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:41:26.557079Z","iopub.execute_input":"2023-12-01T21:41:26.557393Z","iopub.status.idle":"2023-12-01T21:41:26.561758Z","shell.execute_reply.started":"2023-12-01T21:41:26.557365Z","shell.execute_reply":"2023-12-01T21:41:26.560867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    for i in tqdm(range(len(data)), ncols=100):\n        npy_path = os.path.join(npy_dir, f'{i}.npy')\n        if os.path.exists(npy_path): continue\n        video_path = data[i]['video_path']\n        start = data[i]['frame_start']\n        end = data[i]['frame_end']\n        \n        try:\n            video_landmarks = get_video_landmarks(video_path, start, end)\n            np.save(npy_path, video_landmarks)\n            \n        except Exception as e:\n            print(f\"\\nError encoding {video_path}\\n{e}\")\n            continue   \n        clear_output(wait=True)\n\nexcept KeyboardInterrupt:\n    print(\"\\nLoading process interrupted by user.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:42:09.695984Z","iopub.execute_input":"2023-12-01T21:42:09.696373Z","iopub.status.idle":"2023-12-01T21:42:09.838056Z","shell.execute_reply.started":"2023-12-01T21:42:09.696343Z","shell.execute_reply":"2023-12-01T21:42:09.836974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"landmarks_dict = {}\n\nfor filename in os.listdir(npy_dir):\n    if filename.endswith('.npy'):\n        key = filename.split('.')[0]\n        landmarks = np.load(os.path.join(npy_dir, filename), allow_pickle=True)\n        landmarks_dict[key] = landmarks\n\nnp.savez_compressed('/kaggle/working/landmarks_V2.npz', **landmarks_dict)\n\nFileLink(r'landmarks_V2.npz')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:42:48.16379Z","iopub.execute_input":"2023-12-01T21:42:48.164484Z","iopub.status.idle":"2023-12-01T21:53:35.00202Z","shell.execute_reply.started":"2023-12-01T21:42:48.164449Z","shell.execute_reply":"2023-12-01T21:53:35.000023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shutil.rmtree(npy_dir)\n# os.remove('/kaggle/working/landmarks_V2.npz')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:10:26.778754Z","iopub.execute_input":"2023-12-01T23:10:26.779249Z","iopub.status.idle":"2023-12-01T23:10:27.182227Z","shell.execute_reply.started":"2023-12-01T23:10:26.779219Z","shell.execute_reply":"2023-12-01T23:10:27.181215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Loading\">\n    \n**<h2>Data Loading**","metadata":{}},{"cell_type":"code","source":"landmarks_dict = np.load('/kaggle/input/mutemotion-output/landmarks_V2.npz', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:59:54.446568Z","iopub.execute_input":"2023-12-05T02:59:54.447145Z","iopub.status.idle":"2023-12-05T02:59:54.763899Z","shell.execute_reply.started":"2023-12-05T02:59:54.447115Z","shell.execute_reply":"2023-12-05T02:59:54.762911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/mutemotion-output/WLASL_parsed_data.json', 'r') as json_file:\n    data = json.load(json_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:59:54.765762Z","iopub.execute_input":"2023-12-05T02:59:54.766061Z","iopub.status.idle":"2023-12-05T02:59:54.885641Z","shell.execute_reply.started":"2023-12-05T02:59:54.766035Z","shell.execute_reply":"2023-12-05T02:59:54.884759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(split, labels=None, max_labels=None, max_samples=None):\n    \n    if labels is not None:\n        X = [landmarks_dict[k] for k in landmarks_dict.keys() if data[int(k)]['split'] == split and data[int(k)]['gloss'] in labels]\n        Y = [data[int(k)]['gloss'] for k in landmarks_dict.keys() if data[int(k)]['split'] == split and data[int(k)]['gloss'] in labels]\n    \n    elif max_samples is not None:\n        X = [landmarks_dict[k] for k in landmarks_dict.keys() if data[int(k)]['split'] == split][:max_samples]\n        Y = [data[int(k)]['gloss'] for k in landmarks_dict.keys() if data[int(k)]['split'] == split][:max_samples]\n    \n    elif max_labels is not None:\n        label_counts = {}\n        for k in landmarks_dict.keys():\n            label = data[int(k)]['gloss']\n            label_counts[label] = label_counts.get(label, 0) + 1\n        \n        top_labels = sorted(label_counts, key=label_counts.get, reverse=True)[:max_labels]\n        X = [landmarks_dict[k] for k in landmarks_dict.keys() if data[int(k)]['gloss'] in top_labels and data[int(k)]['split'] == split]\n        Y = [data[int(k)]['gloss'] for k in landmarks_dict.keys() if data[int(k)]['gloss'] in top_labels and data[int(k)]['split'] == split]\n        \n    else:\n        X = [landmarks_dict[k] for k in landmarks_dict.keys() if data[int(k)]['split'] == split]\n        Y = [data[int(k)]['gloss'] for k in landmarks_dict.keys() if data[int(k)]['split'] == split]\n    \n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:59:56.797995Z","iopub.execute_input":"2023-12-05T02:59:56.798377Z","iopub.status.idle":"2023-12-05T02:59:56.811534Z","shell.execute_reply.started":"2023-12-05T02:59:56.798342Z","shell.execute_reply":"2023-12-05T02:59:56.810619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = load_data('train', max_labels=250)\nX_val, Y_val = load_data('val', max_labels=250)\nX_test, Y_test = load_data('test', max_labels=250)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:59:58.115062Z","iopub.execute_input":"2023-12-05T02:59:58.116006Z","iopub.status.idle":"2023-12-05T03:00:42.691724Z","shell.execute_reply.started":"2023-12-05T02:59:58.11597Z","shell.execute_reply":"2023-12-05T03:00:42.690626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train), len(X_val), len(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:14.392575Z","iopub.execute_input":"2023-12-05T03:01:14.393328Z","iopub.status.idle":"2023-12-05T03:01:14.400363Z","shell.execute_reply.started":"2023-12-05T03:01:14.393298Z","shell.execute_reply":"2023-12-05T03:01:14.399463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(np.unique(Y_train)), len(np.unique(Y_val)), len(np.unique(Y_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:17.175113Z","iopub.execute_input":"2023-12-05T03:01:17.175828Z","iopub.status.idle":"2023-12-05T03:01:17.186339Z","shell.execute_reply.started":"2023-12-05T03:01:17.175793Z","shell.execute_reply":"2023-12-05T03:01:17.185438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.all(np.unique(Y_val) in np.unique(Y_train)), np.all(np.unique(Y_test) in np.unique(Y_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:17.926128Z","iopub.execute_input":"2023-12-05T03:01:17.926823Z","iopub.status.idle":"2023-12-05T03:01:17.938362Z","shell.execute_reply.started":"2023-12-05T03:01:17.926789Z","shell.execute_reply":"2023-12-05T03:01:17.93741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free space\ndel landmarks_dict, data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:22.895731Z","iopub.execute_input":"2023-12-05T03:01:22.896384Z","iopub.status.idle":"2023-12-05T03:01:23.191519Z","shell.execute_reply.started":"2023-12-05T03:01:22.896349Z","shell.execute_reply":"2023-12-05T03:01:23.190359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Augmentation\">\n    \n**<h2> Data Augmentation**","metadata":{}},{"cell_type":"markdown","source":"<h3> Rotation Augmentations","metadata":{}},{"cell_type":"code","source":"def rotate(data, rotation_matrix):\n    frames, landmarks, _ = data.shape\n    center = np.array([0.5, 0.5, 0])\n    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n    data = data.reshape(-1, 3)\n    data[non_zero] -= center\n    data[non_zero] = np.dot(data[non_zero], rotation_matrix.T)\n    data[non_zero] += center\n    data = data.reshape(frames, landmarks, 3)\n    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n    data[out_of_range] = 0\n    return data\n\ndef rotate_z(data):\n    angle = np.random.choice([np.random.uniform(-30, -10),\n                              np.random.uniform(10, 30)])\n    theta = np.radians(angle)\n    rotation_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    return rotate(data, rotation_matrix)\n\ndef rotate_y(data):\n    angle = np.random.choice([np.random.uniform(-30, -10),\n                              np.random.uniform(10, 30)])\n    theta = np.radians(angle)\n    rotation_matrix = np.array([\n        [np.cos(theta), 0, np.sin(theta)],\n        [0, 1, 0],\n        [-np.sin(theta), 0, np.cos(theta)]\n    ])\n    return rotate(data, rotation_matrix)\n\ndef rotate_x(data):\n    angle = np.random.choice([np.random.uniform(-30, -10),\n                              np.random.uniform(10, 30)])\n    theta = np.radians(angle)\n    rotation_matrix = np.array([\n        [1, 0, 0],\n        [0, np.cos(theta), -np.sin(theta)],\n        [0, np.sin(theta), np.cos(theta)]\n    ])\n    return rotate(data, rotation_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:30.141251Z","iopub.execute_input":"2023-12-05T03:01:30.141908Z","iopub.status.idle":"2023-12-05T03:01:30.155395Z","shell.execute_reply.started":"2023-12-05T03:01:30.141874Z","shell.execute_reply":"2023-12-05T03:01:30.154483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Other Augmentations","metadata":{}},{"cell_type":"code","source":"def zoom(data):\n    factor = np.random.uniform(0.8, 1.2)\n    center = np.array([0.5, 0.5])\n    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n    data[non_zero[:, 0], non_zero[:, 1], :2] = (\n        (data[non_zero[:, 0], non_zero[:, 1], :2] - center) * factor + center\n    )\n    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n    data[out_of_range] = 0\n    return data\n\ndef shift(data):\n    x_shift = np.random.uniform(-0.2, 0.2)\n    y_shift = np.random.uniform(-0.2, 0.2)\n    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n    data[non_zero[:, 0], non_zero[:, 1], 0] += x_shift\n    data[non_zero[:, 0], non_zero[:, 1], 1] += y_shift\n    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n    data[out_of_range] = 0\n    return data\n\ndef mask(data):\n    frames, landmarks, _ = data.shape\n    num_hands = int(0.3 * 42)\n    num_rest = int(0.6 * (landmarks - 42))\n\n    mask = np.zeros(landmarks, dtype=bool)\n    indices = np.concatenate([\n        np.random.choice(42, num_hands, replace=False),\n        np.random.choice(landmarks - 42, num_rest, replace=False) + 42\n    ])\n    mask[indices] = True\n    data[:, mask] = 0\n    return data\n\ndef hflip(data):\n    data[:, :, 0] = 1 - data[:, :, 0]\n    return data\n\ndef speedup(data):\n    return data[::2]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:33.884569Z","iopub.execute_input":"2023-12-05T03:01:33.884949Z","iopub.status.idle":"2023-12-05T03:01:33.898284Z","shell.execute_reply.started":"2023-12-05T03:01:33.884898Z","shell.execute_reply":"2023-12-05T03:01:33.89743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Apply Augmentations","metadata":{}},{"cell_type":"code","source":"def apply_augmentations(data):\n    aug_functions = [rotate_x, rotate_y, rotate_z, zoom, shift, mask, hflip, speedup]\n    np.random.shuffle(aug_functions)\n    counter = 0\n    for fun in aug_functions:\n        if np.random.rand() < 0.5:\n            data = fun(data)\n            counter += 1\n    \n    if counter == 0:\n        data = apply_augmentations(data)\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:37.509907Z","iopub.execute_input":"2023-12-05T03:01:37.510259Z","iopub.status.idle":"2023-12-05T03:01:37.516233Z","shell.execute_reply.started":"2023-12-05T03:01:37.510228Z","shell.execute_reply":"2023-12-05T03:01:37.515332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augment(X, Y, num=None):\n    X_aug = X.copy()\n    Y_aug = Y.copy()\n    \n    if num == None:\n        for i in tqdm(range(len(Y)), ncols=100):\n            num_aug = np.random.choice([1, 2, 3])\n            for n in range(num_aug):\n                X_aug.append(apply_augmentations(X[i].copy()))\n                Y_aug.append(Y[i])\n    elif num > 0:\n        for i in tqdm(range(len(Y)), ncols=100):\n            for n in range(num):\n                X_aug.append(apply_augmentations(X[i].copy()))\n                Y_aug.append(Y[i])\n\n    return X_aug, Y_aug","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:37.697605Z","iopub.execute_input":"2023-12-05T03:01:37.698175Z","iopub.status.idle":"2023-12-05T03:01:37.705134Z","shell.execute_reply.started":"2023-12-05T03:01:37.698143Z","shell.execute_reply":"2023-12-05T03:01:37.704203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_aug, Y_train_aug = augment(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:01:39.020995Z","iopub.execute_input":"2023-12-05T03:01:39.021793Z","iopub.status.idle":"2023-12-05T03:02:41.803632Z","shell.execute_reply.started":"2023-12-05T03:01:39.021757Z","shell.execute_reply":"2023-12-05T03:02:41.80257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train_aug), len(X_train_aug[0]), len(X_train_aug[0][0]), len(X_train_aug[0][0][0])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:03:19.730359Z","iopub.execute_input":"2023-12-05T03:03:19.730721Z","iopub.status.idle":"2023-12-05T03:03:19.737715Z","shell.execute_reply.started":"2023-12-05T03:03:19.73069Z","shell.execute_reply":"2023-12-05T03:03:19.736784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free space\ndel X_train, Y_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:03:21.197919Z","iopub.execute_input":"2023-12-05T03:03:21.198245Z","iopub.status.idle":"2023-12-05T03:03:21.479613Z","shell.execute_reply.started":"2023-12-05T03:03:21.19822Z","shell.execute_reply":"2023-12-05T03:03:21.478679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Test Augmentation (Not necessary when training)","metadata":{}},{"cell_type":"code","source":"frame_landmarks = np.expand_dims(frame_landmarks, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:33:10.925302Z","iopub.execute_input":"2023-12-04T01:33:10.925687Z","iopub.status.idle":"2023-12-04T01:33:10.930392Z","shell.execute_reply.started":"2023-12-04T01:33:10.925654Z","shell.execute_reply":"2023-12-04T01:33:10.929499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame_landmarks.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:33:11.169164Z","iopub.execute_input":"2023-12-04T01:33:11.169799Z","iopub.status.idle":"2023-12-04T01:33:11.17583Z","shell.execute_reply.started":"2023-12-04T01:33:11.169767Z","shell.execute_reply":"2023-12-04T01:33:11.174894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(12, 6))\nimg = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n\nfor landmark in frame_landmarks[0]:\n    x = int(landmark[0] * img.shape[1])\n    y = int(landmark[1] * img.shape[0])\n    cv2.circle(img, (x, y), 10, (0, 255, 0), -1)\n    \naxs[0].set_title('Original Image')\naxs[0].imshow(img[:, :, ::-1])\naxs[0].axis('off')\n\naugmented_landmarks = apply_augmentations(frame_landmarks.copy())\nimg = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n\nfor landmark in augmented_landmarks[0]:\n    x = int(landmark[0] * img.shape[1])\n    y = int(landmark[1] * img.shape[0])\n    cv2.circle(img, (x, y), 10, (0, 255, 0), -1)\n\naxs[1].set_title('Augmented Image')\naxs[1].imshow(img[:, :, ::-1])\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:33:13.115913Z","iopub.execute_input":"2023-12-04T01:33:13.116755Z","iopub.status.idle":"2023-12-04T01:33:19.145737Z","shell.execute_reply.started":"2023-12-04T01:33:13.116721Z","shell.execute_reply":"2023-12-04T01:33:19.144484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Data Permutation","metadata":{}},{"cell_type":"code","source":"permutation_train = list(range(len(Y_train_aug)))\nnp.random.shuffle(permutation_train)\nX_train_aug = [X_train_aug[i] for i in permutation_train]\nY_train_aug = [Y_train_aug[i] for i in permutation_train]\n\npermutation_val = list(range(len(Y_val)))\nnp.random.shuffle(permutation_val)\nX_val = [X_val[i] for i in permutation_val]\nY_val = [Y_val[i] for i in permutation_val]\n\npermutation_test = list(range(len(Y_test)))\nnp.random.shuffle(permutation_test)\nX_test = [X_test[i] for i in permutation_test]\nY_test = [Y_test[i] for i in permutation_test]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:03:29.603786Z","iopub.execute_input":"2023-12-05T03:03:29.604147Z","iopub.status.idle":"2023-12-05T03:03:29.61562Z","shell.execute_reply.started":"2023-12-05T03:03:29.604118Z","shell.execute_reply":"2023-12-05T03:03:29.614836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Preprocessing\">\n    \n**<h2>Data Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"<h3>\n    \n**Method 1:** Sequencing","metadata":{}},{"cell_type":"code","source":"def sequences(X, Y, length=30, step=1, pad=0):\n    X_sequences = []\n    Y_sequences = []\n\n    for inputs, label in zip(X, Y):\n        num = inputs.shape[0]\n\n        if num < length:\n            padding = length - num\n            inputs = np.pad(\n            inputs, ((0, padding), (0, 0), (0, 0)),\n            mode='constant', constant_values=pad\n            )\n            num = length\n\n        for start in range(0, num - length + 1, step):\n            end = start + length\n            sequence = inputs[start:end]\n            X_sequences.append(sequence)\n            Y_sequences.append(label)\n\n    X_sequences = np.array(X_sequences)\n#     Y_sequences = np.array(Y_sequences)\n    return X_sequences, Y_sequences","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:04:18.993619Z","iopub.execute_input":"2023-12-05T03:04:18.993974Z","iopub.status.idle":"2023-12-05T03:04:19.001392Z","shell.execute_reply.started":"2023-12-05T03:04:18.993946Z","shell.execute_reply":"2023-12-05T03:04:19.000481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_seq, Y_val_seq = sequences(X_val, Y_val, length=60, step=20, pad=-100)\nX_test_seq, Y_test_seq = sequences(X_test, Y_test, length=60, step=20, pad=-100)\nX_train_seq, Y_train_seq = sequences(X_train_aug, Y_train_aug, length=60, step=20, pad=-100)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:04:21.162493Z","iopub.execute_input":"2023-12-05T03:04:21.163463Z","iopub.status.idle":"2023-12-05T03:04:23.936834Z","shell.execute_reply.started":"2023-12-05T03:04:21.163426Z","shell.execute_reply":"2023-12-05T03:04:23.936034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_seq.shape, X_test_seq.shape, X_val_seq.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:04:28.460395Z","iopub.execute_input":"2023-12-05T03:04:28.461106Z","iopub.status.idle":"2023-12-05T03:04:28.46704Z","shell.execute_reply.started":"2023-12-05T03:04:28.461074Z","shell.execute_reply":"2023-12-05T03:04:28.466111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\n\nY_val_seq = np.array([nlp(label).vector for label in Y_val_seq])\nY_test_seq = np.array([nlp(label).vector for label in Y_test_seq])\nY_train_seq = np.array([nlp(label).vector for label in Y_train_seq])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:04:34.830753Z","iopub.execute_input":"2023-12-05T03:04:34.831115Z","iopub.status.idle":"2023-12-05T03:05:50.90484Z","shell.execute_reply.started":"2023-12-05T03:04:34.831086Z","shell.execute_reply":"2023-12-05T03:05:50.903811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train_seq.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:06:03.210613Z","iopub.execute_input":"2023-12-05T03:06:03.211442Z","iopub.status.idle":"2023-12-05T03:06:03.217307Z","shell.execute_reply.started":"2023-12-05T03:06:03.211387Z","shell.execute_reply":"2023-12-05T03:06:03.216352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label_encoder = LabelEncoder()\n\n# Y_val_seq = label_encoder.fit_transform(Y_val_seq)\n# Y_test_seq = label_encoder.fit_transform(Y_test_seq)\n# Y_train_seq = label_encoder.fit_transform(Y_train_seq)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:28:04.988042Z","iopub.execute_input":"2023-12-04T03:28:04.988339Z","iopub.status.idle":"2023-12-04T03:28:05.001573Z","shell.execute_reply.started":"2023-12-04T03:28:04.988313Z","shell.execute_reply":"2023-12-04T03:28:05.000858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free space\ndel X_train_aug, Y_train_aug, X_val, Y_val, X_test, Y_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:06:10.50887Z","iopub.execute_input":"2023-12-05T03:06:10.509234Z","iopub.status.idle":"2023-12-05T03:06:10.889813Z","shell.execute_reply.started":"2023-12-05T03:06:10.509205Z","shell.execute_reply":"2023-12-05T03:06:10.888701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>\n    \n**Method 2:** Padding","metadata":{}},{"cell_type":"code","source":"def padding(X, Y, length=None, pad=0):\n    if length is None:\n        length = max(len(x) for x in X)\n    \n    X_padded = []\n    for x in X:\n        if len(x) > length:\n            X_padded.append(x[:length]) #truncate\n        else:\n            pad_length = length - len(x)\n            X_padded.append(np.pad(\n                x, ((0, pad_length), (0, 0), (0, 0)),\n                mode='constant', constant_values=pad\n            ))\n            \n    X_padded = np.array(X_padded)\n    Y = np.array(Y)\n    return X_padded, Y","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:33:36.355085Z","iopub.execute_input":"2023-12-04T01:33:36.355665Z","iopub.status.idle":"2023-12-04T01:33:36.364706Z","shell.execute_reply.started":"2023-12-04T01:33:36.355621Z","shell.execute_reply":"2023-12-04T01:33:36.363556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_pad, Y_val_pad = padding(X_val, Y_val, length=200, pad=-100)\nX_test_pad, Y_test_pad = padding(X_test, Y_test, length=200, pad=-100)\nX_train_pad, Y_train_pad = padding(X_train_aug, Y_train_aug, length=200, pad=-100)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:35:08.423233Z","iopub.execute_input":"2023-12-04T01:35:08.42401Z","iopub.status.idle":"2023-12-04T01:35:27.516356Z","shell.execute_reply.started":"2023-12-04T01:35:08.423978Z","shell.execute_reply":"2023-12-04T01:35:27.515377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pad.shape, X_test_pad.shape, X_val_pad.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:34:23.452305Z","iopub.execute_input":"2023-12-04T01:34:23.453129Z","iopub.status.idle":"2023-12-04T01:34:23.459658Z","shell.execute_reply.started":"2023-12-04T01:34:23.453096Z","shell.execute_reply":"2023-12-04T01:34:23.458757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\n\nY_val_pad = label_encoder.fit_transform(Y_val_pad)\nY_test_pad = label_encoder.fit_transform(Y_test_pad)\nY_train_pad = label_encoder.fit_transform(Y_train_pad)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:35:36.586118Z","iopub.execute_input":"2023-12-04T01:35:36.58682Z","iopub.status.idle":"2023-12-04T01:35:36.596888Z","shell.execute_reply.started":"2023-12-04T01:35:36.586788Z","shell.execute_reply":"2023-12-04T01:35:36.595884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free space\ndel X_train_aug, Y_train_aug, X_val, Y_val, X_test, Y_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:35:39.874939Z","iopub.execute_input":"2023-12-04T01:35:39.875774Z","iopub.status.idle":"2023-12-04T01:35:40.101044Z","shell.execute_reply.started":"2023-12-04T01:35:39.87574Z","shell.execute_reply":"2023-12-04T01:35:40.100069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Models\">\n    \n**<h2> Models**","metadata":{}},{"cell_type":"markdown","source":"<h2>Hady's Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Kamel's Model","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntf.keras.backend.clear_session()\nphysical_devices = tf.config.list_physical_devices('GPU')\nfor device in physical_devices:\n    tf.config.experimental.set_memory_growth(device, True)\nphysical_devices","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:10:41.746904Z","iopub.execute_input":"2023-12-05T03:10:41.747302Z","iopub.status.idle":"2023-12-05T03:10:42.117113Z","shell.execute_reply.started":"2023-12-05T03:10:41.747271Z","shell.execute_reply":"2023-12-05T03:10:42.116218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Masking(mask_value=-100, input_shape=(60, 180, 3)),\n    tf.keras.layers.Reshape((60, 180 * 3)),\n\n    tf.keras.layers.Conv1D(64, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(3, strides=1, padding='valid', depth_multiplier=1, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.Conv1D(64, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(5, strides=2, padding='valid', depth_multiplier=4, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.MaxPool1D(2, 2),\n\n    tf.keras.layers.Conv1D(256, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(3, strides=1, padding='valid', depth_multiplier=1, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.Conv1D(256, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(5, strides=2, padding='valid', depth_multiplier=4, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.GlobalAvgPool1D(),\n    tf.keras.layers.Dropout(rate=0.5),\n\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(rate=0.5),\n\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(rate=0.5),\n\n    tf.keras.layers.Dense(96)\n])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:10:42.35226Z","iopub.execute_input":"2023-12-05T03:10:42.352934Z","iopub.status.idle":"2023-12-05T03:10:42.756496Z","shell.execute_reply.started":"2023-12-05T03:10:42.352901Z","shell.execute_reply":"2023-12-05T03:10:42.755595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_filepath = '/kaggle/working/Kamel_Checkpoints/'\nos.makedirs(checkpoint_filepath, exist_ok=True)\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(checkpoint_filepath, 'model_{epoch:02d}.h5'),\n    save_weights_only=True,\n    save_best_only=True,\n    monitor='val_accuracy',\n    mode='max',\n)\n\nlr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.01,\n    decay_steps=1000,\n    decay_rate=0.9\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    patience=50,\n    restore_best_weights=True\n)\n\nmodel.compile(\n    loss='mse',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:10:48.548242Z","iopub.execute_input":"2023-12-05T03:10:48.548608Z","iopub.status.idle":"2023-12-05T03:10:48.571594Z","shell.execute_reply.started":"2023-12-05T03:10:48.548576Z","shell.execute_reply":"2023-12-05T03:10:48.570774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = model.fit(\n    X_train_seq, Y_train_seq,\n    validation_data=(X_val_seq, Y_val_seq),\n    epochs=1000,\n    batch_size=128,\n    callbacks=[model_checkpoint, early_stopping]\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-05T03:10:51.076082Z","iopub.execute_input":"2023-12-05T03:10:51.076455Z","iopub.status.idle":"2023-12-05T03:13:31.227621Z","shell.execute_reply.started":"2023-12-05T03:10:51.076411Z","shell.execute_reply":"2023-12-05T03:13:31.226641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epoch = 90\n# model.load_weights(f'/kaggle/working/Kamel_Checkpoints/model_{epoch}.h5')\ntest_loss, test_accuracy = model.evaluate(X_test_seq, Y_test_seq)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:13:59.364892Z","iopub.execute_input":"2023-12-05T03:13:59.365234Z","iopub.status.idle":"2023-12-05T03:13:59.990691Z","shell.execute_reply.started":"2023-12-05T03:13:59.365207Z","shell.execute_reply":"2023-12-05T03:13:59.989811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree('/kaggle/working/Kamel_Checkpoints')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:17:31.610219Z","iopub.execute_input":"2023-12-05T03:17:31.6106Z","iopub.status.idle":"2023-12-05T03:17:31.649087Z","shell.execute_reply.started":"2023-12-05T03:17:31.610572Z","shell.execute_reply":"2023-12-05T03:17:31.648292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = hist.history['loss']\nval_loss = hist.history['val_loss']\ntrain_acc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\n\n# Plotting loss\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_loss, 'g', label='Training loss')\nplt.plot(val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting accuracy\nplt.subplot(1, 2, 2)\nplt.plot(train_acc, 'g', label='Training accuracy')\nplt.plot(val_acc, 'b', label='Validation accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:18:44.230138Z","iopub.execute_input":"2023-12-05T03:18:44.231201Z","iopub.status.idle":"2023-12-05T03:18:44.934279Z","shell.execute_reply.started":"2023-12-05T03:18:44.231165Z","shell.execute_reply":"2023-12-05T03:18:44.933333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\nimg = plt.imread('model_plot.png')\nplt.figure(figsize=(40, 30))\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = '12-5'\nmodel_filepath = '/kaggle/working/Kamel_Models'\nos.makedirs(model_filepath, exist_ok=True)\n\nmodel.save(model_filepath + f'/{name}')\nmodel.save(model_filepath + f'/{name}.h5')\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_filepath + f'/{name}')\nconverter.target_spec.supported_ops = [\n  tf.lite.OpsSet.TFLITE_BUILTINS,\n  tf.lite.OpsSet.SELECT_TF_OPS\n]\ntflite_model = converter.convert()\nwith open(model_filepath + f'/{name}.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:20:45.60396Z","iopub.execute_input":"2023-12-05T03:20:45.604349Z","iopub.status.idle":"2023-12-05T03:20:54.41156Z","shell.execute_reply.started":"2023-12-05T03:20:45.604318Z","shell.execute_reply":"2023-12-05T03:20:54.410632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Samir's Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Nour's Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
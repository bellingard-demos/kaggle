{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons","metadata":{"id":"IFGFeZeCSUIX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y = make_moons(n_samples=250, noise=0.05, random_state=42)","metadata":{"id":"VNiEr_mpSkTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X[:,0],X[:,1], c=y, s=100)\nplt.show()","metadata":{"id":"UVpq7UCcTTA6","outputId":"73885308-3099-45e3-a0f9-637477521661"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Showing Vanishing Gradient Problem occuring","metadata":{"id":"_-6FKL38NrVa"}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(10,activation='sigmoid',input_dim=2))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nold_weights = model.get_weights()[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nmodel.fit(X_train, y_train, epochs = 100)","metadata":{"id":"tzfO3jquTVyY","outputId":"6d5e55f4-adb1-4968-8a21-1004a5f060f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_weights = model.get_weights()[0]\n\nmodel.optimizer.get_config()[\"learning_rate\"]","metadata":{"id":"_0MskkInUhLT","outputId":"9c331a7c-a492-435d-d678-5f720e30c978"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient = (old_weights - new_weights)/ 0.001     #calculating gradeint value\npercent_change = abs(100*(old_weights - new_weights)/ old_weights)    #percent change in gradeint value\ngradient, percent_change, old_weights, new_weights","metadata":{"id":"AF6hx-6WPxd8","outputId":"07ece881-ae0d-4304-ab20-04049c8b3bc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## You can clearly see that loss is not reducing even in 100 epochs, and there is very less percent change of all","metadata":{"id":"a5nZO7AUQuQb"}},{"cell_type":"markdown","source":"# Solution 1: <br>\n# reduce complexity of neural network","metadata":{"id":"yWhzlm3TREz_"}},{"cell_type":"code","source":"X,y = make_moons(n_samples=250, noise=0.05, random_state=42)\nmodel = Sequential()\nmodel.add(Dense(10,activation='sigmoid',input_dim=2))\nmodel.add(Dense(10,activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nold_weights = model.get_weights()[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nmodel.fit(X_train, y_train, epochs = 100)","metadata":{"id":"Av5OspqLWl0r","outputId":"e0720a88-2ca0-411c-f49c-363de58a206c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_weights = model.get_weights()[0]\ngradient = (old_weights - new_weights)/ 0.001     #calculating gradeint value, default learning rate is 0.001\npercent_change = abs(100*(old_weights - new_weights)/ old_weights)    #percent change in gradeint value\ngradient, percent_change, old_weights, new_weights","metadata":{"id":"YJsZpaeoWm-8","outputId":"28395913-aeba-47e1-bd0a-d8b87ce77591"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Here clearly loss is continuously reducing and percent change of weights is significant","metadata":{"id":"8gGAahpmRwd-"}},{"cell_type":"markdown","source":"# Solution 2 <br>\n# Use ReLu","metadata":{"id":"QV1rXLb3SAGn"}},{"cell_type":"code","source":"X,y = make_moons(n_samples=250, noise=0.05, random_state=42)\nmodel = Sequential()\nmodel.add(Dense(10,activation='relu',input_dim=2))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nold_weights = model.get_weights()[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nmodel.fit(X_train, y_train, epochs = 100)","metadata":{"id":"5Uu6DITscBx5","outputId":"90cdf099-1311-483d-8aa2-ebb7404498f7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_weights = model.get_weights()[0]\ngradient = (old_weights - new_weights)/ 0.001     #calculating gradeint value, default learning rate is 0.001\npercent_change = abs(100*(old_weights - new_weights)/ old_weights)    #percent change in gradeint value\ngradient, percent_change, old_weights, new_weights","metadata":{"id":"72YfOy_-cEPj","outputId":"b9db4c98-8fd7-41cb-a10e-c8c2deda4187"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clearly this solution also helps","metadata":{"id":"DVC92EViSZ4z"}},{"cell_type":"markdown","source":"### There are more three solutions\n1. Proper weight weight initialization like Xavier / Glorot\n2. Batch normalization\n3. Use residual networks\nWe will see this later on\n","metadata":{"id":"tO7qupdESg4L"}}]}
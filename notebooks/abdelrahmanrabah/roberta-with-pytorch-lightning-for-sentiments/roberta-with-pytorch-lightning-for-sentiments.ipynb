{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import RobertaModel\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nimport pytorch_lightning as pl\nfrom transformers import RobertaTokenizerFast\nfrom torch.utils.data import DataLoader\nimport random\nimport transformers\nimport os, sys, re, uuid, time, warnings, pandas as pd, numpy as np, sklearn, nltk, logging, functools, time\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer, PorterStemmer\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom string import punctuation\n\nimport warnings\nwarnings.filterwarnings('ignore')\nlogging.basicConfig(level=logging.INFO)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-11T19:10:17.264229Z","iopub.execute_input":"2023-05-11T19:10:17.265066Z","iopub.status.idle":"2023-05-11T19:10:32.072126Z","shell.execute_reply.started":"2023-05-11T19:10:17.26503Z","shell.execute_reply":"2023-05-11T19:10:32.071152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextPreprocessor:\n\n    def __init__(self):\n        pass\n    \n    def decontract(text):\n            text = re.sub(r\"can\\'t\", \"can not\", text)\n            text = re.sub(r\"n\\'t\", \" not\", text)\n            text = re.sub(r\"\\'re\", \" are\", text)\n            text = re.sub(r\"\\'s\", \" is\", text)\n            text = re.sub(r\"\\'d\", \" would\", text)\n            text = re.sub(r\"\\'ll\", \" will\", text)\n            text = re.sub(r\"\\'t\", \" not\", text)\n            text = re.sub(r\"\\'ve\", \" have\", text)\n            text = re.sub(r\"\\'m\", \" am\", text)\n            return text\n    \n    def clean_text(text):\n        \"\"\"\n        ChatGPT\n        \"\"\"\n        # Remove links\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r'www\\S+', '', text)\n        \n        # Remove mentions and hashtags\n        text = re.sub(r'@\\S+', '', text)\n        text = re.sub(r'#\\S+', '', text)\n\n        # Remove emojis\n        text = re.sub(r'[\\U0001f600-\\U0001f650]', '', text)\n        \n        # Remove punctuation and convert to lowercase\n        text = ''.join([c for c in text if c not in punctuation])\n        text = text.lower().strip()\n        \n        # Remove stop words and tokenize\n        # stop_words = set(stopwords.words('english'))\n        # tokens = word_tokenize(text)\n        # tokens = [token for token in tokens if token not in stop_words]\n        \n        # # Join the tokens back into a string\n        # text = ' '.join(tokens)\n        \n        return text\n\n\n    def text_cleaning2(text: str) -> str:\n        \"\"\"\n        Cleans the text.\n        \"\"\"\n        # text_no_emo = re.sub(r'[\\:\\;\\=]\\s*[D\\)\\(\\[\\]\\}\\{@\\|\\\\\\/]', '', text) #remove emo text\n        cleaned_text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", \" \", text).strip().lower()\n        return cleaned_text\n\n    def text_cleaning(text: str) -> str:\n        \"\"\"\n        Cleans the text.\n        \"\"\"\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r'bit.ly/\\S+', '', text)\n        text = text.strip('[link]')\n        # remove users\n        text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        # remove puntuation\n        my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n        text = re.sub('[' + my_punctuation + ']+', ' ', text)\n        # remove number\n        text = re.sub('([0-9]+)', '', text)\n        # remove hashtag\n        text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        return text.lower()\n        \n    def remove_stopwords(text: str) -> str:\n        \"\"\"\n        Removes the stopwords from the text.\n        \"\"\"\n        custom_stopwords = set(stopwords.words('english')) - {'not', 'no', 'never'}\n        return ' '.join([word for word in text.split() if word not in custom_stopwords])\n\n    def stemming(text: str) -> str:\n        \"\"\"\n        Stems the text.\n        \"\"\"\n        tokenized = nltk.word_tokenize(text)\n        stemmer = PorterStemmer()\n        return ' '.join([stemmer.stem(word) for word in tokenized])\n\n\n    def lemmatization(text: str) -> str:\n        \"\"\"\n        Lemmatization is the process of grouping together the inflected forms of a word.\n        Parameters:\n            text: str\n        \"\"\"\n        lemmatizer = WordNetLemmatizer()\n        return ' '.join([lemmatizer.lemmatize(word, 'v') for word in text.split()])\n\n    def preprocess_text(text: str, stem=True) -> str:\n        \"\"\"\n        Preprocesses the text.\n        Parameters:\n            text: the text to preprocess.\n            stem: if True, stems the text.\n            else, lemmatizes the text.\n        \"\"\"\n        if stem:\n            return TextPreprocessor.stemming(TextPreprocessor.remove_stopwords(TextPreprocessor.decontract(TextPreprocessor.clean_text(text))))\n        else:\n            return TextPreprocessor.lemmatization(TextPreprocessor.remove_stopwords(TextPreprocessor.decontract(TextPreprocessor.clean_text(text))))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T19:10:52.416648Z","iopub.execute_input":"2023-05-11T19:10:52.417018Z","iopub.status.idle":"2023-05-11T19:10:52.438187Z","shell.execute_reply.started":"2023-05-11T19:10:52.416988Z","shell.execute_reply":"2023-05-11T19:10:52.437311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    PATH = '/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv'\n    LR = 1e-5\n    MAX_LEN = 64\n    BATCH_SIZE = 64\n    SEED = 42\n    train_ratio = 0.9\n    test_ratio = 0.1\n    num_workers = 8\n    roberta_model = \"roberta-base\"\n    tokenizer = RobertaTokenizerFast.from_pretrained(roberta_model)\n\ndef log_execution(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        logging.info(f\"Executing {func.__name__}\")\n        result = func(*args, **kwargs)\n        logging.info(f\"Finished executing {func.__name__}\")\n        return result\n    return wrapper\n\ndef timing_decorator(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(f\"Function {func.__name__} took {end_time - start_time} seconds to run.\")\n        return result\n    return wrapper\n\ndef seed_everything(seed=Config.SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Device:', device)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T19:11:29.41802Z","iopub.execute_input":"2023-05-11T19:11:29.418448Z","iopub.status.idle":"2023-05-11T19:11:29.841088Z","shell.execute_reply.started":"2023-05-11T19:11:29.418411Z","shell.execute_reply":"2023-05-11T19:11:29.840056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Sentiment140Dataset(Dataset):\n    def __init__(self):\n        self.path = Config.PATH\n        self.tokenizer = Config.tokenizer\n        self.cleaned = False\n        self.load_data()\n        \n    @log_execution    \n    def load_data(self):\n        \"\"\"\n        Loads the data.\n        \"\"\"\n        self.data = pd.read_csv(self.path, header=None, names=['targets', 'ids', 'date', 'flag', 'user', 'text'], \n                           encoding='latin-1')\n        self.data.targets = self.data.targets.replace({4: 1})\n        self.check_for_dups()\n        # self.check_targets()\n        # self.X, self.y = self.data.text, self.data.targets #Series\n        # self.X, self.y = self.data.text.to_numpy(), self.data.targets.to_numpy().astype(np.uint8) #numpy\n        self.X, self.y = self.data.text.tolist(), self.data.targets.tolist() #List\n\n    @timing_decorator\n    def deep_clean(self):\n        # List: 370.0727105140686 seconds to run.\n        # Series: 372.0254681110382 seconds to run.\n        # Numpy: 371.67559838294983 seconds to run.\n        # For list\n        # Add stop words removal\n        self.X =  list(map(TextPreprocessor.preprocess_text, self.X))\n        # self.X =  list(map(TextPreprocessor.clean_text, self.X))\n        # self.X =  list(map(TextPreprocessor.remove_stopwords, self.X))\n        # self.X =  list(map(TextPreprocessor.stemming, self.X))\n        # For Numpy\n        # self.X = np.vectorize(TextPreprocessor.decontract)(self.X)\n        # self.X = np.vectorize(TextPreprocessor.clean_text)(self.X)\n        # self.X = np.vectorize(TextPreprocessor.stemming)(self.X)\n        # # For Series\n        # self.X = self.X.apply(TextPreprocessor.decontract)\n        # self.X = self.X.apply(TextPreprocessor.clean_text)\n        # self.X = self.X.apply(TextPreprocessor.stemming)\n\n    def apply_cleaning(self):\n        if not self.cleaned:  # check if data has been cleaned\n            self.deep_clean()\n            print(\"Done cleaning data\")\n            self.cleaned = True\n    \n    def find_max_len(self):\n        self.max_len = self.data['text'].str.len().max()\n        print(\"Maximum Length: \",self.max_len)\n        \n    def check_targets(self):\n        print(\"Target value counts:\", self.data.targets.value_counts())\n\n    def check_for_dups(self):\n        # print('number of duplicates: ', self.data.text.duplicated().sum())\n        if self.data.text.duplicated().sum() > 0:\n            self.data.drop_duplicates('text', inplace=True)\n            # print(\"Done removing duplicates\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        self.apply_cleaning()\n        X, y = self.X[i], self.y[i]\n        encoding = self.tokenizer.encode_plus(\n            X,\n            add_special_tokens = True,\n            max_length=Config.MAX_LEN,\n            pad_to_max_length=True,\n            truncation='longest_first',\n            # truncation=True,\n            # padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        input_ids = encoding[\"input_ids\"][0] #[0]\n        attention_mask = encoding[\"attention_mask\"][0] #[0]\n        labels =  torch.tensor(y, dtype=torch.float)\n        return {'text': X,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'labels': labels\n                }","metadata":{"execution":{"iopub.status.busy":"2023-05-11T19:11:30.851911Z","iopub.execute_input":"2023-05-11T19:11:30.852629Z","iopub.status.idle":"2023-05-11T19:11:30.867159Z","shell.execute_reply.started":"2023-05-11T19:11:30.852587Z","shell.execute_reply":"2023-05-11T19:11:30.866051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaModel(pl.LightningModule):\n    def __init__(self)-> None:\n        super().__init__()\n        self.prepare_loaders()\n        self.roberta = RobertaModel.from_pretrained(Config.roberta_model)\n        self.dropout = nn.Dropout(p=0.2)\n        self.classifier = nn.Linear(self.roberta.config.hidden_size, 1)\n        self.loss_fn = nn.BCEWithLogitsLoss()\n\n    def prepare_loaders(self):\n        ds = Sentiment140Dataset()\n        seed_everything()\n        dataset_size = len(ds)\n        indices = list(range(dataset_size))\n        split = int(np.floor(Config.test_ratio * dataset_size))\n        seed_everything()\n        np.random.shuffle(indices)\n        train_indices, test_indices = indices[split:], indices[:split]\n\n        # create samplers for train and test sets\n        train_sampler = SubsetRandomSampler(train_indices)\n        test_sampler = SubsetRandomSampler(test_indices)\n\n        # create data loaders for train and test sets\n        self.train_loader = DataLoader(ds, batch_size=Config.BATCH_SIZE, sampler=train_sampler)\n        self.val_loader = DataLoader(ds, batch_size=Config.BATCH_SIZE, sampler=test_sampler)\n\n    def train_dataloader(self):\n        return self.train_loader\n\n    def val_dataloader(self):\n        # self.prepare_data()\n        return self.val_loader\n\n    def forward(self, input_ids, attention_mask)-> torch.Tensor:\n        output = self.roberta(input_ids=input_ids,\n                              attention_mask=attention_mask)\n        pooled_output = output.pooler_output\n        # dropout_output = self.dropout(pooled_output)\n        return self.classifier(pooled_output)\n    \n    def accuracy(self, preds, labels):\n        \"\"\"\n        Computes accuracy for binary classification task.\n        \"\"\"\n        # round predictions to the closest integer\n        rounded_preds = torch.round(torch.sigmoid(preds))\n        # compute accuracy\n        acc = (rounded_preds == labels).float().mean()\n        return acc\n\n    def training_step(self, batch, batch_idx):\n        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n        outputs = self(input_ids, attention_mask)\n        loss = self.loss_fn(outputs.view(-1), labels.view(-1))\n        acc = self.accuracy(outputs.view(-1), labels.view(-1))\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log('train_acc', acc, prog_bar=True)\n        return {\"loss\": loss, \n                \"acc\": acc}\n        \n    def validation_step(self, batch, batch_idx):\n        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n        outputs = self(input_ids, attention_mask)\n        loss = self.loss_fn(outputs.view(-1), labels.view(-1))\n        acc = self.accuracy(outputs.view(-1), labels.view(-1))\n        self.log(\"valid_loss\", loss)\n        self.log('valid_acc', acc, prog_bar=True)\n        return {\"loss\": loss, \n                \"acc\": acc}\n\n    def configure_optimizers(self):\n        # optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        # return optimizer\n    \n        param_optimizer = list(self.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        return transformers.AdamW(optimizer_parameters, lr=Config.LR)\n\n    def predict(self, text):\n        encoded_text = Config.tokenizer.encode_plus(\n            text,\n            add_special_tokens = True,\n            max_length=Config.MAX_LEN,\n            pad_to_max_length=True,\n            truncation='longest_first',\n            # truncation=True,\n            # padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        output = self(encoded_text['input_ids'][0], encoded_text['attention_mask'][0])\n        probabilities = torch.softmax(output.logits, dim=1)\n        predicted_label = torch.argmax(probabilities, dim=1)\n        return predicted_label.item()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T19:11:32.025972Z","iopub.execute_input":"2023-05-11T19:11:32.026459Z","iopub.status.idle":"2023-05-11T19:11:32.473357Z","shell.execute_reply.started":"2023-05-11T19:11:32.026421Z","shell.execute_reply":"2023-05-11T19:11:32.47235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nearly_stop_callback = EarlyStopping(\n   monitor='val_loss',\n   min_delta=0.00,\n   patience=2,\n   verbose=False,\n   mode='min'\n)\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor='valid_loss',\n    dirpath='checkpoints',\n    filename='model-{epoch:02d}-{val_loss:.2f}',\n    save_top_k=3,\n    mode='min',\n)\n\ntorch.set_float32_matmul_precision('medium')\n# training_args = pl.TrainingArguments(\n#     ,\n#     output_dir='results_roberta',          # output directory\n#     overwrite_output_dir = True,\n#     evaluation_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     load_best_model_at_end=True\n# )\n\nmodel = RoBERTaModel()\ntrainer = pl.Trainer(accelerator='gpu',\n                     max_epochs = 1,\n                     callbacks=[checkpoint_callback]\n)\ntrainer.fit(model)\nbest_model_path = checkpoint_callback.best_model_path\nbest_model = model.load_from_checkpoint(best_model_path)\ntorch.save(best_model.state_dict(), 'best_model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_model = model.load_from_checkpoint('checkpoints/model-epoch=00-val_loss=0.00.ckpt')\n\ndef replace_sentiment(pred):\n    if pred == 0:\n        return \"Negative\"\n    elif pred == 1:\n        return \"Positive\"\n    else:\n        return \"Unknown\"\n\ndef predict_single_sample(model, text):\n    encoded_text = Config.tokenizer.encode_plus(\n        text,\n        add_special_tokens = True,\n        max_length=Config.MAX_LEN,\n        pad_to_max_length=True,\n        truncation='longest_first',\n        return_tensors=\"pt\",\n    )\n    input_ids = encoded_text[\"input_ids\"] #[0]\n    attention_mask = encoded_text[\"attention_mask\"] #[0]\n    model.eval()\n    with torch.no_grad():\n        print(\"Entered no Grad\")\n        output = model(input_ids.to('cuda'), attention_mask.to('cuda'))\n        print(\"Passed output\")\n        pred = torch.argmax(output).item()\n        sentiment = replace_sentiment(pred)\n    return sentiment\n\nsentiment = predict_single_sample(pred_model, \"I dont hate you\")\nprint(sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
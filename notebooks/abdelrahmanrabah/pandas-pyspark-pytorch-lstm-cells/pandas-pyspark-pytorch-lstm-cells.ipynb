{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import Tokenizer,  CountVectorizer, IDF\nimport os, sys, re, uuid, time, nltk, pandas as pd, numpy as np, sklearn\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-10T14:09:34.814061Z","iopub.execute_input":"2023-03-10T14:09:34.814823Z","iopub.status.idle":"2023-03-10T14:09:34.828761Z","shell.execute_reply.started":"2023-03-10T14:09:34.814783Z","shell.execute_reply":"2023-03-10T14:09:34.827636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gc import collect;\nfrom IPython.display import clear_output;\nimport nltk;\ndler = nltk.downloader.Downloader();\ndler._update_index();\nnltk.download('wordnet');\n\nclear_output();\nfor i in range(3): collect(i);","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:11:53.852854Z","iopub.execute_input":"2023-03-10T14:11:53.853805Z","iopub.status.idle":"2023-03-10T14:11:54.275261Z","shell.execute_reply.started":"2023-03-10T14:11:53.853759Z","shell.execute_reply":"2023-03-10T14:11:54.274223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:06:39.921802Z","iopub.execute_input":"2023-03-10T14:06:39.922469Z","iopub.status.idle":"2023-03-10T14:07:22.386986Z","shell.execute_reply.started":"2023-03-10T14:06:39.922431Z","shell.execute_reply":"2023-03-10T14:07:22.385445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    TRAIN_SIZE = 0.7\n    MAX_NB_WORDS = 100000\n    MAX_SEQUENCE_LENGTH = 25\n    PATH = \"/kaggle/input/preprocessed-sentiment/preprocessed.csv\"\n    NUM_EPOCHS = 10\n    HIDDEN_SIZE = 128\n    OUTPUT_SIZE = 1\n    BATCH_SIZE = 64    \n    LR = 0.001\n    LSTM_LAYERS = 5\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def METRIC(outputs, labels):\n        \"\"\"\n        Binary classification Accuracy Metric.\n        \"\"\"\n        return (outputs.argmax(1) == labels).type(torch.float).sum().item()\n\nclass TextPreprocessor:\n\n    def __init__(self):\n        pass\n\n    def text_cleaning2(text: str) -> str:\n        text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n        return re.sub(text_cleaning_re, \" \", text.lower()).strip()\n\n    def text_cleaning(text: str) -> str:\n        \"\"\"\n        Cleans the text.\n        \"\"\"\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r'bit.ly/\\S+', '', text)\n        text = text.strip('[link]')\n        # remove users\n        text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        # remove puntuation\n        my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n        text = re.sub('[' + my_punctuation + ']+', ' ', text)\n        # remove number\n        text = re.sub('([0-9]+)', '', text)\n        # remove hashtag\n        text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        return text.lower()\n        \n    def remove_stopwords(text: str) -> str:\n        \"\"\"\n        Removes the stopwords from the text.\n        \"\"\"\n        STOP_WORDS = stopwords.words('english')\n        return ' '.join([word for word in text.split() if word not in STOP_WORDS])\n\n    def stemming(text: str) -> str:\n        \"\"\"\n        Stems the text.\n        \"\"\"\n        stemmer = SnowballStemmer(\"english\")\n        return ' '.join([stemmer.stem(word) for word in text.split()])\n\n    def lemmatization(text: str) -> str:\n        \"\"\"\n        Lemmatization is the process of grouping together the inflected forms of a word.\n        Parameters:\n            text: str\n        \"\"\"\n        lemmatizer = WordNetLemmatizer()\n        return ' '.join([lemmatizer.lemmatize(word, 'v') for word in text.split()])\n\n    def tokenize(df: DataFrame, i_p: StringType) -> DataFrame:\n        \"\"\"\n        Tokenizes the text.\n        Parameters:\n            df: DataFrame\n        \"\"\"\n        token = Tokenizer(inputCol=i_p, outputCol=\"words\")\n        return token.transform(df)\n\n    def preprocess_text(text: str, stem=True) -> str:\n        \"\"\"\n        Preprocesses the text.\n        Parameters:\n            text: the text to preprocess.\n            stem: if True, stems the text.\n            else, lemmatizes the text.\n        \"\"\"\n        if stem:\n            return TextPreprocessor.stemming(TextPreprocessor.remove_stopwords(TextPreprocessor.text_cleaning2(text)))\n        else:\n            return TextPreprocessor.lemmatization(TextPreprocessor.remove_stopwords(TextPreprocessor.text_cleaning2(text)))","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:09:36.624645Z","iopub.execute_input":"2023-03-10T14:09:36.625102Z","iopub.status.idle":"2023-03-10T14:09:36.640357Z","shell.execute_reply.started":"2023-03-10T14:09:36.625064Z","shell.execute_reply":"2023-03-10T14:09:36.639229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessing:\n    \"\"\"\n    This class was created to preprocess the Pandas dataframe\n    Instead of PySpark because PySpark dataframe when converted to Pandas\n    Dataframe, there were missing values.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the preprocessing class.\n        \"\"\"\n        self.path = Config.PATH\n        self.len = Config.MAX_SEQUENCE_LENGTH\n        self.max_words = Config.MAX_NB_WORDS\n        self.train_size = Config.TRAIN_SIZE\n        self.df = self.load_data()\n\n    def load_data(self):\n        \"\"\"\n        Loads the data.\n        \"\"\"\n        return pd.read_csv(self.path,encoding='latin-1')\n\n\n    def preprocesss(self):\n        \"\"\"\n        Preprocesses the data.\n        \"\"\"\n        self.df['cleaned'] = self.df.text.apply(TextPreprocessor.preprocess_text, args=(False,)) #stem = False\n        self.target = self.df.y.to_numpy()\n        self.X = self.df['cleaned'].to_numpy()\n        #self.df.drop(['text', 'y', 'cleaned_text'], axis=1, inplace=True)\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.target, \n                                                                                test_size=1-self.train_size, \n                                                                                random_state=42)\n        print(\"Done preprocessing.\")\n\n    def prepare_tokens(self):\n        \"\"\"\n        Prepares the tokens.\n        \"\"\"\n        self.tokenizer = Tokenizer(oov_token='UNK')\n        self.tokenizer.fit_on_texts(self.X_train)\n\n    def get_tokenizer(self):\n        \"\"\"\n        Returns the tokenizer.\n        \"\"\"\n        return self.tokenizer\n    \n    def sequence_to_token(self):\n        \"\"\"\n        Converts the text to sequences.\n        \"\"\"\n        self.X_train = self.tokenizer.texts_to_sequences(self.X_train)\n        self.X_test = self.tokenizer.texts_to_sequences(self.X_test)\n        \n    def padding(self):\n        \"\"\"\n        Pads the sequences.\n        \"\"\"\n        self.X_train = pad_sequences(self.X_train, maxlen=self.len, padding='post')\n        self.X_test = pad_sequences(self.X_test, maxlen=self.len, padding='post')\n\n    def adjust_outputs(self):\n        \"\"\"\n        Adjusts the outputs.\n        \"\"\"\n        self.y_train = self.y_train.reshape(-1, 1)\n        self.y_test = self.y_test.reshape(-1, 1)\n\n    def text2seq(self):\n        \"\"\"\n        Converts the text to sequences.\n        \"\"\"\n        self.preprocesss()\n        self.prepare_tokens()\n        self.sequence_to_token()\n        self.padding()\n        self.adjust_outputs()\n        print(\"Done text2seq.\")\n\n    def get_data(self):\n        \"\"\"\n        Returns the data.\n        \"\"\"\n        return self.X_train, self.X_test, self.y_train, self.y_test","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:09:38.845126Z","iopub.execute_input":"2023-03-10T14:09:38.845548Z","iopub.status.idle":"2023-03-10T14:09:38.859153Z","shell.execute_reply.started":"2023-03-10T14:09:38.845515Z","shell.execute_reply":"2023-03-10T14:09:38.857859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\n!python3 -m nltk.downloader wordnet","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:11:00.991625Z","iopub.execute_input":"2023-03-10T14:11:00.992114Z","iopub.status.idle":"2023-03-10T14:11:03.143795Z","shell.execute_reply.started":"2023-03-10T14:11:00.992067Z","shell.execute_reply":"2023-03-10T14:11:03.142555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\ndef load_data(path, schema, CSV=True, provide_header=False):\n    \"\"\"\n    Loads the data from the given path.\n    \"\"\"\n    if CSV:\n        if provide_header:\n            df = sc.read.format(\"csv\")\\\n                .option(\"header\", \"false\")\\\n                .schema(schema)\\\n                .load(path)\n        else:\n            df = sc.read.csv(path, header=True, inferSchema=True)\n    else:\n        df = sc.read.json(path)\n    df.persist()\n    return df\n\ndef show_df(df: DataFrame)-> DataFrame:\n    \"\"\"\n    Shows the dataframe.\n    \"\"\"\n    return df.show()\n\ndef describe_data(df):\n    \"\"\"\n    Prints the dataframe's statistics.\n    \"\"\"\n    return(df.describe().show())\n\ndef check_nan(df: DataFrame) -> DataFrame:\n    \"\"\"\n    Checks for NaN values in the dataframe.\n    \"\"\"\n    return(df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show())\n\ndef check_nunique(df: DataFrame)-> DataFrame:\n    \"\"\"\n    Checks the number of unique values in the dataframe.\n    \"\"\"\n    return(df.select([count(when(c.isNotNull(), c)).alias(c) for c in df.columns]).show())\n\ndef drop_columns(df: DataFrame, cols: list) -> DataFrame:\n    \"\"\"\n    Drops the columns that are not needed.\n    \"\"\"\n    return df.drop(*cols)\n\ndef map_label(y):\n    \"\"\"\n    Maps the label to nicer presentation\n    \"\"\"\n    return 0 if y == 0 else 1 if y == 4 else 2\n\ndef split(df: DataFrame, train_size: float, valid_size:float, test_size:float) -> DataFrame:\n    \"\"\"\n    Splits the dataframe into train and test sets.\n    \"\"\"\n    assert (train_size + valid_size + test_size) == 1.0, \"The sum of train, valid and test sizes must be 1.0\"\n    return df.randomSplit([train_size, valid_size, test_size], seed=42)\n\ndef Preprocessing(df: DataFrame, stem: bool=True) -> DataFrame:\n    \"\"\"\n    Preprocesses the dataframe.\n    \"\"\"\n    STOP_WORDS = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n    stemmer = SnowballStemmer(\"english\")\n    #text = df.select(\"text\")\n    def text_cleaning2(text: str) -> str:\n        text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n        return re.sub(text_cleaning_re, \" \", text.lower()).strip()\n\n\n    def text_cleaning(text: str) -> str:\n        \"\"\"\n        Cleans the text.\n        \"\"\"\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r'bit.ly/\\S+', '', text)\n        text = text.strip('[link]')\n\n        # remove users\n        text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n\n        # remove puntuation\n        my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n        text = re.sub('[' + my_punctuation + ']+', ' ', text)\n\n        # remove number\n        text = re.sub('([0-9]+)', '', text)\n\n        # remove hashtag\n        text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n        return text.lower()\n        \n    def remove_stopwords(text: str) -> str:\n        \"\"\"\n        Removes the stopwords from the text.\n        \"\"\"\n        return ' '.join([word for word in text.split() if word not in STOP_WORDS])\n\n    def stemming(text: str) -> str:\n        \"\"\"\n        Stems the text.\n        \"\"\"\n        return ' '.join([stemmer.stem(word) for word in text.split()])\n\n    def lemmatization(text: str) -> str:\n        \"\"\"\n        Lemmatization is the process of grouping together the inflected forms of a word.\n        Parameters:\n            text: str\n        \"\"\"\n        return ' '.join([lemmatizer.lemmatize(word, 'v') for word in text.split()])\n\n    def tokenize(df: DataFrame, i_p: StringType) -> DataFrame:\n        \"\"\"\n        Tokenizes the text.\n        Parameters:\n            df: DataFrame\n        \"\"\"\n        token = Tokenizer(inputCol=i_p, outputCol=\"words\")\n        return token.transform(df)\n\n    def preprocess_text(text: str, stem: bool=False) -> str:\n        \"\"\"\n        Preprocesses the text.\n        Parameters:\n            text: the text to preprocess.\n            stem: if True, stems the text.\n        \"\"\"\n        if stem:\n            statement = stemming(remove_stopwords(text_cleaning2(text)))\n            return statement\n        else:\n            statement = lemmatization(remove_stopwords(text_cleaning2(text)))\n            return statement\n\n    cleaned_text = udf(lambda x: preprocess_text(x), StringType())\n    df = df.withColumn(\"cleaned_text\", cleaned_text(\"text\"))\n    return df\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nPATH = \"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\"\nprint(\"Starting the program...\")\nbeginning = time.time()\nsc = SparkSession.builder.master(\"local[*]\").appName(\"Sentiment Analysis\").getOrCreate()\nschema = StructType([\\\n    StructField(\"y\", IntegerType(), True),\\\n    StructField(\"ids\", IntegerType(), True),\\\n    StructField(\"date\", StringType(), True),\\\n    StructField(\"flag\", StringType(), True),\\\n    StructField(\"user\", StringType(), True),\\\n    StructField(\"text\", StringType(), True)])\ny_schema = StructType([\\\n    StructField(\"y\", IntegerType(), True)])\nDROPPED_COLS = ['ids', 'date', 'flag', 'user']\ndf = load_data(PATH, schema, provide_header=True)\ndf = drop_columns(df, DROPPED_COLS)\ndf = Preprocessing(df, True)\nlabel = udf(lambda x: map_label(x), IntegerType())\ndf = df.withColumn(\"y\", label(\"y\"))\nfinal_df = df.toPandas()\nfinal_df.to_csv(\"preprocessed.csv\", index=False)\nprint(\"The program has finished. Preprocessing took {} seconds\".format(time.time() - beginning))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, input_size):\n        \"\"\"\n        Initializes the classifier's parameters..\n        \"\"\"\n        super().__init__()\n        self.input_size = input_size #vocab_size\n        self.hidden_dim = Config.HIDDEN_SIZE\n        self.output_size = Config.OUTPUT_SIZE\n        self.LSTM_layers = Config.LSTM_LAYERS\n        self.dropout = nn.Dropout(0.5)\n        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=128)\n        self.fc2 = nn.Linear(128, self.output_size)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass.\n        \"\"\"\n        h0 = torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim, device=x.device).float()\n        c0 = torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim, device=x.device).float()\n        # h0 = torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim).float()\n        # c0 = torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim).float()\n        out = self.embedding(x)\n        out, _ = self.lstm(out, (h0,c0))\n        out = torch.relu_(self.fc1(out[:,-1,:]))\n        out = torch.sigmoid(self.fc2(out))       \n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:09:49.444255Z","iopub.execute_input":"2023-03-10T14:09:49.44464Z","iopub.status.idle":"2023-03-10T14:09:49.458018Z","shell.execute_reply.started":"2023-03-10T14:09:49.444603Z","shell.execute_reply":"2023-03-10T14:09:49.457017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Executing:\n    \"\"\"\n    The Execution Class\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the executing class.\n        \"\"\"\n        self.batch_size = Config.BATCH_SIZE\n        self.epochs = Config.NUM_EPOCHS\n        self.lr = Config.LR\n        self.metric = Config.METRIC\n\n    def on_epoch_start(self, epoch):\n        print(f'Epoch {epoch+1}/{self.epochs}')\n\n    def prepare_batches(self):\n        \"\"\"\n        Prepares the batches.\n        \"\"\"\n        self.X_train, self.X_test, self.y_train, self.y_test = self.df.get_data()\n        self.X_train = torch.tensor(self.X_train, dtype=torch.long)\n        self.y_train = torch.tensor(self.y_train, dtype=torch.float32)\n        self.X_test = torch.tensor(self.X_test, dtype=torch.long)\n        self.y_test = torch.tensor(self.y_test, dtype=torch.float32)\n        self.train_dataset = torch.utils.data.TensorDataset(self.X_train, self.y_train)\n        self.test_dataset = torch.utils.data.TensorDataset(self.X_test, self.y_test)\n        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size)\n        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, shuffle=True)\n\n    def prepare_data(self):\n        \"\"\"\n        Prepares the data.\n        \"\"\"\n        start = time.time()\n        self.df = Preprocessing()\n        self.df.text2seq()\n        tokenizer = self.df.get_tokenizer()\n        self.input_size = len(tokenizer.word_index) + 1\n        self.model = Classifier(self.input_size)\n        self.prepare_batches()\n        print(\"Done preparing data, done in {:.2f} seconds\".format(time.time() - start))\n\n    def fit(self):\n        \"\"\"\n        Trains the model.\n        \"\"\"\n        self.prepare_data()\n        self.model.to(Config.DEVICE)\n        criterion = nn.BCELoss()\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        self.history = []\n        for epoch in range(0, self.epochs):\n            start = time.time()    \n            self.model.train()\n            self.on_epoch_start(epoch)\n            train_loss, train_acc, = 0, 0\n            val_loss, val_acc = 0, 0\n            for x, y in self.train_loader:\n                trainSteps = len(self.train_loader.dataset) // self.batch_size\n                x = x.to(Config.DEVICE)\n                y = y.to(Config.DEVICE)\n                y_pred = self.model(x)\n                loss = criterion(y_pred, y)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                train_loss += loss\n                train_acc += self.metric(y_pred, y)\n                with torch.no_grad():\n                    self.model.eval()\n                    for x_test, y_test in self.test_loader:\n                        testSteps = len(self.test_loader.dataset) // self.batch_size\n                        x_test = x_test.to(Config.DEVICE)\n                        y_test = y_test.to(Config.DEVICE)\n                        pred_test = self.model(x_test)\n                        val_acc += self.metric(pred_test, y_test)\n                        val_loss += criterion(pred_test, y_test)\n            valCorrect = val_acc / len(self.test_loader.dataset)\n            avgValLoss = val_loss / testSteps\n            avgTrainLoss = train_loss / trainSteps\n            trainCorrect = train_acc / len(self.train_loader.dataset)\n            his = {'train_loss': avgTrainLoss, 'train_accuracy': trainCorrect, 'val_loss': avgValLoss, 'val_accuracy': valCorrect}\n            self.history.append(his)\n            self.on_epoch_end(his)\n            print(f'Epoch {epoch+1} done in {time.time() - start:.2f} seconds')\n            print(\"-\"*10)\n\n    def on_epoch_end(self, logs):\n        \"\"\"\n        Prints the logs.\n        \"\"\"\n        print(f'train_loss: {logs[\"train_loss\"]:.2f}, train_accuracy: {logs[\"train_accuracy\"]:.2f}')\n        print(f'val_loss: {logs[\"val_loss\"]:.2f}, val_accuracy: {logs[\"val_accuracy\"]:.2f}')\n        \n        \n    def get_history(self):\n        \"\"\"\n        Returns the history.\n        \"\"\"\n        return self.history\n\n    def get_model(self):\n        \"\"\"\n        Returns the model.\n        \"\"\"\n        return self.model","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:11:58.666006Z","iopub.execute_input":"2023-03-10T14:11:58.666385Z","iopub.status.idle":"2023-03-10T14:11:58.685289Z","shell.execute_reply.started":"2023-03-10T14:11:58.666351Z","shell.execute_reply":"2023-03-10T14:11:58.684197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('all')\nprint(\"Start training process.\")\nstart = time.time()\nexecute = Executing()\nexecute.fit()\nend = time.time()\nprint(f'Time taken for training: {end-start}.')","metadata":{"execution":{"iopub.status.busy":"2023-03-10T14:12:29.348518Z","iopub.execute_input":"2023-03-10T14:12:29.349587Z","iopub.status.idle":"2023-03-10T14:12:51.339565Z","shell.execute_reply.started":"2023-03-10T14:12:29.349545Z","shell.execute_reply":"2023-03-10T14:12:51.33766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
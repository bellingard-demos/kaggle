{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport re,os\nimport xgboost as xgb\nfrom scipy.stats import skew\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-05T11:55:02.291788Z","iopub.execute_input":"2023-09-05T11:55:02.292182Z","iopub.status.idle":"2023-09-05T11:55:02.298002Z","shell.execute_reply.started":"2023-09-05T11:55:02.292156Z","shell.execute_reply":"2023-09-05T11:55:02.297045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:55:02.526628Z","iopub.execute_input":"2023-09-05T11:55:02.526987Z","iopub.status.idle":"2023-09-05T11:55:02.55528Z","shell.execute_reply.started":"2023-09-05T11:55:02.526961Z","shell.execute_reply":"2023-09-05T11:55:02.554013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of Training Examples = {}'.format(train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(test.shape[0]))\nprint('Training X Shape = {}'.format(train.shape))\nprint('Training y Shape = {}\\n'.format(train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(test.shape))\nprint('Test y Shape = {}\\n'.format(test.shape[0]))\nprint(train.columns)\nprint(test.columns)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:55:02.848004Z","iopub.execute_input":"2023-09-05T11:55:02.848378Z","iopub.status.idle":"2023-09-05T11:55:02.858606Z","shell.execute_reply.started":"2023-09-05T11:55:02.848343Z","shell.execute_reply":"2023-09-05T11:55:02.857146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(15)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:55:03.490017Z","iopub.execute_input":"2023-09-05T11:55:03.49042Z","iopub.status.idle":"2023-09-05T11:55:03.510788Z","shell.execute_reply.started":"2023-09-05T11:55:03.490389Z","shell.execute_reply":"2023-09-05T11:55:03.509268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ProbabilityEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.probabilities = None\n\n    def fit(self, X, y=None):\n        total_count = X.value_counts().sum()\n        self.probabilities = (X.value_counts() / total_count).to_dict()\n        return self\n\n    def transform(self, X):\n        X_encoded = X.map(self.probabilities)\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:55:04.312936Z","iopub.execute_input":"2023-09-05T11:55:04.313789Z","iopub.status.idle":"2023-09-05T11:55:04.321465Z","shell.execute_reply.started":"2023-09-05T11:55:04.313757Z","shell.execute_reply":"2023-09-05T11:55:04.320219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nPassengerId = test['PassengerId']\n\ndef extract_titles(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ndef clean_and_combine_titles(title):\n    # Define a dictionary to map similar titles to a common title\n    title_mapping = {\n        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master', 'Ms': 'Miss', 'Mme': 'Mrs',\n        'Mlle': 'Miss', 'Dr': 'Noble', 'Rev': 'Noble', 'Don': 'Noble', 'Major': 'Noble', 'Lady': 'Noble',\n        'Sir': 'Noble', 'Col': 'Noble', 'Capt': 'Noble', 'Countess': 'Noble', 'Jonkheer': 'Noble', 'Dona': 'Noble'\n    }\n#     title2 = title_mapping.get(title, 'Other')\n#     title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Noble\": 4}\n    return title_mapping.get(title, 'Other')\n\ndef impute_age(row):\n    if np.isnan(row['Age']):\n        return title_means[row['title']]\n    else:\n        return row['Age']\n    \n\ndata_dict = {\"train\": train, \"test\": test}\nfor key in data_dict.keys():\n    data_dict[key]['title'] = data_dict[key]['Name'].apply(extract_titles)\n    data_dict[key]['title'] = data_dict[key]['title'].apply(clean_and_combine_titles)\n    data_dict[key]['Embarked'] = data_dict[key]['Embarked'].fillna(\"Other\")\n\n    if key == \"train\":\n        pe_title = ProbabilityEncoder()\n        pe_embarked = ProbabilityEncoder()\n        pe_sex = ProbabilityEncoder()\n        data_dict[key]['title'] = pe_title.fit_transform(data_dict[key]['title'])\n        data_dict[key]['Embarked'] = pe_embarked.fit_transform(data_dict[key]['Embarked'])\n#         data_dict[key]['Sex'] = pe_sex.fit_transform(data_dict[key]['Sex'])\n    else:\n        data_dict[key]['title'] = pe_title.transform(data_dict[key]['title'])\n        data_dict[key]['Embarked'] = pe_embarked.transform(data_dict[key]['Embarked'])\n#         data_dict[key]['Sex'] = pe_sex.transform(data_dict[key]['Sex'])\n\n    data_dict[key]['Has_Cabin'] = data_dict[key][\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n    data_dict[key]['FamilySize'] = data_dict[key]['SibSp'] + data_dict[key]['Parch'] + 1\n    data_dict[key]['IsAlone'] = 0\n    data_dict[key].loc[data_dict[key]['FamilySize'] == 1, 'IsAlone'] = 1\n    data_dict[key]['Sex'] = data_dict[key]['Sex'].map({'female': 0, 'male': 1}).astype(int)\n    \n\n\nfull_data = pd.concat([train, test])\ntitle_means = full_data.groupby('title')['Age'].mean()\nfull_data['Age'] = full_data.apply(impute_age, axis=1)\nfull_data['Fare'] = full_data['Fare'].fillna(train['Fare'].median())\n\n# bin_edges = [0, 12, 19, 39, 59, 100]\n# bin_numerical_codes = [0, 1, 2, 3, 4]\n\n# # Create a new column 'AgeGroupCode' based on age bins\n# # full_data['AgeGroupCode'] = pd.cut(full_data['Age'], bins=bin_edges, labels=bin_numerical_codes, include_lowest=True, right=False).astype(int)\n# full_data['AgeGroupCode'] = pd.qcut(full_data['Age'], q=10, labels=False)\n\n# # Use qcut to create fare groups\n# full_data['FareGroupCode'] = pd.qcut(full_data['Fare'], q=10, labels=False)\n\nfull_data.loc[ full_data['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\nfull_data.loc[(full_data['Fare'] > 7.91) & (full_data['Fare'] <= 14.454), 'Fare'] = 1\nfull_data.loc[(full_data['Fare'] > 14.454) & (full_data['Fare'] <= 31), 'Fare']   = 2\nfull_data.loc[ full_data['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\nfull_data['Fare'] = full_data['Fare'].astype(int)\n\n# Mapping Age\nfull_data.loc[ full_data['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\nfull_data.loc[(full_data['Age'] > 16) & (full_data['Age'] <= 32), 'Age'] = 1\nfull_data.loc[(full_data['Age'] > 32) & (full_data['Age'] <= 48), 'Age'] = 2\nfull_data.loc[(full_data['Age'] > 48) & (full_data['Age'] <= 64), 'Age'] = 3\nfull_data.loc[ full_data['Age'] > 64, 'Age'] = 4 ;\n\nfull_data = full_data.drop(['Cabin', 'SibSp', 'Parch', 'Ticket', 'Name'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:09:38.498233Z","iopub.execute_input":"2023-09-05T12:09:38.498583Z","iopub.status.idle":"2023-09-05T12:09:38.571379Z","shell.execute_reply.started":"2023-09-05T12:09:38.498561Z","shell.execute_reply":"2023-09-05T12:09:38.570111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:09:39.246171Z","iopub.execute_input":"2023-09-05T12:09:39.246549Z","iopub.status.idle":"2023-09-05T12:09:39.264889Z","shell.execute_reply.started":"2023-09-05T12:09:39.246494Z","shell.execute_reply":"2023-09-05T12:09:39.263622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = full_data[full_data['PassengerId'].isin(PassengerId)]\ntest_df.drop([\"Survived\"], axis=1, inplace=True)\nX = full_data[~full_data['PassengerId'].isin(PassengerId)]\ny = X['Survived']\nfeatures = [col for col in X.columns if col != \"PassengerId\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:09:44.033395Z","iopub.execute_input":"2023-09-05T12:09:44.033741Z","iopub.status.idle":"2023-09-05T12:09:44.043436Z","shell.execute_reply.started":"2023-09-05T12:09:44.033713Z","shell.execute_reply":"2023-09-05T12:09:44.042559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = X[features].corr()\n\n# Create a heatmap to visualize the correlations\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\nplt.title('Correlation Matrix for X')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:09:44.471914Z","iopub.execute_input":"2023-09-05T12:09:44.472288Z","iopub.status.idle":"2023-09-05T12:09:44.946981Z","shell.execute_reply.started":"2023-09-05T12:09:44.47226Z","shell.execute_reply":"2023-09-05T12:09:44.94522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the figure and axes for subplots\nnum_cols = len(X.columns)\nnum_rows = (num_cols + 1) // 2  # Ensure enough rows for all columns\nfig, axes = plt.subplots(nrows=num_rows, ncols=2, figsize=(14, 6 * num_rows))\n\n# Iterate through all columns\nfor i, col in enumerate(X.columns):\n    row_idx = i // 2\n    col_idx = i % 2\n    ax = axes[row_idx, col_idx]\n\n    # Create a histogram for the current column\n    sns.histplot(data=X, x=col, bins=50, kde=True, ax=ax)\n    ax.set_xlabel(col)\n    ax.set_ylabel(\"Value\")\n    ax.set_title(f\"Histogram of {col}\")\n\n    # Calculate skewness for the current column\n    skewness = skew(X[col])\n    print(f\"Skewness {col}:\", skewness)\n\n# Adjust layout and spacing between subplots\nplt.tight_layout()\n\n# Show the plots\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:09:45.115809Z","iopub.execute_input":"2023-09-05T12:09:45.116168Z","iopub.status.idle":"2023-09-05T12:09:48.479571Z","shell.execute_reply.started":"2023-09-05T12:09:45.116142Z","shell.execute_reply":"2023-09-05T12:09:48.478442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.drop(columns=[\"Survived\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:09:48.486375Z","iopub.execute_input":"2023-09-05T12:09:48.486777Z","iopub.status.idle":"2023-09-05T12:09:48.493939Z","shell.execute_reply.started":"2023-09-05T12:09:48.48675Z","shell.execute_reply":"2023-09-05T12:09:48.492797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_params = {\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"n_estimators\": 1000,\n    \"seed\": 42\n}\n\nxgb_params = {\n    \"verbosity\": 0,\n    \"booster\": \"gbtree\",\n    \"eta\": 0.05,\n    \"max_depth\": None,  # Equivalent to no limit in XGBoost\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"n_estimators\": 1000,\n    \"seed\": 42\n}\n\nfeatures = [col for col in X.columns if col != \"PassengerId\"]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:10:05.148535Z","iopub.execute_input":"2023-09-05T12:10:05.148897Z","iopub.status.idle":"2023-09-05T12:10:05.155032Z","shell.execute_reply.started":"2023-09-05T12:10:05.14887Z","shell.execute_reply":"2023-09-05T12:10:05.154057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[features]","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:10:08.081884Z","iopub.execute_input":"2023-09-05T12:10:08.082568Z","iopub.status.idle":"2023-09-05T12:10:08.100962Z","shell.execute_reply.started":"2023-09-05T12:10:08.082533Z","shell.execute_reply":"2023-09-05T12:10:08.099714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X[features], y, test_size=0.1, random_state=42)\n\n# Create a LightGBM classifier\nclf = lgb.LGBMClassifier(**lgbm_params)\n\n# Fit the classifier on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = clf.predict(X_test)\n\n# Evaluate the model's accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n\nlgb.plot_importance(clf, height=0.5, figsize=(8, 6))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:10:12.699094Z","iopub.execute_input":"2023-09-05T12:10:12.699469Z","iopub.status.idle":"2023-09-05T12:10:18.884973Z","shell.execute_reply.started":"2023-09-05T12:10:12.699442Z","shell.execute_reply":"2023-09-05T12:10:18.883721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X[features], y, test_size=0.1, random_state=42)\n\n# Create an XGBoost classifier\nclf = xgb.XGBClassifier()\n\n# Fit the classifier on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = clf.predict(X_test)\n\n# Evaluate the model's accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\nxgb.plot_importance(clf, height=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:10:20.65096Z","iopub.execute_input":"2023-09-05T12:10:20.651321Z","iopub.status.idle":"2023-09-05T12:10:20.939791Z","shell.execute_reply.started":"2023-09-05T12:10:20.651295Z","shell.execute_reply":"2023-09-05T12:10:20.938656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Survived'] = clf.predict(test_df[features]).astype(int)\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': test_df.Survived})\noutput.to_csv(\"my_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:10:36.706754Z","iopub.execute_input":"2023-09-05T12:10:36.707195Z","iopub.status.idle":"2023-09-05T12:10:36.722337Z","shell.execute_reply.started":"2023-09-05T12:10:36.707166Z","shell.execute_reply":"2023-09-05T12:10:36.720837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:10:38.591625Z","iopub.execute_input":"2023-09-05T12:10:38.591986Z","iopub.status.idle":"2023-09-05T12:10:38.6058Z","shell.execute_reply.started":"2023-09-05T12:10:38.591959Z","shell.execute_reply":"2023-09-05T12:10:38.604106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}
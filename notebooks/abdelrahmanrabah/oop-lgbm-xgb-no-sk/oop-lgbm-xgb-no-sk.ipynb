{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, f1_score,roc_curve, precision_score, recall_score,roc_auc_score\nfrom sklearn import linear_model, tree, ensemble\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.simplefilter(action=\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-08T19:20:30.212127Z","iopub.execute_input":"2023-04-08T19:20:30.213306Z","iopub.status.idle":"2023-04-08T19:20:30.220935Z","shell.execute_reply.started":"2023-04-08T19:20:30.213242Z","shell.execute_reply":"2023-04-08T19:20:30.219285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e12/train.csv', index_col=[0])\ndf_test = pd.read_csv('/kaggle/input/playground-series-s3e12/test.csv', index_col=[0])\noriginal = pd.read_csv('/kaggle/input/kidney-stone-prediction-based-on-urine-analysis/kindey stone urine analysis.csv')\ntrain['is_generated'] = 1\ndf_test['is_generated'] = 1\noriginal['is_generated'] = 0\noriginal = original.reset_index()\noriginal['id'] = original['index'] + df_test.index[-1] + 1\noriginal = original.drop(columns = ['index']).set_index('id')\ndf_train = pd.concat([train, original])","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:20:40.907687Z","iopub.execute_input":"2023-04-08T19:20:40.908067Z","iopub.status.idle":"2023-04-08T19:20:40.969307Z","shell.execute_reply.started":"2023-04-08T19:20:40.908033Z","shell.execute_reply":"2023-04-08T19:20:40.968111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessing:\n    def __init__(self, df, is_test=False):\n        self.df = df\n        self.is_test = is_test\n        \n    def shape(self):\n        print(f'shape: {self.df.shape}')\n    \n    def dtypes(self, pr=False):\n        print(\"Types\")\n        if pr:\n            print(self.df.dtypes)\n                \n    def isNaN(self, pr=False):\n        if pr:\n            print(\"Contain NaN\")\n            print(self.df.isnull().sum())\n        else:\n            return self.df.columns[self.df.isna().any()].tolist()\n    \n    def isObject(self):\n        return [column for column in self.df.columns if self.df[column].dtype == 'object']\n        \n    def check_dataframe(self):\n        self.shape()\n        self.dtypes(True)\n        self.isNaN(True)\n        \n    # Thanks to https://www.kaggle.com/code/lusfernandotorres/s03e12-stacking-tuned-models\n    # Also: https://www.kaggle.com/code/tetsutani/ps3e12-eda-ensemble-baseline#Pre-Processing\n    def feature_engineering(self):\n        print(\"Feature Engineering\")\n        self.df['pH_cat'] = pd.cut(self.df['ph'], bins=[0, 6, 8, 14], labels=['Acid' , 'Normal', 'Base'])\n#         self.df['osmo-to-urea-ratio'] = self.df['osmo']/self.df['urea']\n#         self.df['osmo-to-cond-diff'] = self.df['osmo']-self.df['cond']\n#         self.df['calc-to-ph-ratio'] = self.df['calc']/self.df['osmo']\n#         self.df['osmo-to-urea-diff'] = self.df['osmo']-self.df['urea']\n        self.df[\"ion_product\"] = self.df[\"calc\"] * self.df[\"urea\"]\n        self.df[\"calcium_to_urea_ratio\"] = self.df[\"calc\"] / self.df[\"urea\"]\n        self.df[\"electrolyte_balance\"] = self.df[\"cond\"] / (10 ** (-self.df[\"ph\"]))\n        self.df[\"osmolality_to_sg_ratio\"] = self.df[\"osmo\"] / self.df[\"gravity\"]\n        self.df['osmo_density'] = self.df['osmo'] * self.df['gravity']\n        \n    def get_df(self):\n        self.feature_engineering()\n        return self.df\n    \n    def split_target(self):\n        print(\"Split Target\")\n        if not self.is_test:\n            self.feature_engineering()\n            self.X = self.df.drop('target', axis=1)\n            self.y = self.df['target'].astype(int).to_numpy()\n        else:\n            self.feature_engineering()\n            self.X = self.df\n#     def get_X(self):\n#         self.split_target()\n#         return self.X\n\n    def find_enc_method(self):\n        print(\"Find Encoding Method\")\n        self.split_target()\n        one_hot_cols = [column for column in self.X.columns if self.X[column].dtype == 'category' or self.X[column].dtype == 'object']\n        return one_hot_cols\n    \n    def encoding(self):\n        print(\"Encoding\")\n        one_hot_cols = self.find_enc_method()\n        num_cols = [col for col in self.X.columns if col not in one_hot_cols]\n        print(one_hot_cols, num_cols)\n        X_OHE, X_NUM = self.X[one_hot_cols].copy(), self.X[num_cols].copy()\n        self.OHE = OneHotEncoder(drop='first', handle_unknown='error')\n        X_OHE = self.OHE.fit_transform(X_OHE).toarray()\n        return X_OHE, X_NUM.to_numpy()\n\n    def scaling(self):\n        print(\"Scaling\")\n        X_OHE, X_num = self.encoding()\n        self.SS = StandardScaler()\n        X_num = self.SS.fit_transform(X_num)\n        self.X_total = np.concatenate((X_OHE, X_num), axis=1)\n        \n    def get_encoders(self):\n        return self.OHE\n        \n    def get_scaler(self):\n        return self.SS\n      \n    def get_Xy(self):\n        if not self.is_test:\n            self.scaling()\n            return self.X_total, self.y\n        else:\n            self.split_target()\n            return self.X\n        \n\npre_test = Preprocessing(df_test, True)\nX_test = pre_test.get_Xy()\nprint('-'*50)\npre_train = Preprocessing(df_train)\nX, y = pre_train.get_Xy()\nSS = pre_train.get_scaler()\nOHE = pre_train.get_encoders()","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:42:43.426294Z","iopub.execute_input":"2023-04-08T19:42:43.426884Z","iopub.status.idle":"2023-04-08T19:42:43.483122Z","shell.execute_reply.started":"2023-04-08T19:42:43.426827Z","shell.execute_reply":"2023-04-08T19:42:43.481749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_cols = [column for column in X_test.columns if X_test[column].dtype == 'category' or X_test[column].dtype == 'object']\nnum_cols = [col for col in X_test.columns if col not in one_hot_cols]\nX_OHE, X_NUM = X_test[one_hot_cols].copy(), X_test[num_cols].copy()\nX_OHE = OHE.transform(X_OHE).toarray()\nX_NUM = SS.transform(X_NUM)\nX_test = np.concatenate((X_OHE, X_NUM), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:42:44.577246Z","iopub.execute_input":"2023-04-08T19:42:44.578254Z","iopub.status.idle":"2023-04-08T19:42:44.591347Z","shell.execute_reply.started":"2023-04-08T19:42:44.578204Z","shell.execute_reply":"2023-04-08T19:42:44.589767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(y, return_counts=True)\nunique, counts","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:42:45.533687Z","iopub.execute_input":"2023-04-08T19:42:45.534234Z","iopub.status.idle":"2023-04-08T19:42:45.545107Z","shell.execute_reply.started":"2023-04-08T19:42:45.534178Z","shell.execute_reply":"2023-04-08T19:42:45.543536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\ndef apply_smote(X, y):\n    \"\"\"\n    Applies SMOTE to the input features (X) and target variable (y) to balance the dataset.\n    \n    Parameters:\n    X: numpy array or pandas DataFrame with the input features\n    y: numpy array or pandas Series with the target variable\n    random_state: int, default=None, controls the randomness of the SMOTE algorithm\n    \n    Returns:\n    X_resampled: numpy array with the resampled input features\n    y_resampled: numpy array with the resampled target variable\n    \"\"\"\n    smote = SMOTE(random_state=42)\n    X_resampled, y_resampled = smote.fit_resample(X, y)\n    return X_resampled, y_resampled\n\nX_resampled, y_resampled = apply_smote(X, y)\nprint(X_resampled.shape)\nprint(y_resampled.shape)\nunique, counts = np.unique(y_resampled, return_counts=True)\nunique, counts","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:42:46.588866Z","iopub.execute_input":"2023-04-08T19:42:46.589284Z","iopub.status.idle":"2023-04-08T19:42:46.61543Z","shell.execute_reply.started":"2023-04-08T19:42:46.589248Z","shell.execute_reply":"2023-04-08T19:42:46.613374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom lightgbm import LGBMClassifier\ndef objective_lgbm(trial):\n    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n    param = {\n        'random_state': 42,\n        'n_estimators': trial.suggest_categorical(\"n_estimators\", [150, 200, 300, 3000]),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.001, 0.006,0.008,0.01,0.014,0.017,0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n    }\n    model = LGBMClassifier(**param)\n    model.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_test, y_test)], verbose=False)    \n    preds = model.predict_proba(X_test)[:, 1]\n    auc_score = roc_auc_score(y_test, preds) \n\n    return auc_score\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective_lgbm, n_trials=50)\n# params_lgbm = study.best_trial.params\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', params_lgbm)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:42:47.655033Z","iopub.execute_input":"2023-04-08T19:42:47.655439Z","iopub.status.idle":"2023-04-08T19:42:47.666853Z","shell.execute_reply.started":"2023-04-08T19:42:47.655401Z","shell.execute_reply":"2023-04-08T19:42:47.665967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With higher number of features, performance of LGBM Decreases\nparams = {'n_estimators': 150, 'reg_alpha': 0.0020019026674570717, 'reg_lambda': 0.018189202721399202, \n          'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.008, \n          'max_depth': 10, 'num_leaves': 776, 'min_child_samples': 1, 'min_data_per_groups': 86}\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\nlgbm_model = LGBMClassifier(**params)\nlgbm_model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_valid, y_valid)], verbose=False)   \ny_pred = lgbm_model.predict_proba(X_valid)[:, 1]\ny_test = lgbm_model.predict_proba(X_test)[:, 1]\nprint(f\"Auc: {round(roc_auc_score(y_valid,y_pred), 2)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:45:56.981652Z","iopub.execute_input":"2023-04-08T19:45:56.982071Z","iopub.status.idle":"2023-04-08T19:46:00.436407Z","shell.execute_reply.started":"2023-04-08T19:45:56.982034Z","shell.execute_reply":"2023-04-08T19:46:00.433927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndef cross_validate_model(model, X_test, n_splits=10,):\n    KF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    aucs = []\n    y_test = []\n    oof_preds = []\n    test_preds = []\n    for index, (train_index, val_index) in enumerate(KF.split(X_resampled, y_resampled)):\n        print(f\"Fold {index+1} out of {n_splits}\")\n        start = time.time()\n        X_train, X_val = X_resampled[train_index], X_resampled[val_index]\n        y_train, y_val = y_resampled[train_index], y_resampled[val_index]\n        model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_val, y_val)], verbose=False)  \n        y_pred = model.predict_proba(X_val)[:, 1]\n        y_test = model.predict_proba(X_test)[:, 1]\n        oof_preds.append(y_pred)\n        test_preds.append(y_test)\n        auc = roc_auc_score(y_val, y_pred)\n        print(f\"Auc: {round(roc_auc_score(y_val, y_pred), 2)}\")\n        \n        aucs.append(auc)\n        end = time.time()\n        print(f'This Fold {index+1}, took {end - start} seconds.')\n        \n    return aucs, y_test/n_splits\n# aucs, y_test = cross_validate_model(LGBMClassifier(**params), X_test=X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:39:06.135981Z","iopub.execute_input":"2023-04-08T19:39:06.136373Z","iopub.status.idle":"2023-04-08T19:39:45.271086Z","shell.execute_reply.started":"2023-04-08T19:39:06.136338Z","shell.execute_reply":"2023-04-08T19:39:45.269612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.read_csv('/kaggle/input/playground-series-s3e12/sample_submission.csv')\n# submission['target'] = y_test\n# submission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:46:03.382092Z","iopub.execute_input":"2023-04-08T19:46:03.382841Z","iopub.status.idle":"2023-04-08T19:46:03.395294Z","shell.execute_reply.started":"2023-04-08T19:46:03.3828Z","shell.execute_reply":"2023-04-08T19:46:03.393846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom xgboost import XGBClassifier\ndef objective_xgb(trial):\n    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n    param = {\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_categorical(\"n_estimators\", [150, 200, 300, 3000]),\n        'max_depth': trial.suggest_categorical('max_depth', [4,5,7,9,11,13,15,17]),\n        'random_state': 42,\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_test, y_test)], verbose=False)    \n    preds = model.predict_proba(X_test)[:, 1]\n    auc_score = roc_auc_score(y_test, preds) \n\n    return auc_score  \n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective_xgb, n_trials=50)\n# params_xgb = study.best_trial.params\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', params_xgb)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:50:18.383381Z","iopub.execute_input":"2023-04-08T19:50:18.383846Z","iopub.status.idle":"2023-04-08T19:50:18.509083Z","shell.execute_reply.started":"2023-04-08T19:50:18.383803Z","shell.execute_reply":"2023-04-08T19:50:18.508018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGB accuracy increases with more features\nparams_xgb = {'lambda': 0.2147524663546028, 'alpha': 0.5750492421465946, \n              'colsample_bytree': 0.7, 'subsample': 0.6, \n              'learning_rate': 0.008, 'n_estimators': 300, \n              'max_depth': 15, 'min_child_weight': 1}\nX_train, X_valid, y_train, y_valid = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\nmodel = XGBClassifier(**params_xgb)\nmodel.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_valid, y_valid)], verbose=False)   \ny_pred = model.predict_proba(X_valid)[:, 1]\ny_test2 = model.predict_proba(X_test)[:,1]\nprint(f\"Auc: {round(roc_auc_score(y_valid,y_pred), 2)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:54:35.083799Z","iopub.execute_input":"2023-04-08T19:54:35.084758Z","iopub.status.idle":"2023-04-08T19:54:37.417079Z","shell.execute_reply.started":"2023-04-08T19:54:35.084701Z","shell.execute_reply":"2023-04-08T19:54:37.415541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsubmission = pd.read_csv('/kaggle/input/playground-series-s3e12/sample_submission.csv')\nsubmission['target'] = np.mean( np.array([[ y_test, y_test2]]), axis=0 )[0]\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:56:17.68052Z","iopub.execute_input":"2023-04-08T19:56:17.680926Z","iopub.status.idle":"2023-04-08T19:56:17.694435Z","shell.execute_reply.started":"2023-04-08T19:56:17.68089Z","shell.execute_reply":"2023-04-08T19:56:17.692631Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
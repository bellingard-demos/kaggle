{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, f1_score,roc_curve, precision_score, recall_score,roc_auc_score\nfrom sklearn import linear_model, tree, ensemble\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.simplefilter(action=\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-29T17:19:29.15435Z","iopub.execute_input":"2023-03-29T17:19:29.155338Z","iopub.status.idle":"2023-03-29T17:19:31.048523Z","shell.execute_reply.started":"2023-03-29T17:19:29.155271Z","shell.execute_reply":"2023-03-29T17:19:31.04721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df):\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-03-28T23:49:48.771946Z","iopub.execute_input":"2023-03-28T23:49:48.772473Z","iopub.status.idle":"2023-03-28T23:49:48.786558Z","shell.execute_reply.started":"2023-03-28T23:49:48.772426Z","shell.execute_reply":"2023-03-28T23:49:48.784837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessing:\n    def __init__(self, df):\n        self.df = df\n        \n    def shape(self):\n        print(f'shape: {self.df.shape}')\n    \n    def dtypes(self, pr=False):\n        print(\"Types\")\n        if pr:\n            print(self.df.dtypes)\n            \n    def supposed2beint(self):\n        float_cols = [column for column in self.df.columns if self.df[column].dtype == 'float']\n        int2be_cols = []\n        for col in float_cols:\n            if (self.df[col].fillna(-9999) % 1  == 0).all() == True:\n                int2be_cols.append(col)\n        return int2be_cols\n                \n    def isNaN(self, pr=False):\n        if pr:\n            print(\"Contain NaN\")\n            print(self.df.isnull().sum())\n        else:\n            return self.df.columns[self.df.isna().any()].tolist()\n    \n    def isObject(self):\n        return [column for column in self.df.columns if self.df[column].dtype == 'object']\n        \n    def check_dataframe(self):\n        self.shape()\n        self.dtypes(True)\n        self.isNaN(True)\n        \n    def fillNaN(self):\n        nan_cols = self.isNaN()\n        int2be_cols = self.supposed2beint()\n        # nan_cols == int2be_cols they are the same\n        for col in int2be_cols:\n            self.df[col].fillna(round(self.df[col].mean()), inplace=True)\n            self.df[col] = self.df[col].astype(int)\n            \n    def adjust_category_cols(self):\n        self.fillNaN()\n        #PreferredLoginDevice\n        self.df.loc[self.df[\"PreferredLoginDevice\"] == \"Mobile Phone\", \"PreferredLoginDevice\"] = \"Phone\"\n        #PreferredPaymentMode\n        self.df.loc[self.df[\"PreferredPaymentMode\"] == \"Credit Card\", \"PreferredPaymentMode\"] = \"CC\"\n        self.df.loc[self.df[\"PreferredPaymentMode\"] == \"Cash on Delivery\", \"PreferredPaymentMode\"] = \"COD\"\n        #PreferredLoginDevice\n        self.df.loc[self.df[\"PreferredLoginDevice\"] == \"Mobile Phone\", \"PreferredLoginDevice\"] = \"Phone\"\n        \n    def drop_useless_cols(self):\n        self.df.drop(['CustomerID'], axis=1, inplace=True)\n        \n    def split_target(self):\n        self.adjust_category_cols()\n        self.drop_useless_cols()\n        self.X = self.df.drop('Churn', axis=1)\n        self.y = self.df['Churn'].astype(int).to_numpy()\n        \n    def find_enc_method(self):\n        cat_cols = self.isObject()\n        one_hot_cols = [col for col in cat_cols if self.X[col].nunique() <=3]\n        label_enc_cols = [col for col in cat_cols if col not in one_hot_cols]\n        return one_hot_cols, label_enc_cols, cat_cols\n    \n    def encoding(self):\n        one_hot_cols, label_enc_cols, cat_cols = self.find_enc_method()\n        num_cols = [col for col in self.X.columns if col not in cat_cols]\n        X_OHE, X_LE, X_NUM = self.X[one_hot_cols].copy(), self.X[label_enc_cols].copy(), self.X[num_cols].copy()\n        self.OHE = OneHotEncoder(drop='first', handle_unknown='error')\n        X_OHE = self.OHE.fit_transform(X_OHE).toarray()\n        self.le_dict = {}\n        self.LE = LabelEncoder()\n#         X_LE[label_enc_cols] = X_LE[label_enc_cols].apply(lambda col: self.LE.fit_transform(col))   \n        for col in X_LE.columns:\n            self.le_dict[col] = self.LE.fit(X_LE[col])\n            X_LE[col] = self.le_dict[col].transform(X_LE[col])\n        return X_OHE, X_LE.to_numpy(), X_NUM.to_numpy()\n\n    def scaling(self):\n        X_OHE, X_LE, X_num = self.encoding()\n        self.SS = StandardScaler()\n        X_num = self.SS.fit_transform(X_num)\n        self.X_total = np.concatenate((X_OHE, X_LE, X_num), axis=1)\n#         self.X_total = self.SS.fit_transform(self.X_total)\n        \n    def get_encoders(self):\n        return self.OHE, self.LE, self.le_dict\n    \n    def get_scaler(self):\n        return self.SS   \n\n    def get_default_Xy(self):\n        self.split_target()\n        return self.X, self.y\n      \n    def get_Xy(self):\n        self.split_target()\n        self.scaling()\n        return self.X_total, self.y\n\ndf = pd.read_excel('/kaggle/input/ecommerce-customer-churn-analysis-and-prediction/E Commerce Dataset.xlsx', sheet_name='E Comm')\ndata = pd.read_excel('/kaggle/input/ecommerce-customer-churn-analysis-and-prediction/E Commerce Dataset.xlsx', sheet_name='Data Dict')\npre = Preprocessing(df)\nX, y = pre.get_Xy()\nunique, counts = np.unique(y, return_counts=True)\nunique, counts","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:21:21.711401Z","iopub.execute_input":"2023-03-29T17:21:21.712005Z","iopub.status.idle":"2023-03-29T17:21:23.759993Z","shell.execute_reply.started":"2023-03-29T17:21:21.711956Z","shell.execute_reply":"2023-03-29T17:21:23.757657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\ndef apply_smote(X, y, random_state=None):\n    \"\"\"\n    Applies SMOTE to the input features (X) and target variable (y) to balance the dataset.\n    \n    Parameters:\n    X: numpy array or pandas DataFrame with the input features\n    y: numpy array or pandas Series with the target variable\n    random_state: int, default=None, controls the randomness of the SMOTE algorithm\n    \n    Returns:\n    X_resampled: numpy array with the resampled input features\n    y_resampled: numpy array with the resampled target variable\n    \"\"\"\n    smote = SMOTE(random_state=random_state)\n    X_resampled, y_resampled = smote.fit_resample(X, y)\n    return X_resampled, y_resampled\n\nX_resampled, y_resampled = apply_smote(X, y, random_state=42)\nprint(X_resampled.shape)\nprint(y_resampled.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:19:59.840825Z","iopub.execute_input":"2023-03-29T17:19:59.841365Z","iopub.status.idle":"2023-03-29T17:20:00.188741Z","shell.execute_reply.started":"2023-03-29T17:19:59.84131Z","shell.execute_reply":"2023-03-29T17:20:00.186663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nmodel = linear_model.LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nCR = classification_report(y_test, y_pred)\nCM = confusion_matrix(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred)\nprint(\"=\"*60)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"=\"*60)\nprint(f\"Classification Report:\\n {CR}\")\nprint(\"=\"*60)\nprint(f\"Confusion Matrix: {CM}\")\nprint(\"=\"*60)\nprint(f\"F1 Score: {f1:.4f}\")\nprint(\"=\"*60)\nprint(f\"ROC AUC Score: {roc_auc:.4f}\")\nprint(\"=\"*60)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T15:05:50.81279Z","iopub.execute_input":"2023-03-29T15:05:50.813107Z","iopub.status.idle":"2023-03-29T15:05:50.930184Z","shell.execute_reply.started":"2023-03-29T15:05:50.813081Z","shell.execute_reply":"2023-03-29T15:05:50.928537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom xgboost import XGBClassifier\ndef objective_xgb(trial):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n    param = {\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_categorical(\"n_estimators\", [150, 200, 300, 3000]),\n        'max_depth': trial.suggest_categorical('max_depth', [4,5,7,9,11,13,15,17]),\n        'random_state': 42,\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_test, y_test)], verbose=False)   \n    preds = model.predict(X_test)\n    \n    acc = accuracy_score(y_test, preds)      \n\n    return acc\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective_xgb, n_trials=50)\nparams_xgb = study.best_trial.params\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', params_xgb)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:32:06.31986Z","iopub.execute_input":"2023-03-29T14:32:06.3202Z","iopub.status.idle":"2023-03-29T14:41:54.352348Z","shell.execute_reply.started":"2023-03-29T14:32:06.320172Z","shell.execute_reply":"2023-03-29T14:41:54.351654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\nmodel = XGBClassifier(**params_xgb)\nmodel.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_test, y_test)], verbose=False)   \ny_pred = model.predict(X_test)\nprint(f\"Accuracy: {round(accuracy_score(y_pred, y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"Recall: {round(recall_score(y_pred,y_test),2)}\")\nprint(\"=\"*60)\nprint(f\"Precision: {round(precision_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"F1: {round(f1_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"Auc: {round(roc_auc_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nCR = classification_report(y_test, y_pred)\nCM = confusion_matrix(y_test, y_pred)\nprint(f\"Classification Report:\\n {CR}\")\nprint(\"=\"*60)\nprint(f\"Confusion Matrix: {CM}\")\nprint(\"=\"*60)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T15:05:58.527511Z","iopub.execute_input":"2023-03-29T15:05:58.527852Z","iopub.status.idle":"2023-03-29T15:06:26.523883Z","shell.execute_reply.started":"2023-03-29T15:05:58.527824Z","shell.execute_reply":"2023-03-29T15:06:26.522446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom lightgbm import LGBMClassifier\ndef objective_lgbm(trial):\n    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n    param = {\n        'random_state': 42,\n        'n_estimators': trial.suggest_categorical(\"n_estimators\", [150, 200, 300, 3000]),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n    }\n    model = LGBMClassifier(**param)\n    model.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_test, y_test)], verbose=False)    \n    preds = model.predict(X_test)\n    \n    acc = accuracy_score(y_test, preds)      \n\n    return acc\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective_lgbm, n_trials=50)\nparams_lgbm = study.best_trial.params\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', params_lgbm)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:27:18.874119Z","iopub.execute_input":"2023-03-29T17:27:18.874692Z","iopub.status.idle":"2023-03-29T17:47:00.615328Z","shell.execute_reply.started":"2023-03-29T17:27:18.87464Z","shell.execute_reply":"2023-03-29T17:47:00.614316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y)\nmodel = LGBMClassifier(**params_lgbm)\nmodel.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_test, y_test)], verbose=False)   \ny_pred = model.predict(X_test)\nprint(f\"Accuracy: {round(accuracy_score(y_pred, y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"Recall: {round(recall_score(y_pred,y_test),2)}\")\nprint(\"=\"*60)\nprint(f\"Precision: {round(precision_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"F1: {round(f1_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"Auc: {round(roc_auc_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nCR = classification_report(y_test, y_pred)\nCM = confusion_matrix(y_test, y_pred)\nprint(f\"Classification Report:\\n {CR}\")\nprint(\"=\"*60)\nprint(f\"Confusion Matrix: {CM}\")\nprint(\"=\"*60)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:47:00.61742Z","iopub.execute_input":"2023-03-29T17:47:00.617827Z","iopub.status.idle":"2023-03-29T17:47:28.752614Z","shell.execute_reply.started":"2023-03-29T17:47:00.617787Z","shell.execute_reply":"2023-03-29T17:47:28.751017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X, y):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt+=1\n\nscore = cross_val_score(ensemble.RandomForestClassifier(random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:25:22.217277Z","iopub.execute_input":"2023-03-29T14:25:22.217618Z","iopub.status.idle":"2023-03-29T14:25:25.245841Z","shell.execute_reply.started":"2023-03-29T14:25:22.217584Z","shell.execute_reply":"2023-03-29T14:25:25.244738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\ncatboost_model = CatBoostClassifier(verbose=False, random_state=42).fit(X_train, y_train)\ny_pred = catboost_model.predict(X_test)\n\nprint(f\"Accuracy: {round(accuracy_score(y_pred, y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"Recall: {round(recall_score(y_pred,y_test),2)}\")\nprint(\"=\"*60)\nprint(f\"Precision: {round(precision_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"F1: {round(f1_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nprint(f\"Auc: {round(roc_auc_score(y_pred,y_test), 2)}\")\nprint(\"=\"*60)\nCR = classification_report(y_test, y_pred)\nCM = confusion_matrix(y_test, y_pred)\nprint(f\"Classification Report:\\n {CR}\")\nprint(\"=\"*60)\nprint(f\"Confusion Matrix: {CM}\")\nprint(\"=\"*60)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T15:05:33.048396Z","iopub.execute_input":"2023-03-29T15:05:33.048761Z","iopub.status.idle":"2023-03-29T15:05:35.359259Z","shell.execute_reply.started":"2023-03-29T15:05:33.048734Z","shell.execute_reply":"2023-03-29T15:05:35.358221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook I trained several Regression models for 'all features' vs 'selected features' data\n\nSome of features might be very helpful important and have huge impact on determining Housing prices\n\nFeature Selection techniques are used in order to improve model's accuracy\n\nAnd in this notebook we will take a look at by how much the accuracy of our models would improve","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-06T23:27:27.384174Z","iopub.execute_input":"2023-05-06T23:27:27.38465Z","iopub.status.idle":"2023-05-06T23:27:27.429296Z","shell.execute_reply.started":"2023-05-06T23:27:27.384612Z","shell.execute_reply":"2023-05-06T23:27:27.428122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:27.432495Z","iopub.execute_input":"2023-05-06T23:27:27.432962Z","iopub.status.idle":"2023-05-06T23:27:28.30745Z","shell.execute_reply.started":"2023-05-06T23:27:27.432921Z","shell.execute_reply":"2023-05-06T23:27:28.306201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:28.309393Z","iopub.execute_input":"2023-05-06T23:27:28.309706Z","iopub.status.idle":"2023-05-06T23:27:28.502038Z","shell.execute_reply.started":"2023-05-06T23:27:28.30968Z","shell.execute_reply":"2023-05-06T23:27:28.500919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:28.503648Z","iopub.execute_input":"2023-05-06T23:27:28.504128Z","iopub.status.idle":"2023-05-06T23:27:29.110402Z","shell.execute_reply.started":"2023-05-06T23:27:28.504085Z","shell.execute_reply":"2023-05-06T23:27:29.109217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.113705Z","iopub.execute_input":"2023-05-06T23:27:29.114209Z","iopub.status.idle":"2023-05-06T23:27:29.118339Z","shell.execute_reply.started":"2023-05-06T23:27:29.114167Z","shell.execute_reply":"2023-05-06T23:27:29.117256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/house/Housing.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.120121Z","iopub.execute_input":"2023-05-06T23:27:29.120875Z","iopub.status.idle":"2023-05-06T23:27:29.16473Z","shell.execute_reply.started":"2023-05-06T23:27:29.120831Z","shell.execute_reply":"2023-05-06T23:27:29.163699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Superficial analysis of the dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.166286Z","iopub.execute_input":"2023-05-06T23:27:29.166996Z","iopub.status.idle":"2023-05-06T23:27:29.205581Z","shell.execute_reply.started":"2023-05-06T23:27:29.166951Z","shell.execute_reply":"2023-05-06T23:27:29.204187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.209147Z","iopub.execute_input":"2023-05-06T23:27:29.209483Z","iopub.status.idle":"2023-05-06T23:27:29.24199Z","shell.execute_reply.started":"2023-05-06T23:27:29.209454Z","shell.execute_reply":"2023-05-06T23:27:29.240833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.243507Z","iopub.execute_input":"2023-05-06T23:27:29.243862Z","iopub.status.idle":"2023-05-06T23:27:29.304583Z","shell.execute_reply.started":"2023-05-06T23:27:29.243832Z","shell.execute_reply":"2023-05-06T23:27:29.303584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for null values and replacing values with mean values of current features","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.305841Z","iopub.execute_input":"2023-05-06T23:27:29.306536Z","iopub.status.idle":"2023-05-06T23:27:29.316171Z","shell.execute_reply.started":"2023-05-06T23:27:29.306501Z","shell.execute_reply":"2023-05-06T23:27:29.315016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.columns[:-1]:\n    df[i].fillna(np.mean(df[i]), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.321203Z","iopub.execute_input":"2023-05-06T23:27:29.321674Z","iopub.status.idle":"2023-05-06T23:27:29.335623Z","shell.execute_reply.started":"2023-05-06T23:27:29.32164Z","shell.execute_reply":"2023-05-06T23:27:29.334712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, corner=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:29.337191Z","iopub.execute_input":"2023-05-06T23:27:29.337627Z","iopub.status.idle":"2023-05-06T23:27:58.468341Z","shell.execute_reply.started":"2023-05-06T23:27:29.337589Z","shell.execute_reply":"2023-05-06T23:27:58.467233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:58.469539Z","iopub.execute_input":"2023-05-06T23:27:58.469951Z","iopub.status.idle":"2023-05-06T23:27:58.480281Z","shell.execute_reply.started":"2023-05-06T23:27:58.469915Z","shell.execute_reply":"2023-05-06T23:27:58.47901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Pipeline\n\nEvaluation pipeline to automate the process of training and evaluation, instead of training and evaluating for every model","metadata":{}},{"cell_type":"code","source":"class Evaluation:\n    def __init__(self, train, test):\n        self.train = train\n        self.test = test\n        \n    def evaluate(self, model, name):\n        x, y = self.train\n        y_pred = model.predict(x)\n        mae = mean_absolute_error(y_pred, y)\n        mse = mean_squared_error(y_pred, y)\n        r2 = r2_score(y_pred, y)\n        print(name, \"\\n\", \"-\"*20)\n        print(\"MAE: {}\\nMSE: {}\\nr2: {}\".format(mae, mse, r2))\n        \n    def training(self, model, name):\n        x, y = self.train\n        model.fit(x, y)\n        self.evaluate(model, name)\n        return model","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:58.482581Z","iopub.execute_input":"2023-05-06T23:27:58.483029Z","iopub.status.idle":"2023-05-06T23:27:58.491886Z","shell.execute_reply.started":"2023-05-06T23:27:58.482992Z","shell.execute_reply":"2023-05-06T23:27:58.490795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining models, tuning their hyperparameters","metadata":{}},{"cell_type":"code","source":"lnr = LinearRegression()\nrfr = RandomForestRegressor(n_estimators=150, max_depth=115, criterion='friedman_mse',\n                           max_features='log2')\ndtr = DecisionTreeRegressor(max_depth=110,criterion='friedman_mse')\nsvr = SVR(C=0.7)\nabr = AdaBoostRegressor(n_estimators=50, learning_rate=0.5)\nxgb = XGBRegressor(n_estimators=1000, max_depth=11, eta=0.31)\n\nmodels = [lnr, rfr, dtr, svr, abr, xgb]\nnames = ['Linear Regression', 'Random Forest Regressor',\n        'Decision Tree Regressor', 'SVR',\n        'Ada Boost Regressor', 'XGBRegressor']\n\nassesment = Evaluation((x_train, y_train), (x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:58.493547Z","iopub.execute_input":"2023-05-06T23:27:58.493979Z","iopub.status.idle":"2023-05-06T23:27:58.50925Z","shell.execute_reply.started":"2023-05-06T23:27:58.493942Z","shell.execute_reply":"2023-05-06T23:27:58.508051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of models","metadata":{}},{"cell_type":"code","source":"trained = []\nfor i, j in zip(models, names):\n    trained += [assesment.training(i, j)]\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:58.511055Z","iopub.execute_input":"2023-05-06T23:27:58.511785Z","iopub.status.idle":"2023-05-06T23:27:59.508304Z","shell.execute_reply.started":"2023-05-06T23:27:58.511719Z","shell.execute_reply":"2023-05-06T23:27:59.507368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\n\nWe will perform Feature Selection using Feature Importance techniques.\n\nWe are peforming Regression techniques with mostly numerical inputs, which means, that we either perform Pearson's correlation or Spearman's correlation\n\n![image](https://machinelearningmastery.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)","metadata":{}},{"cell_type":"markdown","source":"## Pearson's correlation\n\nI decided to go for Pearson's correlation rates\n\nAs it seems from Pairplot - most of features have monotonic relationship or linear relationship with the dominance of\nlinear relationship, Therefore I decided to perform feature selection using Pearson's correlation\n\n![formula](https://editor.analyticsvidhya.com/uploads/39170Formula.JPG)","metadata":{}},{"cell_type":"code","source":"corr_matrix = df.corr(method='pearson')\nsns.heatmap(corr_matrix, annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:27:59.512711Z","iopub.execute_input":"2023-05-06T23:27:59.513492Z","iopub.status.idle":"2023-05-06T23:28:00.513031Z","shell.execute_reply.started":"2023-05-06T23:27:59.513445Z","shell.execute_reply":"2023-05-06T23:28:00.511678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Threshold\n\nLet's classify ranges for Pearson's correlation\n\nAs it seems coefficient ranging from +/-0.4 to +/-0.59 inclusive is supposed to be moderate\n\nIf we choose more than 0.6 - we get very low amount of features, hence we choose features with correlation rate of more than 0.4\n\n![pearson](https://www.researchgate.net/profile/Mahiswaran-Selvanathan/publication/345693737/figure/tbl1/AS:956412914040832@1605038016475/The-scale-of-Pearsons-Correlation-Coefficient.png)","metadata":{}},{"cell_type":"code","source":"selected = []\nfor i in corr_matrix.index[:-1]:\n    if corr_matrix.loc[i, \"MEDV\"] > 0.4 or corr_matrix.loc[i, \"MEDV\"] < -0.4:\n        selected += [i]","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:29:33.346439Z","iopub.execute_input":"2023-05-06T23:29:33.346847Z","iopub.status.idle":"2023-05-06T23:29:33.353251Z","shell.execute_reply.started":"2023-05-06T23:29:33.346814Z","shell.execute_reply":"2023-05-06T23:29:33.352107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, vars=selected+['MEDV'], corner=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:29:35.987Z","iopub.execute_input":"2023-05-06T23:29:35.98748Z","iopub.status.idle":"2023-05-06T23:29:44.005321Z","shell.execute_reply.started":"2023-05-06T23:29:35.987439Z","shell.execute_reply":"2023-05-06T23:29:44.003995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_s = df.loc[:, selected].values\ny_s = df.loc[:, 'MEDV'].values\nxs_train, xs_test, ys_train, ys_test = train_test_split(x_s, y_s, random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:29:51.032325Z","iopub.execute_input":"2023-05-06T23:29:51.032706Z","iopub.status.idle":"2023-05-06T23:29:51.042067Z","shell.execute_reply.started":"2023-05-06T23:29:51.032677Z","shell.execute_reply":"2023-05-06T23:29:51.040855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of Selected Features\n\nDrastic improvement for:\n* Random Forest Classifier\n* Decision Tree Classifier\n* Ada Boost Regressor\n* XGBRegressor\n\nSlight improvement for Linear Regression\n\nAnd SVR remain horrible, that r^2 score remains negative","metadata":{}},{"cell_type":"code","source":"assesment_selected = Evaluation((xs_train, ys_train), (xs_test, ys_test))\nselected_trained = []\nfor i, j in zip(models, names):\n    selected_trained += [assesment_selected.training(i, j)]","metadata":{"execution":{"iopub.status.busy":"2023-05-06T23:29:58.740933Z","iopub.execute_input":"2023-05-06T23:29:58.741317Z","iopub.status.idle":"2023-05-06T23:29:59.589299Z","shell.execute_reply.started":"2023-05-06T23:29:58.741285Z","shell.execute_reply":"2023-05-06T23:29:59.588441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nIn this dataset there is a list of features that are **ALL** affect Housing price.\n\nHowever, in order to improve Regression Models' accuracies we need to perform the removal of such preprocessing techniques as scaling the data, outliers detection and feature selection\n\nMost of ML algorithms using sklearn perform just as good with the data that is not preprocessed using Min Max Scaler technique or Stanard Scaler technique, etc. \n\nThere are not that much of significant outliers.\n\nTherefore the only thing that's left is to perform Feature Selection. And I decided to go for Feature Importance Pearson's correlation technique.\n\nI compared the results of models trained on data that did not go through feature selection and models trained on selected features.\n\nIn the end, most models' performance was drastically improved.","metadata":{}}]}
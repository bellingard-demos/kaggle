{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-13T10:05:27.215536Z","iopub.execute_input":"2023-03-13T10:05:27.216333Z","iopub.status.idle":"2023-03-13T10:05:27.228298Z","shell.execute_reply.started":"2023-03-13T10:05:27.216262Z","shell.execute_reply":"2023-03-13T10:05:27.227312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.22999Z","iopub.execute_input":"2023-03-13T10:05:27.230359Z","iopub.status.idle":"2023-03-13T10:05:27.238156Z","shell.execute_reply.started":"2023-03-13T10:05:27.230325Z","shell.execute_reply":"2023-03-13T10:05:27.236843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/medical-student-mental-health/Data Carrard et al. 2022 MedTeach.csv')\ncodebook = pd.read_csv('/kaggle/input/medical-student-mental-health/Codebook Carrard et al. 2022 MedTeach.csv', sep=';')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.240111Z","iopub.execute_input":"2023-03-13T10:05:27.240754Z","iopub.status.idle":"2023-03-13T10:05:27.261981Z","shell.execute_reply.started":"2023-03-13T10:05:27.2407Z","shell.execute_reply":"2023-03-13T10:05:27.260973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"codebook.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.264268Z","iopub.execute_input":"2023-03-13T10:05:27.264776Z","iopub.status.idle":"2023-03-13T10:05:27.280287Z","shell.execute_reply.started":"2023-03-13T10:05:27.264723Z","shell.execute_reply":"2023-03-13T10:05:27.278928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_groups(value):\n    cleaned = [i.strip(\" \") for i in value.split(\";\")]\n    grouped_dict = dict()\n    for i in cleaned:\n        div = i.split(\"=\")\n        grouped_dict[int(div[0])] = div[1]\n    return grouped_dict","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.281872Z","iopub.execute_input":"2023-03-13T10:05:27.282367Z","iopub.status.idle":"2023-03-13T10:05:27.290601Z","shell.execute_reply.started":"2023-03-13T10:05:27.282319Z","shell.execute_reply":"2023-03-13T10:05:27.28925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maps = dict()\ncolumns = codebook.iloc[:, 0].values\nids = [2, 3, 4, 5, 6, 8, 9]\ncount = 0\nfor i in codebook.iloc[ids, 2]:\n    maps[columns[ids[count]]] = create_groups(i)\n    count += 1","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.291677Z","iopub.execute_input":"2023-03-13T10:05:27.29215Z","iopub.status.idle":"2023-03-13T10:05:27.308258Z","shell.execute_reply.started":"2023-03-13T10:05:27.292104Z","shell.execute_reply":"2023-03-13T10:05:27.307279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mapping encoded values from the dataset for pie charts and frequency counts with clear distinction and labellings","metadata":{}},{"cell_type":"code","source":"maps","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.311688Z","iopub.execute_input":"2023-03-13T10:05:27.312211Z","iopub.status.idle":"2023-03-13T10:05:27.32418Z","shell.execute_reply.started":"2023-03-13T10:05:27.312157Z","shell.execute_reply":"2023-03-13T10:05:27.322816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.326304Z","iopub.execute_input":"2023-03-13T10:05:27.326727Z","iopub.status.idle":"2023-03-13T10:05:27.348471Z","shell.execute_reply.started":"2023-03-13T10:05:27.326675Z","shell.execute_reply":"2023-03-13T10:05:27.347093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.34991Z","iopub.execute_input":"2023-03-13T10:05:27.35043Z","iopub.status.idle":"2023-03-13T10:05:27.359771Z","shell.execute_reply.started":"2023-03-13T10:05:27.350376Z","shell.execute_reply":"2023-03-13T10:05:27.358438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_classes = df.copy().iloc[:, ids]","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.363606Z","iopub.execute_input":"2023-03-13T10:05:27.364266Z","iopub.status.idle":"2023-03-13T10:05:27.372295Z","shell.execute_reply.started":"2023-03-13T10:05:27.364204Z","shell.execute_reply":"2023-03-13T10:05:27.370926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df_classes.columns:\n    df_classes[i] = df_classes[i].map(maps[i])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.374093Z","iopub.execute_input":"2023-03-13T10:05:27.37454Z","iopub.status.idle":"2023-03-13T10:05:27.392315Z","shell.execute_reply.started":"2023-03-13T10:05:27.374441Z","shell.execute_reply":"2023-03-13T10:05:27.390648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(20, 25))\ncols = df_classes.columns\nfor i in range(0, 3):\n    freqs = df_classes[cols[i]].value_counts()\n    axes[i].pie(freqs, labels=freqs.index, autopct=\"%0.2f%%\")\n    axes[i].set_title(cols[i])\nfig, axes = plt.subplots(ncols=4, nrows=1, figsize=(20, 25))\nfor i in range(3, df_classes.shape[1]):\n    freqs = df_classes[cols[i]].value_counts()\n    axes[i-3].pie(freqs, labels=freqs.index, autopct=\"%0.2f%%\")\n    axes[i-3].set_title(cols[i])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:27.393953Z","iopub.execute_input":"2023-03-13T10:05:27.394466Z","iopub.status.idle":"2023-03-13T10:05:28.592627Z","shell.execute_reply.started":"2023-03-13T10:05:27.394412Z","shell.execute_reply":"2023-03-13T10:05:28.591068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang_freqs = df_classes['glang'].value_counts()\nlang_freqs","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:28.594909Z","iopub.execute_input":"2023-03-13T10:05:28.595816Z","iopub.status.idle":"2023-03-13T10:05:28.607107Z","shell.execute_reply.started":"2023-03-13T10:05:28.595763Z","shell.execute_reply":"2023-03-13T10:05:28.606107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mental health satisfaction among different groups","metadata":{}},{"cell_type":"code","source":"numerical = [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nsns.pairplot(df, hue='psyt', vars=df.columns[numerical])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:05:28.60874Z","iopub.execute_input":"2023-03-13T10:05:28.609581Z","iopub.status.idle":"2023-03-13T10:06:13.749474Z","shell.execute_reply.started":"2023-03-13T10:05:28.60953Z","shell.execute_reply":"2023-03-13T10:06:13.748398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Suggestion is to make a binary classification on whether student will take terapy or not, but the dataset is most likely to\n# be unbalanced in that sense. Hence, we would need to later apply SMOTE technique or any other techniques to balanced the \n# dataset in a sense of classes\n# I am planning on applying GaussianNB and Tree structures for classification tasks.\n# We will use features importance of Random Forest Classifier class to determine most important features","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:13.752575Z","iopub.execute_input":"2023-03-13T10:06:13.753203Z","iopub.status.idle":"2023-03-13T10:06:13.758427Z","shell.execute_reply.started":"2023-03-13T10:06:13.753163Z","shell.execute_reply":"2023-03-13T10:06:13.757118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_evaluation(model, test_sample, truth, categorical=True):\n    yhat = model.predict(test_sample)\n    score = accuracy_score(yhat, truth)\n    text = \"Accuracy score for the model {}%\".format(round(score*100, 2))\n    if categorical:\n        cm = confusion_matrix(yhat, truth)\n        sns.heatmap(cm, annot=True)\n        plt.title(text)\n    else:\n        dd = dict()\n        for i in range(test_sample.shape[1]):\n            name = \"data:{}\".format(i)\n            dd[name] = test_sample[:, i]\n#         dd['target'] = truth\n        dd['predicted'] = yhat\n        print(text)\n        df = pd.DataFrame(dd)\n#         fig, axes = plt.subplots(nrows=2, ncols=1)\n        sns.pairplot(df, hue='predicted', vars=df.columns[:-1])\n        plt.title(text)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:13.760551Z","iopub.execute_input":"2023-03-13T10:06:13.761188Z","iopub.status.idle":"2023-03-13T10:06:13.774317Z","shell.execute_reply.started":"2023-03-13T10:06:13.761118Z","shell.execute_reply":"2023-03-13T10:06:13.773048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Determining Important Features","metadata":{}},{"cell_type":"code","source":"temp = [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nX = df.iloc[:, temp].values\ny = df.iloc[:, 9].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n# ids[:-1]+numerical\nforest = RandomForestClassifier(random_state=0)\nforest.fit(X_train, y_train)\n\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n\nforest_importances = pd.Series(forest.feature_importances_, index=df.columns[temp])\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:13.775988Z","iopub.execute_input":"2023-03-13T10:06:13.777109Z","iopub.status.idle":"2023-03-13T10:06:14.503746Z","shell.execute_reply.started":"2023-03-13T10:06:13.777058Z","shell.execute_reply":"2023-03-13T10:06:14.502272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting Features with decrease of impurity more than 0.04. And applying SMOTE technique to get rid of class imbalance","metadata":{}},{"cell_type":"code","source":"temp = [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nx = df.iloc[:, temp].values\ny = df.loc[:, 'psyt'].values\noversample = SMOTE()\nx_all, y_all = oversample.fit_resample(x, y)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:14.505444Z","iopub.execute_input":"2023-03-13T10:06:14.505936Z","iopub.status.idle":"2023-03-13T10:06:14.531364Z","shell.execute_reply.started":"2023-03-13T10:06:14.505893Z","shell.execute_reply":"2023-03-13T10:06:14.529072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x_all, y_all, random_state=42, test_size=0.2)\n# ids[:-1]+numerical\nforest = RandomForestClassifier(random_state=0)\nforest.fit(X_train, y_train)\n\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n\nforest_importances = pd.Series(forest.feature_importances_, index=df.columns[temp])\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:14.533721Z","iopub.execute_input":"2023-03-13T10:06:14.538618Z","iopub.status.idle":"2023-03-13T10:06:15.411192Z","shell.execute_reply.started":"2023-03-13T10:06:14.538535Z","shell.execute_reply":"2023-03-13T10:06:15.409584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_os = np.hstack((x_all, y_all.reshape(-1, 1)))\ndf_os = pd.DataFrame(temp_os, columns=df.columns[temp+[9]])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:15.413449Z","iopub.execute_input":"2023-03-13T10:06:15.413973Z","iopub.status.idle":"2023-03-13T10:06:15.421902Z","shell.execute_reply.started":"2023-03-13T10:06:15.413918Z","shell.execute_reply":"2023-03-13T10:06:15.420109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.pop(3)\ntemp.pop(3)\nx, y = df.iloc[:, temp].values, df.loc[:, 'psyt'].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:15.424215Z","iopub.execute_input":"2023-03-13T10:06:15.424813Z","iopub.status.idle":"2023-03-13T10:06:15.438125Z","shell.execute_reply.started":"2023-03-13T10:06:15.424757Z","shell.execute_reply":"2023-03-13T10:06:15.436844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(max_depth=20, max_features='sqrt')\nrfc.fit(x_train, y_train)\nmodel_evaluation(rfc, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:15.439965Z","iopub.execute_input":"2023-03-13T10:06:15.440529Z","iopub.status.idle":"2023-03-13T10:06:16.007767Z","shell.execute_reply.started":"2023-03-13T10:06:15.440473Z","shell.execute_reply":"2023-03-13T10:06:16.006424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(x_train, y_train)\nmodel_evaluation(gnb, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:16.009857Z","iopub.execute_input":"2023-03-13T10:06:16.010408Z","iopub.status.idle":"2023-03-13T10:06:16.327139Z","shell.execute_reply.started":"2023-03-13T10:06:16.010352Z","shell.execute_reply":"2023-03-13T10:06:16.326163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(max_depth=20, max_features=\"sqrt\")\ndtc.fit(x_train, y_train)\nmodel_evaluation(dtc, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T10:06:16.328301Z","iopub.execute_input":"2023-03-13T10:06:16.329276Z","iopub.status.idle":"2023-03-13T10:06:16.642666Z","shell.execute_reply.started":"2023-03-13T10:06:16.329235Z","shell.execute_reply":"2023-03-13T10:06:16.641633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see. Random Forest Classifier outperforms classical Decision Tree Algorithm.\nI decided to use Tree classifiers and GaussianNB algorithm because I assumed all of the features of the dataset are important, both categorical and numerical ones. If we used algorithms that are dependant on linear dimension, such as SVMs, KNN, Logistic Regression and etc. we would require to have more of numerical inputs, rather than categorical inputs. However, some of them, as I expected, would make the classification task a bit messier: glang(because there is a huge number of categories) and part column, because it has bigger amount of impurity. That is why I got rid of them and proceeded with classification tasks.","metadata":{}}]}
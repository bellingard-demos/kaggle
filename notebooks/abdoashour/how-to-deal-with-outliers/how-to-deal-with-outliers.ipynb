{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6997972,"sourceType":"datasetVersion","datasetId":4022799}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Outliers\n\nIn this section, we'll be learning about different outlier detection techniques and outlier imputation techniques, by analyzing our simulated customer lifetime value data. In this section, we'll be covering how to: \n\n**Outlier Detection**\n- Box Plots\n- Z-Scores\n- Isolation Forests\n- DBSCAN\n\n**Outlier Treatment**\n- Removal\n- Winsorize\n\n## Import Libraries\n\nFirst, we'll need to import the relevant libraries. We'll be using the standard `pandas`, `numpy` libraries for data manipulation. We'll need to use a few functions from `scipy` for our imputation techniques.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:12:39.028482Z","iopub.execute_input":"2023-11-18T19:12:39.02959Z","iopub.status.idle":"2023-11-18T19:12:40.838249Z","shell.execute_reply.started":"2023-11-18T19:12:39.02955Z","shell.execute_reply":"2023-11-18T19:12:40.837061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n\nNext, we'll load our customer lifetime value dataset. You'll see in our dataset, we have about 6 columns. The `purchases` column is the column we care about in our customer lifetime value problem. ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/clv-data/clv_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:13:23.300375Z","iopub.execute_input":"2023-11-18T19:13:23.301513Z","iopub.status.idle":"2023-11-18T19:13:23.346789Z","shell.execute_reply.started":"2023-11-18T19:13:23.301474Z","shell.execute_reply":"2023-11-18T19:13:23.345621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier Detection\n\nFirst, we'll dive into different methods to detect outliers.....","metadata":{}},{"cell_type":"markdown","source":"## Box Plot\n\nThe first plot we'll use is a boxplot. A boxplot is a method of displaying a distribution of data based off the minimum, maximum lower quartile, upper quartile and the median. An outlier is a datapoint that falls outside the whiskers of the plot. You'll see in this plot, the data point above the whisker would be considered the outlier:","metadata":{}},{"cell_type":"code","source":"sns.boxplot(df['purchases'])","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:13:27.026457Z","iopub.execute_input":"2023-11-18T19:13:27.026831Z","iopub.status.idle":"2023-11-18T19:13:27.427504Z","shell.execute_reply.started":"2023-11-18T19:13:27.026803Z","shell.execute_reply":"2023-11-18T19:13:27.426278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_outliers_from_boxplot(array):\n    ## Get IQR\n    iqr_q1 = np.quantile(array, 0.25)\n    iqr_q3 = np.quantile(array, 0.75)\n\n    # finding the iqr region\n    iqr = iqr_q3 - iqr_q1\n\n    # finding upper and lower whiskers\n    upper_bound = iqr_q3 + (1.5 * iqr)\n    lower_bound = iqr_q1 - (1.5 * iqr)\n\n    outliers = array[(array > upper_bound) | (array < lower_bound)]\n    print('Outliers within the box plot are :{}'.format(outliers))\n    return outliers\n\n# Assuming df is your DataFrame and 'purchases' is a column in it\noutliers = extract_outliers_from_boxplot(df['purchases'])","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:07.352033Z","iopub.execute_input":"2023-11-18T19:15:07.352744Z","iopub.status.idle":"2023-11-18T19:15:07.364388Z","shell.execute_reply.started":"2023-11-18T19:15:07.352691Z","shell.execute_reply":"2023-11-18T19:15:07.363607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Violin Plot\n\nAn alternative to a boxplot is a violin plot. A violin plot includes all the data in a boxplot while also adding density forms. This allows you to see how well your points are distributed across the entire dataset: ","metadata":{}},{"cell_type":"code","source":"plt.violinplot(df['purchases'])","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:10.960568Z","iopub.execute_input":"2023-11-18T19:15:10.960951Z","iopub.status.idle":"2023-11-18T19:15:11.243504Z","shell.execute_reply.started":"2023-11-18T19:15:10.960922Z","shell.execute_reply":"2023-11-18T19:15:11.242339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Z-Scores\n\nA similar method to boxplots is using z-scores. The core difference, is using z-scores, we can specify the percentile we want to use, to classify a point as an outlier:","metadata":{}},{"cell_type":"code","source":"purchases = df['purchases']\n\ndef percentile_outliers(array,\n                        lower_bound_perc,\n                        upper_bound_perc):\n    \n    upper_bound = np.percentile(df['purchases'], upper_bound_perc)\n    lower_bound = np.percentile(df['purchases'], lower_bound_perc)\n    \n    outliers = array[(array <= lower_bound) | (array >= upper_bound)]\n    \n    return outliers\n\ndef z_score_outliers(array,\n                     z_score_lower,\n                     z_score_upper):\n\n    z_scores = scipy.stats.zscore(array)\n    outliers = (z_scores > 1.96) | (z_scores < -1.96)\n    \n    return array[outliers]","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:14.451473Z","iopub.execute_input":"2023-11-18T19:15:14.452764Z","iopub.status.idle":"2023-11-18T19:15:14.459643Z","shell.execute_reply.started":"2023-11-18T19:15:14.452713Z","shell.execute_reply":"2023-11-18T19:15:14.458562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers = percentile_outliers(df['purchases'],\n               upper_bound_perc = 99,\n               lower_bound_perc = 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:16.329562Z","iopub.execute_input":"2023-11-18T19:15:16.33001Z","iopub.status.idle":"2023-11-18T19:15:16.33764Z","shell.execute_reply.started":"2023-11-18T19:15:16.329975Z","shell.execute_reply":"2023-11-18T19:15:16.336372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z_score_outliers(df['purchases'],\n                     z_score_lower = -1.96,\n                     z_score_upper = 1.96)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:17.340107Z","iopub.execute_input":"2023-11-18T19:15:17.340516Z","iopub.status.idle":"2023-11-18T19:15:17.351712Z","shell.execute_reply.started":"2023-11-18T19:15:17.340484Z","shell.execute_reply":"2023-11-18T19:15:17.350516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Isolation Forests\n\nThe next approach is an algorithm based approach called Isolation Forests. Isolation forest is essentially a decision tree that will randomly select a feature to split on. Outliers would likely get split first by the decision tree, which tells us where the outliers are:  ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nfeatures = ['age','income','days_on_platform','purchases']\n\n## We'll do a simple drop null for now\ndf = df.dropna()\n\n## Create a training-test set\nX = df[features]\nX_train = X[:4000]\nX_test = X[1000:]\n\n## Fit Model\nclf = IsolationForest(n_estimators=50, max_samples=100)\nclf.fit(X_train)\n\n## Get Scores\ndf['scores'] = clf.decision_function(X_train)\ndf['anomaly'] = clf.predict(X)\n\n## Get Anomalies\noutliers=df.loc[df['anomaly']==-1]\n\noutliers","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:19.251655Z","iopub.execute_input":"2023-11-18T19:15:19.252051Z","iopub.status.idle":"2023-11-18T19:15:20.050849Z","shell.execute_reply.started":"2023-11-18T19:15:19.25202Z","shell.execute_reply":"2023-11-18T19:15:20.049654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier Treatment\n\nNow that we have some techniques for detecting outliers, let's look into different ways to treat outliers. ","metadata":{}},{"cell_type":"markdown","source":"## Removal\n\nThe first method is simply removing our outliers. The typical way to remove outliers is through z-score removal. Specify the z-score or percentile cutoff you want for your outliers, then, remove any point that falls above or below that threshold. We've written out a few functions you can use: ","metadata":{}},{"cell_type":"code","source":"def z_score_removal(df, column, lower_z_score, upper_z_score):\n    \n    col_df = df[column]\n\n    z_scores = scipy.stats.zscore(purchases)\n    outliers = (z_scores > upper_z_score) | (z_scores < lower_z_score)\n    return df[~outliers]\n\ndef percentile_removal(df, column, lower_bound_perc, upper_bound_perc):\n    \n    col_df = df[column]\n    \n    upper_bound = np.percentile(col_df, upper_bound_perc)\n    lower_bound = np.percentile(col_df, lower_bound_perc)\n\n    z_scores = scipy.stats.zscore(purchases)\n    outliers = (z_scores > upper_bound) | (z_scores < lower_bound)\n    return df[~outliers]\n\nfiltered_df = z_score_removal(df, 'purchases', -1.96, 1.96)\npercentile_removal(df, 'purchases', lower_bound_perc = 1, upper_bound_perc = 99)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:23.081666Z","iopub.execute_input":"2023-11-18T19:15:23.082039Z","iopub.status.idle":"2023-11-18T19:15:23.120883Z","shell.execute_reply.started":"2023-11-18T19:15:23.082011Z","shell.execute_reply":"2023-11-18T19:15:23.119808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Winsorize\n\nDropping outliers is the crudest approach. If you feel those rows are valuable, we can winsorize, also known as \"capping\" our outliers. Rather than keep the outlier value, if the value falls above a specific threshold, we can replace the outlier with that threshold value. Here, we've written a function for you: ","metadata":{}},{"cell_type":"code","source":"def winsorize(df, column, upper, lower):\n    col_df = df[column]\n    \n    perc_upper = np.percentile(df[column],upper)\n    perc_lower = np.percentile(df[column],lower)\n    \n    df[column] = np.where(df[column] >= perc_upper, \n                          perc_upper, \n                          df[column])\n    \n    df[column] = np.where(df[column] <= perc_lower, \n                          perc_lower, \n                          df[column])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:24.576822Z","iopub.execute_input":"2023-11-18T19:15:24.577241Z","iopub.status.idle":"2023-11-18T19:15:24.584581Z","shell.execute_reply.started":"2023-11-18T19:15:24.577208Z","shell.execute_reply":"2023-11-18T19:15:24.583355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"winsorize(df, 'purchases', 97.5, 0.025)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:15:25.75463Z","iopub.execute_input":"2023-11-18T19:15:25.755021Z","iopub.status.idle":"2023-11-18T19:15:25.782415Z","shell.execute_reply.started":"2023-11-18T19:15:25.754989Z","shell.execute_reply":"2023-11-18T19:15:25.781319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Additional Outlier Detection Techniques\n\nOn top of these methods, there are many other methods:\n\n- Mahalanobis Distance: This is a distance metric that helps us detect multivariate outliers. \n- Robust Mahalanobis Distance: Adds a layer on the original, by only using data points where the determinant of the covariance matrix is as small as possible. \n\nThere are a number of additional Algorithm-Based techniques:\n\n- DBScan Cluster Outlier Detection\n- K-Means Cluster Outlier Detection\n- Hierarchical Clustering Detection \n\nThere are also algorithms that are robust to outliers, so you don't need to worry as much if you're using these models:\n\n- Random Forest\n- Gradient Boosted Trees\n\nWe will add these methods in future iterations of the course. ","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nIn conclusion, we've gone over both techniques for detecting outliers and treating outliers. To review, we went over the following methods for detecting outliers: \n\n- Box Plots\n- Violin Plots\n- Z-score method\n- Percentile Method\n- Isolation Forests\n\nTo treat outliers, we went over: \n\n- Z-score Removal\n- Winsorizing","metadata":{}}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:31:24.887801Z","iopub.execute_input":"2023-11-03T06:31:24.88827Z","iopub.status.idle":"2023-11-03T06:31:24.922826Z","shell.execute_reply.started":"2023-11-03T06:31:24.888178Z","shell.execute_reply":"2023-11-03T06:31:24.921397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering for Numeric Variables\nIn this notebook we will cover scaling, transformations, and interactive features. This notebook is the This is a companion workbook for the 365 Data Science course on ML Process. The in-depth explanantions theories and pros/cons for each of these techniques can be found there. \n\n## Feature Scaling\nFeature scaling is important for we are using models with a distance metric. If our features are of different scales, they can be overcompensated for in the models. \n- Absolute Max Scaling\n- MinMax Scaling\n- Z-Score Normalization (Standard Scaler)\n- Robust Scaler \n## Transformations \n- Logarithmic \n- Square Root \n- Exponential\n- Box-Cox\n## Interaction Features\n- Arethmetic Interaction\n- Binning\n- Creative Features \n\nThis a companion notebook for the **365 Data Science Course \"Machine Learning Process A-Z\"**. In the course, there is a video walkthrough of this notebook as well as theory and definitions of each of the techinques. We've designed this notebook to be a stand alone learning tool, but if you're interested in the additional features of the paid course, you can access it at a discount here: https://365datascience.com/learn-machine-learning-process-a-z/\n\n**Check out our 3 course bundle for additional learning (limited time discount 68% off!)** --> [The Machine Learning A-Z Bundle](https://bit.ly/3NAZ5oP)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/craigslist-carstrucks-data/vehicles.csv')\n\n#let's add a column for car age that will help us later on: \ndf['car_age'] = df['year'].max() - df['year']","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:31:30.140954Z","iopub.execute_input":"2023-11-03T06:31:30.141418Z","iopub.status.idle":"2023-11-03T06:32:19.446556Z","shell.execute_reply.started":"2023-11-03T06:31:30.14137Z","shell.execute_reply":"2023-11-03T06:32:19.445302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:40:16.927224Z","iopub.execute_input":"2023-11-03T06:40:16.927765Z","iopub.status.idle":"2023-11-03T06:40:16.938824Z","shell.execute_reply.started":"2023-11-03T06:40:16.927726Z","shell.execute_reply":"2023-11-03T06:40:16.937401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()\n# Columns we may want to normalize \n# Price, Year, Odometer","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:41:33.069981Z","iopub.execute_input":"2023-11-03T06:41:33.070465Z","iopub.status.idle":"2023-11-03T06:41:33.268717Z","shell.execute_reply.started":"2023-11-03T06:41:33.070426Z","shell.execute_reply":"2023-11-03T06:41:33.26741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:42:05.521529Z","iopub.execute_input":"2023-11-03T06:42:05.522115Z","iopub.status.idle":"2023-11-03T06:42:06.019377Z","shell.execute_reply.started":"2023-11-03T06:42:05.522058Z","shell.execute_reply":"2023-11-03T06:42:06.018236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's just use a few features to create an example model and remove Nulls. Learn mnore about different imputation techniques in this other companion notebook. \n#pd.get_dummies() creates dummy variables for the categorical features (see this notebook for more on that)\n#notebook: https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1\ndf_example = pd.get_dummies(df.loc[:,['price','car_age','odometer','manufacturer','condition']].dropna())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:42:00.017782Z","iopub.execute_input":"2023-11-03T06:42:00.018203Z","iopub.status.idle":"2023-11-03T06:42:00.238886Z","shell.execute_reply.started":"2023-11-03T06:42:00.018172Z","shell.execute_reply":"2023-11-03T06:42:00.237563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_example","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:46:21.111426Z","iopub.execute_input":"2023-11-03T06:46:21.11193Z","iopub.status.idle":"2023-11-03T06:46:21.154848Z","shell.execute_reply.started":"2023-11-03T06:46:21.11189Z","shell.execute_reply":"2023-11-03T06:46:21.153527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#see section on train test split: https://www.kaggle.com/code/kenjee/cross-validation-foundations-section-8\nfrom sklearn.model_selection import train_test_split\n\nX = df_example.drop('price',axis =1 )\ny = df_example[['price']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:46:56.380135Z","iopub.execute_input":"2023-11-03T06:46:56.380675Z","iopub.status.idle":"2023-11-03T06:46:56.977883Z","shell.execute_reply.started":"2023-11-03T06:46:56.380633Z","shell.execute_reply":"2023-11-03T06:46:56.976683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.price.plot.box()","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:47:00.558647Z","iopub.execute_input":"2023-11-03T06:47:00.55907Z","iopub.status.idle":"2023-11-03T06:47:00.84106Z","shell.execute_reply.started":"2023-11-03T06:47:00.559038Z","shell.execute_reply":"2023-11-03T06:47:00.839772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.price.plot.box()\ndf.car_age.plot.box()\n#df.odometer.plot.box()","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:47:04.454016Z","iopub.execute_input":"2023-11-03T06:47:04.455197Z","iopub.status.idle":"2023-11-03T06:47:04.688183Z","shell.execute_reply.started":"2023-11-03T06:47:04.455145Z","shell.execute_reply":"2023-11-03T06:47:04.686839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.odometer.plot.box()","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:47:07.996912Z","iopub.execute_input":"2023-11-03T06:47:07.997364Z","iopub.status.idle":"2023-11-03T06:47:08.167698Z","shell.execute_reply.started":"2023-11-03T06:47:07.997308Z","shell.execute_reply":"2023-11-03T06:47:08.166381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling\nFeature scaling is important for we are using models with a distance metric. If our features are of different scales, they can be overcompensated for in the models. \n- Absolute Max Scaling\n- MinMax Scaling\n- Z-Score Normalization (Standard Scaler)\n- Robust Scaler \n","metadata":{}},{"cell_type":"markdown","source":"## Absolute Maximum Scaling\nAbsolute maximum scaling will have you take the maximum value within the data and then divide the raw data by this absolute maximum value.\n\nFor absolute max scaling, this works best if our data doesn't have massive outliers. In this case, we would likely want to remove outliers from price and odometer. This also keeps the same distribution of the data. For absolute maximum scaling, let's do this on the year data for the cars. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MaxAbsScaler\n\n#Scale data \ndf_am = MaxAbsScaler().fit_transform(X_train)\n\n#convert to dataframe to see table\ndf_am = pd.DataFrame(df_am, columns = X_train.columns)\n\n#obvious problems with outliers regarding price & odometer ","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:53:20.276783Z","iopub.execute_input":"2023-11-03T06:53:20.277414Z","iopub.status.idle":"2023-11-03T06:53:20.448752Z","shell.execute_reply.started":"2023-11-03T06:53:20.277365Z","shell.execute_reply":"2023-11-03T06:53:20.446733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Min Max Scaling\nAnother simple form of scaling is called min max. Min Max scaling will scale all our data points between 0 and 1. We’d use the following formula to scale our data, where we subtract the min from the raw data and then divide it by the max minus the min. \n\nAgain, this approach is not robust to outliers.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndf_min_max = MinMaxScaler().fit_transform(X_train)\ndf_min_max = pd.DataFrame(df_min_max, columns = X_train.columns)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:53:31.068907Z","iopub.execute_input":"2023-11-03T06:53:31.06938Z","iopub.status.idle":"2023-11-03T06:53:31.212468Z","shell.execute_reply.started":"2023-11-03T06:53:31.069342Z","shell.execute_reply":"2023-11-03T06:53:31.21155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Z Score Normalization (Standard Scaling)\n\nAnother approach is standardization which transforms the data into the z-score, where the mean is zero and the standard deviation is 1.\n\nThis approach is more robust to outliers, but still can have issues if outliers cause massive changes to standard deviation. However, this does assume a normal distribution which is inaccurate for some of our data (Year).","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ndf_std = X_train.copy()\n#only scale numeric varaibles in this case rather than the dummy variables for categories \ndf_std.loc[:,['car_age','odometer']] = StandardScaler().fit_transform(df_std.loc[:, ['car_age','odometer']])\ndf_std","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:53:36.981122Z","iopub.execute_input":"2023-11-03T06:53:36.981591Z","iopub.status.idle":"2023-11-03T06:53:37.11967Z","shell.execute_reply.started":"2023-11-03T06:53:36.981553Z","shell.execute_reply":"2023-11-03T06:53:37.118404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Robust Scaler\nWith Robust Scaler, we’re subtracting the median and then scaling the column by the IQR.\n\nThis is the approach most robust to outliers that we will cover.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\ndf_rob = X_train.copy()\n#only scale numeric varaibles in this case rather than the dummy variables for categories \ndf_rob.loc[:,['car_age','odometer']] = RobustScaler().fit_transform(df_rob.loc[:, ['car_age','odometer']])\ndf_rob","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:53:56.077602Z","iopub.execute_input":"2023-11-03T06:53:56.078023Z","iopub.status.idle":"2023-11-03T06:53:56.209054Z","shell.execute_reply.started":"2023-11-03T06:53:56.07799Z","shell.execute_reply":"2023-11-03T06:53:56.207933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's do a simple exmaple where we compare results with the different features scaling techniques. We will remove the categorical data for this. \n\n#the model we will be using is K Nearest Neighbors which can use euclidean distance. \n\n#we will use year and odometer to predict price \n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n#noscaling \nneigh_am = KNeighborsRegressor(n_neighbors=3)\nneigh_am.fit(X_train.loc[:,['car_age','odometer']], y_train)\npred = neigh_am.predict(X_test.loc[:,['car_age','odometer']])\n\n#absolute max \nneigh_am = KNeighborsRegressor(n_neighbors=3)\nneigh_am.fit(df_am.loc[:,['car_age','odometer']], y_train)\nam_pred = neigh_am.predict(X_test.loc[:,['car_age','odometer']])\n\n#min max (should get same results)\nneigh_mm = KNeighborsRegressor(n_neighbors=3)\nneigh_mm.fit(df_min_max.loc[:,['car_age','odometer']], y_train)\nmm_pred = neigh_mm.predict(X_test.loc[:,['car_age','odometer']])\n\n#standard (z score)\nneigh_std = KNeighborsRegressor(n_neighbors=3)\nneigh_std.fit(df_std.loc[:,['car_age','odometer']], y_train)\nstd_pred = neigh_std.predict(X_test.loc[:,['car_age','odometer']])\n\n#robust scaler \nneigh_rob = KNeighborsRegressor(n_neighbors=3)\nneigh_rob.fit(df_rob.loc[:,['car_age','odometer']], y_train)\nrob_pred = neigh_rob.predict(X_test.loc[:,['car_age','odometer']])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:55:34.950887Z","iopub.execute_input":"2023-11-03T06:55:34.951369Z","iopub.status.idle":"2023-11-03T06:55:36.428225Z","shell.execute_reply.started":"2023-11-03T06:55:34.951334Z","shell.execute_reply":"2023-11-03T06:55:36.427279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('No Scaling: %.3f' % mean_absolute_error(y_test,pred))\nprint('Abosolute Max Score: %.3f' % mean_absolute_error(y_test,am_pred))\nprint('Min Max Score: %.3f' % mean_absolute_error(y_test,mm_pred))\nprint('Standard Scaling Score: %.3f' % mean_absolute_error(y_test,std_pred))\nprint('Robust Scaler Score: %.3f' % mean_absolute_error(y_test,rob_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:55:36.429862Z","iopub.execute_input":"2023-11-03T06:55:36.430415Z","iopub.status.idle":"2023-11-03T06:55:36.448122Z","shell.execute_reply.started":"2023-11-03T06:55:36.430381Z","shell.execute_reply":"2023-11-03T06:55:36.44675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformations \nA data transformation is the process of using a math expression to change the structure of our data. As we mentioned before, some models need data to fit a specific type of distribution for them to produce optimal results. Unfortunately, the data we get in the real world, doesn’t always fit the distributions our models call for. \n\nLet's look at the shape of our data and if it has any outliers before we do our transforms","metadata":{}},{"cell_type":"code","source":"# visual of the distribution of the odometer without any outlier removal (see boxplots above)\n#data is clearly impacted heavily by outliers \nprint(\"max odometer: \" + str(df_example['odometer'].max()))\nprint(\"median odometer: \" + str(df_example['odometer'].median()))\n\ndf_example['odometer'].hist(bins=50)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:55:49.371665Z","iopub.execute_input":"2023-11-03T06:55:49.372111Z","iopub.status.idle":"2023-11-03T06:55:49.681044Z","shell.execute_reply.started":"2023-11-03T06:55:49.372073Z","shell.execute_reply":"2023-11-03T06:55:49.679919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data after very basic oultier removal (kept only data < 99th percentile)\n#clear right skew in data \ndf_example[df['odometer']<df['odometer'].quantile(.99)]['odometer'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:33.565003Z","iopub.execute_input":"2023-11-03T06:56:33.565442Z","iopub.status.idle":"2023-11-03T06:56:33.994076Z","shell.execute_reply.started":"2023-11-03T06:56:33.56541Z","shell.execute_reply":"2023-11-03T06:56:33.992854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visual of the distribution of the odometer without any outlier removal (see boxplots above)\n#data is clearly impacted heavily by outliers \nprint(\"max price: \" + str(df_example['price'].max()))\nprint(\"median price: \" + str(df_example['price'].median()))\n\ndf_example['price'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:34.940055Z","iopub.execute_input":"2023-11-03T06:56:34.940545Z","iopub.status.idle":"2023-11-03T06:56:35.26281Z","shell.execute_reply.started":"2023-11-03T06:56:34.940497Z","shell.execute_reply":"2023-11-03T06:56:35.261563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data after very basic oultier removal (kept only data < 99th percentile)\n#clear right skew in data \n\ndf_example[df['price']<df['price'].quantile(.99)]['price'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:35.369001Z","iopub.execute_input":"2023-11-03T06:56:35.369666Z","iopub.status.idle":"2023-11-03T06:56:35.797433Z","shell.execute_reply.started":"2023-11-03T06:56:35.369628Z","shell.execute_reply":"2023-11-03T06:56:35.795258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's do some simple feature engineering to get how old the cars are\n\n#df_example['car_age'] = df_example['car_age'].max() - df_example['car_age']\n\nprint(\"max age: \" + str(df_example['car_age'].max()))\nprint(\"median age: \" + str(df_example['car_age'].median()))\n\nX_train['car_age'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:36.156232Z","iopub.execute_input":"2023-11-03T06:56:36.15671Z","iopub.status.idle":"2023-11-03T06:56:36.511009Z","shell.execute_reply.started":"2023-11-03T06:56:36.156671Z","shell.execute_reply":"2023-11-03T06:56:36.509758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logarithmic Transformation\nA very popular, common type of transformation is the log transformation. Log transformations fall under the family of power transformations. Typically, we apply logarithmic transformations to our variables when our variables are heavily right skewed, driven by a few outliers. \n\nLet's see how these transformations impact some of our skewed data (Odometer & Price)\n\n### Transforms we will cover\n- Logarithmic \n- Exponential\n- Square Root \n- Box-Cox","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\n\ndef log_transform(x):\n    return np.log(x + 1)\n\ntransformer_log = FunctionTransformer(log_transform)\ntransformed_log = transformer_log.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:41.187883Z","iopub.execute_input":"2023-11-03T06:56:41.188373Z","iopub.status.idle":"2023-11-03T06:56:41.403194Z","shell.execute_reply.started":"2023-11-03T06:56:41.188309Z","shell.execute_reply":"2023-11-03T06:56:41.401837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_logp = FunctionTransformer(log_transform)\ntransformed_logp = transformer_logp.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:41.916839Z","iopub.execute_input":"2023-11-03T06:56:41.917299Z","iopub.status.idle":"2023-11-03T06:56:41.925556Z","shell.execute_reply.started":"2023-11-03T06:56:41.917262Z","shell.execute_reply":"2023-11-03T06:56:41.924536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as you can see, using log transform in this case actually creates some right skew. \n#It does however almost completely normalize the outliers that were present\n\ntransformed_log['odometer'].hist(bins = 100)\n#X_train['odometer'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:45.312729Z","iopub.execute_input":"2023-11-03T06:56:45.313177Z","iopub.status.idle":"2023-11-03T06:56:46.056339Z","shell.execute_reply.started":"2023-11-03T06:56:45.31314Z","shell.execute_reply":"2023-11-03T06:56:46.054007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_log['car_age'].hist(bins = 20)\n#X_train['car_age'].hist(bins = 20)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:47.113176Z","iopub.execute_input":"2023-11-03T06:56:47.113635Z","iopub.status.idle":"2023-11-03T06:56:47.369571Z","shell.execute_reply.started":"2023-11-03T06:56:47.113597Z","shell.execute_reply":"2023-11-03T06:56:47.36836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as you can see, using log transform in this case actually creates some right skew. \n#It does however almost completely normalize the outliers that were present\n\ntransformed_logp.hist(bins =100)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:48.517637Z","iopub.execute_input":"2023-11-03T06:56:48.518837Z","iopub.status.idle":"2023-11-03T06:56:48.93434Z","shell.execute_reply.started":"2023-11-03T06:56:48.518787Z","shell.execute_reply":"2023-11-03T06:56:48.932078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Square Root Transform\nSquare/Square Root transformations will compress the spread of your larger values but spread out your lower values. Log transformations have a similar effect but are much more aggressive","metadata":{}},{"cell_type":"code","source":"def sqrt_transform(x):\n    return np.sqrt(x)\n\ntransformer_sqrt = FunctionTransformer(sqrt_transform)\ntransformed_sqrt = transformer_sqrt.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:50.978966Z","iopub.execute_input":"2023-11-03T06:56:50.980211Z","iopub.status.idle":"2023-11-03T06:56:51.16312Z","shell.execute_reply.started":"2023-11-03T06:56:50.980162Z","shell.execute_reply":"2023-11-03T06:56:51.161589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_sqrtp = FunctionTransformer(sqrt_transform)\ntransformed_sqrtp = transformer_sqrtp.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:51.304894Z","iopub.execute_input":"2023-11-03T06:56:51.305339Z","iopub.status.idle":"2023-11-03T06:56:51.311011Z","shell.execute_reply.started":"2023-11-03T06:56:51.305286Z","shell.execute_reply":"2023-11-03T06:56:51.310054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_sqrt['odometer'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:52.406038Z","iopub.execute_input":"2023-11-03T06:56:52.407776Z","iopub.status.idle":"2023-11-03T06:56:52.83275Z","shell.execute_reply.started":"2023-11-03T06:56:52.407717Z","shell.execute_reply":"2023-11-03T06:56:52.831437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_sqrt['car_age'].hist(bins = 20)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:53.711743Z","iopub.execute_input":"2023-11-03T06:56:53.712197Z","iopub.status.idle":"2023-11-03T06:56:53.986599Z","shell.execute_reply.started":"2023-11-03T06:56:53.712159Z","shell.execute_reply":"2023-11-03T06:56:53.98541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_sqrtp = FunctionTransformer(sqrt_transform)\ntransformed_sqrtp = transformer_sqrtp.fit_transform(y_train[y_train['price'] < y_train['price'].quantile(.99)])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:54.698678Z","iopub.execute_input":"2023-11-03T06:56:54.699515Z","iopub.status.idle":"2023-11-03T06:56:54.716306Z","shell.execute_reply.started":"2023-11-03T06:56:54.699476Z","shell.execute_reply":"2023-11-03T06:56:54.71459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_sqrtp.hist(bins=50) ","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:56.555807Z","iopub.execute_input":"2023-11-03T06:56:56.556609Z","iopub.status.idle":"2023-11-03T06:56:56.884899Z","shell.execute_reply.started":"2023-11-03T06:56:56.556567Z","shell.execute_reply":"2023-11-03T06:56:56.883735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exponential Transformation\nA close cousin of the log transform is the exponential transformation. There are many instances where you'd use an exponential transform:\n- Anytime you apply a log transform to your target variable, you can apply an exponential transformation to revert it back to the original value.\n- Log and Exponential transformations are the inverse of each other. You can use either to perform the same task. Whether you want a log-linear or linear-log model.\n- Use Exponential transformations when you wanto magnify small differences.","metadata":{}},{"cell_type":"code","source":"def exp_transform(x):\n    return np.exp(x)\n\ntransformer_exp = FunctionTransformer(exp_transform)\n\n## In our dataset, car age may be something we want to magnify\ntransformed_exp = X_train.copy()\n\ntransformed_exp['car_age'] = transformer_exp.fit_transform(transformed_exp['car_age'])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:57.963414Z","iopub.execute_input":"2023-11-03T06:56:57.96404Z","iopub.status.idle":"2023-11-03T06:56:57.976682Z","shell.execute_reply.started":"2023-11-03T06:56:57.964002Z","shell.execute_reply":"2023-11-03T06:56:57.975302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(X_train['car_age'])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:58.658413Z","iopub.execute_input":"2023-11-03T06:56:58.659166Z","iopub.status.idle":"2023-11-03T06:56:58.881953Z","shell.execute_reply.started":"2023-11-03T06:56:58.659112Z","shell.execute_reply":"2023-11-03T06:56:58.881037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## X and y-scale here are much larger\nplt.hist(transformed_exp['car_age'])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:56:59.506684Z","iopub.execute_input":"2023-11-03T06:56:59.507969Z","iopub.status.idle":"2023-11-03T06:56:59.743772Z","shell.execute_reply.started":"2023-11-03T06:56:59.507916Z","shell.execute_reply":"2023-11-03T06:56:59.742683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(transformed_exp['odometer'])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:57:00.378304Z","iopub.execute_input":"2023-11-03T06:57:00.379201Z","iopub.status.idle":"2023-11-03T06:57:00.627804Z","shell.execute_reply.started":"2023-11-03T06:57:00.379143Z","shell.execute_reply":"2023-11-03T06:57:00.626538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Box-Cox Transformation\nThe Box-Cox transformation is a transformation that helps your dataset follow a normal distribution. Typically, we use Box-Cox transformation when our dataset is not normal, but close to being normal. When we want to either run tests or generate significance from our dataset, Box-Cox transformation is a good option to transform our target variable so it resembles a normal distribution.\n\nBox-Cox aggregates multiple power transformers into a single transformer. You use lambda to adjust the transformation. Lambda varies from -5 to 5. If we set lambda equal to zero, it becomes simply a log transformation. ","metadata":{}},{"cell_type":"code","source":"## Redo the pipeline for this example\nfrom sklearn.model_selection import train_test_split\n\n## Clip Outliers\ndf_example = df_example[df_example['price'] < np.percentile(df_example['price'], 95)]\n\n## Remove prices that are 0 to make notebook work\ndf_example = df_example[df_example['price'] > 0].copy()\n\nX = df_example.drop('price',axis =1 )\ny = df_example[['price']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:57:01.403416Z","iopub.execute_input":"2023-11-03T06:57:01.403995Z","iopub.status.idle":"2023-11-03T06:57:01.722043Z","shell.execute_reply.started":"2023-11-03T06:57:01.403943Z","shell.execute_reply":"2023-11-03T06:57:01.721019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_train['price'])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:57:02.091954Z","iopub.execute_input":"2023-11-03T06:57:02.092716Z","iopub.status.idle":"2023-11-03T06:57:02.3345Z","shell.execute_reply.started":"2023-11-03T06:57:02.092661Z","shell.execute_reply":"2023-11-03T06:57:02.333415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll apply a boxcox transformation to make this dataset a bit more normal. Within scipy.stats, we can set lmbda = None and the boxcox function will find the lambda value that will maximize the log-likelihood function of the dataset:","metadata":{}},{"cell_type":"code","source":"from scipy.stats import boxcox\n\nboxcox_y_train = boxcox(y_train['price'], lmbda = None)\n\nplt.hist(boxcox_y_train[0])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:57:03.127271Z","iopub.execute_input":"2023-11-03T06:57:03.128134Z","iopub.status.idle":"2023-11-03T06:57:03.489828Z","shell.execute_reply.started":"2023-11-03T06:57:03.128089Z","shell.execute_reply":"2023-11-03T06:57:03.488521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Lambda Parameter {0}\".format(boxcox_y_train[1]))","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:57:03.674613Z","iopub.execute_input":"2023-11-03T06:57:03.675076Z","iopub.status.idle":"2023-11-03T06:57:03.681865Z","shell.execute_reply.started":"2023-11-03T06:57:03.67504Z","shell.execute_reply":"2023-11-03T06:57:03.680223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Interactions \nLike a chef remixing their ingredients, as a data scientist, we have a ton of different ways we can engineer features with our variables. Here are a few common methods:\n\n- Arethmetic Interaction (addition, subtraction, division, or multiplication of variables)\n- Binning (grouping variables in ranges)\n- Creative Features (alternative metrics for evaluation)","metadata":{}},{"cell_type":"markdown","source":"## Arethmetic Interaction\nWe actually already did some arethmetic interaction at the beginning of our analysis here. One of the earliest things we did was get the car age by taking the difference between the newest eyar and the year of each vehicle. While this is interaction with the variable itself, we can also take differences, ratios and mutliples of two or more variables. Let's try a few: ","metadata":{}},{"cell_type":"code","source":"#first example of getting car's age:\ndf['car_age'] = df['year'].max() - df['year']\n\n#let's look at price per mile. This could be a good way to normalize across different car brands \ndf['price_per_mile'] = df['price']/ df['odometer']\n\n#We can try these newly created features in our model to see if they produce better results.","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:57:05.422184Z","iopub.execute_input":"2023-11-03T06:57:05.422686Z","iopub.status.idle":"2023-11-03T06:57:05.436599Z","shell.execute_reply.started":"2023-11-03T06:57:05.42265Z","shell.execute_reply":"2023-11-03T06:57:05.435524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binning \nBinning allows us to group specific variables in a range. This can be useful if we know something specific or non-linear about the data at hand. For example, if most cars go out of warranty at 50,000 miles or after 5 years, we can create a varaible bin based on that. \n\nWe can also split data into multiple ranges if we would like to. ","metadata":{}},{"cell_type":"code","source":"#create warranty bin > 50,000 miles \n# we are using a lambda function here. This lets us write a function without defining it\n# we are also using a ternary operator which is an if, else statement in a single line. (explained in the full video)\n\ndf['warranty_miles'] = df['odometer'].apply(lambda x: 0 if (x > 50000 or np.isnan(x)) else 1)\ndf['warranty_age'] = df['car_age'].apply(lambda x: 0 if (x > 5 or np.isnan(x)) else 1)\n\n\n# We can also combine these together in a single statement by defining a function.\ndef warranty(miles, age):\n    if (miles > 50000 or age > 5):\n        return 0\n    else:\n        return 1\n    \ndf['warranty'] = df.apply(lambda x: min(x.warranty_miles,x.warranty_age), axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:57:06.864052Z","iopub.execute_input":"2023-11-03T06:57:06.864517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[:,['odometer','car_age','warranty_miles','warranty_age','warranty']].dropna().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We might also know that cars lose value non-linearly after 50k miles or 100k miles. \n#In this case, we may want to create bucks for <50k, 50k-100k, and 100k+ miles.\n#we did something similiar in the eda notebook located here: https://www.kaggle.com/code/kenjee/basic-eda-example\nbins = pd.IntervalIndex.from_tuples([(0, 50000), (50000, 100000), (100000,float(\"inf\"))])\ndf['mile_groups'] = pd.cut(df['odometer'],bins)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creative features \nOften, we have a good subject area understanding of our data domain. We might want to create features based on our understanding of the specific problem or domain. For cars, maybe we could create our own classification of imports or US manufactured cars that could help us predict pricing better. Maybe there is a car desirabity metric that you could create based on the other factors. Or maybe there is a way to look at the amount of similar cars close by to approximate demand. These are some potential ideas for you to implement yourself! \n\nAnother way to get creative features are to find more data and add it to your dataset. We could find a car sales website and scrape the average price of the cars in the market based on the make and model. ","metadata":{}},{"cell_type":"markdown","source":"# Summary\nIn this notebook, we covered the basics of feature scaling, transformations and interaction features. Working on these techiniques should help you to improve your models significantly! \n## Feature Scaling\n- Absolute Max Scaling\n- MinMax Scaling\n- Z-Score Normalization (Standard Scaler)\n- Robust Scaler \n## Transformations \n- Logarithmic \n- Square Root \n- Exponential\n- Box-Cox\n## Interaction Features\n- Arethmetic Interaction\n- Binning\n- Creative Features ","metadata":{}},{"cell_type":"markdown","source":"## Additional Resources\n- [About Feature Scaling and Normalization by Sebastian Raschka](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n- [Feature Scaling Techniques in Python – A Complete Guide by Eddie_4072](https://www.analyticsvidhya.com/blog/2021/05/feature-scaling-techniques-in-python-a-complete-guide/)\n- [Feature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization by Aniruddha Bhandari](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/#:~:text=Normalization%20is%20a%20scaling%20technique,known%20as%20Min%2DMax%20scaling.&text=Here%2C%20Xmax%20and%20Xmin%20are,values%20of%20the%20feature%20respectively.)\n- [Robust Scaler - Sklearn Docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)\n- [Log Transformation: Purpose and Interpretation by Kyaw Saw Htoon](https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9)\n- [Best exponential transformation to linearize your data with Scipy](https://towardsdatascience.com/best-exponential-transformation-to-linearize-your-data-with-scipy-cca6110313a6)\n- [Exponentially scaling your data in order to zoom in on small differences](https://rikunert.com/exponential_scaler)\n- [Box Cox Transformation by Ted Hessing](https://sixsigmastudyguide.com/box-cox-transformation/)\n- [Box-Cox Transformation and Target Variable: Explained](https://builtin.com/data-science/box-cox-transformation-target-variable)\n- [Additional Kaggle Example](https://www.kaggle.com/code/mysarahmadbhat/all-about-feature-scaling)","metadata":{}},{"cell_type":"markdown","source":"## Related Course Workbooks - Machine Learning Process A-Z\n- [**Dealing with Missing Values - Section 5.1**](https://www.kaggle.com/code/kenjee/dealing-with-missing-values-section-5-1)\n- [**Dealing with Outliers - Section 5.2**](https://www.kaggle.com/code/kenjee/dealing-with-outliers-section-5-2)\n- [**Basic EDA Example - Section 6**](https://www.kaggle.com/code/kenjee/basic-eda-example-section-6)\n- [**Categorical Feature Engineering - Section 7.1**](https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1)\n- [**Numeric Feature Engineering - Section 7.2**](https://www.kaggle.com/kenjee/numeric-feature-engineering-section-7-2)\n- [**Cross Validation Foundations - Section 8**](https://www.kaggle.com/code/kenjee/cross-validation-foundations-section-8)\n- [**Feature Selection - Section 9**](https://www.kaggle.com/code/kenjee/feature-selection-section-9)\n- [**Dealing with Imbalanced Data - Section 10**](https://www.kaggle.com/code/kenjee/dealing-with-imbalanced-data-section-10)\n- [**Model Building Example - Section 11**](https://www.kaggle.com/code/kenjee/model-building-example-section-11)\n- [**Model Evaluation (Classification) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-classification-section-12)\n- [**Model Evlauation (Regression) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-regression-12)","metadata":{}}]}
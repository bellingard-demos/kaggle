{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!unzip /content/glove.zip -d /content/drive/MyDrive/dataset/","metadata":{"id":"hDenoN2NfxVD","outputId":"010fd5d7-f5c0-401e-abb4-3df61e69b3e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Activation,Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM","metadata":{"id":"bY07ZWAeMrpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/content/drive/MyDrive/dataset/cleaned_train.csv\")\ndf.columns","metadata":{"id":"I5JWMlEMPxdz","outputId":"a9a361af-f9b5-426e-9f12-35cbf4ef2178"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['text','location'],axis=1,inplace=True)","metadata":{"id":"y269-T6iQELr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.keyword.fillna('unknown',inplace=True)\ndf.isnull().sum()","metadata":{"id":"o8uJlGQdQN95","outputId":"eee759c7-adf7-4f02-8741-f96dd517abc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.read_csv(\"/content/drive/MyDrive/dataset/cleaned_test.csv\")\ndf1.columns","metadata":{"id":"P1Pcj00DPxuS","outputId":"05dce88f-9e3b-46a5-ac8f-f1bb12f96c56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.drop(['text','location'],axis=1,inplace=True)","metadata":{"id":"6DA1gZ_aQjr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.keyword.fillna('unknown',inplace=True)\ndf1.isnull().sum()","metadata":{"id":"AYOr0eZBQjxE","outputId":"4a4aa0b5-4fe1-4600-f6f0-97edc092f6e6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"creation of vocabulary","metadata":{"id":"E_73v8qDQt1-"}},{"cell_type":"code","source":"# Bag of Words model\nfrom keras.preprocessing.text import Tokenizer\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","metadata":{"id":"kJLBsYPHPqH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets use only tweet text to build the model\nX = df.text_cleaned\ny = df.target","metadata":{"id":"12lHwF81S4wx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"id":"p_D55h26TZQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create and apply tokenizer on the training dataset\ntokenizer = create_tokenizer(X_train)\nX_train_set = tokenizer.texts_to_matrix(X_train, mode = 'freq')","metadata":{"id":"CJ80kMmdTl3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the model\ndef define_model(n_words):\n    # define network\n    model = Sequential()\n    model.add(Dense(128, input_shape=(n_words,), activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [get_score])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","metadata":{"id":"Wyb87ftYTsC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to calculate f1 score for each epoch\nimport keras.backend as K\ndef get_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","metadata":{"id":"jeaBTrJnUFzo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the model\nn_words = X_train_set.shape[1]\nmodel = define_model(n_words)","metadata":{"id":"iYpF_VJFT75g","outputId":"35a44d49-2c68-4c62-caa8-c852d4d406ee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fit network\nmodel.fit(X_train_set,y_train,epochs=10,verbose=2)","metadata":{"id":"ID4ZMUcVUA3A","outputId":"a5a89d0b-4421-494b-d18c-cc9671c2bbe6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction on the test dataset\nX_test_set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\n#y_pred = model.predict_classes(X_test_set)\ny_pred1=model.predict(X_test_set) \ny_pred=np.argmax(y_pred1,axis=1)","metadata":{"id":"BCIPAz6JUPtd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# important metrices\nprint(classification_report(y_test, y_pred))","metadata":{"id":"2dOWwlLvUXiz","outputId":"88c69fe1-fae2-4337-b694-11db3415f526"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply tokenizer on the test dataset\ntest_set1 = tokenizer.texts_to_matrix(df1.text_cleaned, mode = 'freq')","metadata":{"id":"DmMZnYecU7y5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions on the test dataset\n#y_test_pred = model.predict_classes(test_set)\ny_test_pred2=model.predict(test_set1) \ny_test_pred3=np.argmax(y_test_pred2,axis=1)","metadata":{"id":"Gv-tpoboVLKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id=df1.id","metadata":{"id":"_g-kBM7HVYKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_id)","metadata":{"id":"CLun8WHsVvAq","outputId":"b0ae1485-4527-4dd5-a0f6-0340479f6565"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred3\nsub.to_csv('submission_1.csv',index=False)","metadata":{"id":"2BDyu5Y8VLGh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GLOVE with KERAS Word Embeddings","metadata":{"id":"huOR-pyNWlAt"}},{"cell_type":"code","source":"# Fitting a tokenizer on text will create a list of unique words with an integer assigned to it\ntoken = Tokenizer()\ntoken.fit_on_texts(X_train.tolist())","metadata":{"id":"lv5t3cgtVLDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets save the size of the vocab\nvocab_size = len(token.word_index) + 1","metadata":{"id":"mF5LIwSYVLAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('/content/drive/MyDrive/dataset/glove.6B.100d.txt', mode='rt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","metadata":{"id":"uATRGfoKVK84","outputId":"225e835b-825b-4076-e394-989faf3047f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will now perform the encoding\nencoded_docs = token.texts_to_sequences(X_train.tolist())\n\n# embedding layer require all the encoded sequences to be of the same length, lets take max lenght as 100\n# and apply padding on the sequences which are of lower size\nmax_length = 100\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","metadata":{"id":"o-ETbI1jZqs6","outputId":"170be4f3-ff04-40c2-dc88-4e47ef5a1a5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a weight matrix for words in training docs\nmis_spelled = []\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in token.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        mis_spelled.append(word)","metadata":{"id":"gFX3OWp0ZvTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check how many words are not spelled correctly \nlen(mis_spelled)","metadata":{"id":"JWKwI14vZvEZ","outputId":"a4d69ab8-270c-4feb-dcce-b2705bbb86e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define model\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[get_score])\n# summarize the model\nmodel.summary()\n# fit the model\nmodel.fit(padded_docs, y_train, epochs=50, verbose=0)","metadata":{"id":"tJNFnfP_ZzuO","outputId":"09cf5c6b-44f9-43d3-fefa-caf23bf9d6a2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)","metadata":{"id":"qPf8vgvzZ403"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy)","metadata":{"id":"we2FzsTGZ7GM","outputId":"995c3177-26ef-4870-d9a3-fdf26a00f730"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_docs = token.texts_to_sequences(X_test.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","metadata":{"id":"RWUGaAHMZ7C4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction on the test dataset\npredict_x=model.predict(padded_docs) \ny_pred=np.argmax(predict_x,axis=1)","metadata":{"id":"-GON0SnOZ6_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_docs = token.texts_to_sequences(df1.text_cleaned.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","metadata":{"id":"esdp6yLuaAsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_y = model.predict(padded_docs)\ny_test_pred=np.argmax(predict_y,axis=1)","metadata":{"id":"l94LmlZjaAqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_2.csv',index=False)","metadata":{"id":"Dig1YgR3aAju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CNN with word Embeddings","metadata":{"id":"MzDIFOAtciq2"}},{"cell_type":"code","source":"max_length = max([len(s) for s in df.text_cleaned])\nprint('Maximum length: %d' % max_length)","metadata":{"id":"UWHKdsiqaAWb","outputId":"6d92a389-3047-4af1-dc5c-3036cffecaf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will now perform the encoding\nencoded_docs = token.texts_to_sequences(X_train.tolist())\n\n# embedding layer require all the encoded sequences to be of the same length, lets take max lenght as 100\n# and apply padding on the sequences which are of lower size\n\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","metadata":{"id":"xY1q_Is7c3m8","outputId":"cc00bc0d-96c4-4c8f-b3ae-c61c53912fee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the model\ndef define_model(vocab_size, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, input_length=max_length))\n    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","metadata":{"id":"S76-cDnOc3iR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define model\nmodel = define_model(vocab_size, max_length)\n# fit network\nmodel.fit(padded_docs, y_train, epochs=10, verbose=2)","metadata":{"id":"l7htT5Ffc3fG","outputId":"4910da76-749b-49c8-af06-4269b0bb73f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)","metadata":{"id":"Bq4nGSzxc3cJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy)\n","metadata":{"id":"CLPIAfmedV8b","outputId":"33b45c7d-7b9c-469f-b4c5-dffa2594e6bf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_docs = token.texts_to_sequences(X_test.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n# prediction on the test dataset","metadata":{"id":"2NBM_ybAdV4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(padded_docs)\ny_pred=np.argmax(preds,axis=1)","metadata":{"id":"RIgc7y1phrlS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nencoded_docs = token.texts_to_sequences(df1.text_cleaned.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\ny_test_pred1 = model.predict(padded_docs)\ny_test_pred=np.argmax(y_test_pred1,axis=1)","metadata":{"id":"juxbbk4BdkCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_cnn.csv',index=False)","metadata":{"id":"vMMI5Lrbdlg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"RTwrR_e8iGS8"},"execution_count":null,"outputs":[]}]}
{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IASA Natural Language Processing Workshop (Part 1)\n\nThe purpose of this workshop is a general overview of various techniques and methods of text analytics, and in particular natural language processing from classical approaches to modern state of the art ones. After completing this course attentive students will have a solid general understanding of the NLP domain and will be able to implement various basic algorithms for text data processing. However, for a deeper understanding, a more detailed study of proposed materials will be necessary. \n\nThese particular materials were created by <a href=\"https://www.kaggle.com/abazdyrev\">Anton Bazdyrev</a> and <a href=\"https://www.kaggle.com/yakuben\">Oleksii Yakubenko</a> for IASA students and inspired by <a href=\"https://mlcourse.ai/\">MLCOURSE.AI</a> and <a href=\"https://ods.ai/\">ODS.AI</a> by <a href=\"https://www.kaggle.com/kashnitsky\">\nYury Kashnitsky</a>."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Mining and NLP roadmaps and tasks overview\n## 1. Low Level Tasks\n\n1. Tokenization <br>\n    Goal: Split given sentense to sequence of tokens.\n    \n2. Sentence boundary detection <br>\n    Goal: Split given text to sequence of sentences."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/tokenization.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Text Meaning/Representation Tasks\n\nGoal: represent meaning and context <br>\nApplications: creation of words and sentence embeddings, finding similar words (similar vectors) <br>\nRepresentation: word vectors, the mapping of words to vectors (n-dimensional numeric vectors) aka embeddings\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/embeddings.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Text Classification Tasks\n\nGoal: predict classes (categories) for given text. <br>\nApplications: spam detection, toxic detection, domain classification, importance detection. <br>\nRepresentation: bag of words, count and tf-idf vectorizers (does not preserve word order), embeddings (preserve word order)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/classification.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Sequence Processing Tasks\n\nGoal: language modeling - predict next/previous word(s), text generation, sequence labeling <br>\nApplications: sequence tagging (predict POS tags for each word in sequence), named entity recognition <br>\nRepresentation: sequences of embedding vectors of words (preserves word order) "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/ner.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Sequence to Sequence Tasks\n\nGoal: generate text for some given text <br>\nApplications: machine translation, chatbots, Q&A systems, summarization <br>\nRepresentation: sequences of embedding vectors of words (preserves word order)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/seq2seq-teacher-forcing.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Roadmaps:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/textmining.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/nlp.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task: Real or Not? NLP with Disaster Tweets\n<a href=\"https://www.kaggle.com/c/nlp-getting-started/overview\">Kaggle Competition</a> <br>\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image('../input/nlp-lectures-images/tweet_screenshot.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n\nIn this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive."},{"metadata":{},"cell_type":"markdown","source":"## Data Loading\nWhat am I predicting?<br>\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv', index_col='id')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns=['keyword', 'location'])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\n\ntrain['target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_samples = train[train['target'] == 1]\nfake_samples = train[train['target'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for sample in disaster_samples['text'].sample(3, random_state=42):\n    print(sample)\n    print('\\n=======\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for sample in fake_samples['text'].sample(3, random_state=42):\n    print(sample)\n    print('\\n=======\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = disaster_samples['text'].values\n\nwc = WordCloud(max_font_size=60, background_color=\"black\", max_words=2000, stopwords=STOPWORDS)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(12,6))\nplt.axis(\"off\")\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17),interpolation=\"bilinear\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = fake_samples['text'].values\n\nwc = WordCloud(max_font_size=60, background_color=\"black\", max_words=2000, stopwords=STOPWORDS)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(12,6))\nplt.axis(\"off\")\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17),interpolation=\"bilinear\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline solutions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trivial Solution"},{"metadata":{"trusted":true},"cell_type":"code","source":"constant_prediction = [0 for text in train['text']]\n\nconstant_accuracy_score = accuracy_score(train['target'], constant_prediction)\nprint(f'accuracy_score : {constant_accuracy_score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Popular words differences"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ndisaster_text = ' '.join(disaster_samples['text'].tolist())\ndisaster_words = disaster_text.split()\ndisaster_count = Counter(disaster_words)\ndisaster_count = pd.Series(disaster_count)\ndisaster_count = disaster_count.sort_values(ascending=False)\n\nfake_text = ' '.join(fake_samples['text'].tolist())\nfake_words = fake_text.split()\nfake_count = Counter(fake_words)\nfake_count = pd.Series(fake_count)\nfake_count = fake_count.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_count.iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing stopwords\ndisaster_count[~disaster_count.index.str.lower().isin(STOPWORDS)].iloc[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Disaster', disaster_count[~disaster_count.index.str.lower().isin(STOPWORDS)].iloc[:30].index)\nprint('Fake', fake_count[~fake_count.index.str.lower().isin(STOPWORDS)].iloc[:30].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems that if we have words like fire, killed, suicide, disaster, bomb, crash, families, police, buildings, fatal, train, burning and so on in text it means that they re rather disaster than fake."},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_words = ['fire', 'killed', 'suicide', 'disaster', 'bomb', 'crash', 'families', 'police', 'buildings', 'fatal', 'train', 'burning']\n\nbasic_prediction = [any([d_w in text.lower() for d_w in disaster_words]) for text in train['text']]\n\nbasic_accuracy_score = accuracy_score(train['target'], basic_prediction)\nprint(f'accuracy_score : {basic_accuracy_score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Approach\n![mlvd](https://github.com/mephistopheies/mlworkshop39_042017/raw/a6426fd652faa38864c3ea4538e000539106fb56/1_ml_intro/ipy/images/bengio.png)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/nlp-lectures-images/machine_learning_overview.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Supervised learning\n\n### Model\n\n*Model* - parametric space of functions (hypotheses):\n\n$\\large \\mathcal{H} = \\left\\{ h\\left(x, \\theta\\right) | \\theta \\in \\Theta \\right\\}$\n\n* where\n    * $\\large h: X \\times \\Theta \\rightarrow Y$    \n    * $\\large \\Theta$ - parametric space\n    * $\\large X$ - factor space (exogenous variables)\n    * $\\large Y$ - target space\n    \n### Training algorithm\n\n\n*Training algorithm* - map from data space to hypotheses space:\n\n$\\large \\mathcal{M}: X \\times Y \\rightarrow \\mathcal{H}$\n\nThere are 2 steps in algorithm:\n1. Selection of hypothesis: $\\large h = \\mathcal{M}\\left(D\\right)$, where $\\large D$ - our particular dataset\n2. Testing for given example $\\large x$ calculation of model prediction $\\large \\hat{y} = h\\left(x\\right)$\n\n### Selection of hypothesis\n\nDefine *loss function*:\n$\\large L: Y \\times Y \\rightarrow \\mathbb{R}$ <br>\n\nWith loss function we can measure how the prediction $\\large \\hat {y}$ differs from the ground truth values $\\large y$.\n\n$\\large Q_{\\text{emp}}\\left(h\\right) = \\frac{1}{n} \\sum_{i=1}^n L\\left(h\\left(x_i\\right), y_i\\right)$, \nwhere $\\large \\mathcal{D} = \\left\\{ \\left(x_i, y_i\\right) \\right\\}$ - our training dataset, $\\large h$ - hypothesis (function)\n\nWe should select hypothesis, that minimizes average loss:\n$\\large \\hat{h} = \\arg \\min_{h \\in \\mathcal{H}} Q_{\\text{emp}}\\left(h\\right)$\n\nExamples of loss functions:\n* classification: $\\large L\\left(\\hat{y}, y\\right) = \\text{I}\\left[\\hat{y} = y\\right]$\n* regression: $\\large L\\left(\\hat{y}, y\\right) = \\left(\\hat{y} - y\\right)^2$\n\n## Example: Logistic Regression\n\n\n### Model\nGiven $\\large \\mathcal{D} = \\left\\{ \\left(x_i, y_i\\right) \\right\\}$ - our training dataset, $\\large \\mathcal{H} = \\left\\{ h\\left(x, \\theta\\right) | \\theta \\in R^n \\right\\}$ - logistic regression model\n\n* where\n    * $\\large x_i \\in  R^n$    \n    * $\\large y_i \\in  \\left\\{0, 1\\right\\}$\n    * $\\large h(x_i, \\theta) = \\sigma \\left(\\theta ^ T x_i \\right) = \\dfrac{1}{1 + e ^ \\left( - \\theta ^ T x_i \\right)} = \\dfrac{1}{1 + e ^ \\left( - \\sum_{j=1}^n \\theta_j x_{ij} \\right)},$ $\\large h(x_i, \\theta) \\in \\left(0, 1\\right)$\n    * $\\sigma \\left(z\\right) = \\dfrac{1}{1 + e ^ \\left(-z \\right)}$ - logistic function, aka sigmoid\n    \n### Loss function\n$y_i \\sim Bernoulli(p)$, where p is an unknown parameter <br>\nLet's estimate conditional probability $Pr\\left\\{y_i = 1 | x_i \\right\\} = \\large h(x_i, \\theta)$ with Maximum Likelihood Estimation.<br> Obviously, that $Pr\\left\\{y_i = 0 | x_i \\right\\} = 1 - \\large h(x_i, \\theta)$ <br>\nAlso we can define $Pr\\left\\{y = y_i | x_i \\right\\} = \\large h(x_i, \\theta)^{y_i} \\left(1 - \\large h(x_i, \\theta)\\right)^{1 - y_i}$ <br> \nWe now calculate the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed: <br>\n\n$L\\left(\\theta | x\\right) = \\prod_{i=1}^m Pr\\left\\{y = y_i | x_i \\right\\} = \\prod_{i=1}^m h\\left(x_i, \\theta\\right)^{y_i} \\left(1 - h\\left(x_i, \\theta\\right)\\right)^{1 - y_i}$ <br>\n\n\nTypically, the log likelihood is maximized: \n\n$log\\left(L\\left(\\theta | x\\right) \\right) = log\\left(\\prod_{i=1}^m Pr\\left\\{y = y_i | x_i \\right\\}\\right) = log\\left(\\prod_{i=1}^m h\\left(x_i, \\theta\\right)^{y_i} \\left(1 - h\\left(x_i, \\theta\\right)\\right)^{1 - y_i}\\right) = \\sum_{i=1}^m log\\left(h\\left(x_i, \\theta\\right)^{y_i} \\left(1 - h\\left(x_i, \\theta\\right)\\right)^{1 - y_i}\\right) = \\sum_{i=1}^m y_ilog\\left(h\\left(x_i, \\theta\\right) \\right) + \\left(1 - y_i\\right) log\\left(1 - h\\left(x_i, \\theta\\right) \\right)$ <br>\n\nSo, finally, we can define our classification loss function as $\\large L\\left(\\hat{y}, y\\right) = - ylog\\left(\\hat{y} \\right) - \\left(1 - y\\right) log\\left(1 - \\hat{y} \\right)$ where $\\hat{y} = h\\left(x_i, \\theta\\right) $\n\nNow, when we have dataset, model and loss function, we can perform training and find the best estimation $\\large \\hat{h} =\\arg \\min_{h \\in \\mathcal{H}} \\left(\\frac{1}{n} \\sum_{i=1}^n L\\left(h\\left(x_i\\right), y_i\\right)\\right)$ <br>\n\n### Optimization\nWe can find it using Gradient Descent method: <br>\nGradient descent is based on the observation that if the multi-variable function $\\large F$ is defined and differentiable in a neighborhood of a point $\\large a$, then $\\large F$ decreases fastest if one goes from a in the direction of the negative gradient of $\\large F$ at $\\large a$. <br>\n\nDefine $\\vec{w_{n + 1}} = \\vec{w_{n}} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} \\left(\\vec{w_{n}}\\right)$, where $\\alpha$ is a small number, called learning rate. <br>\n\nIf $\\vec{w_{n}}$ converges, then it converges to local minimum. If $\\mathcal{L}$ is a convex function, then $\\vec{w_{n}}$ converges to the global minimum.\n\nIn our case of optimization our loss function (log-loss) for logistic regression model: <br>\n\n$$\\large \\begin{array}{rcl} - \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}}\\sum_{i=1}^n y_i \\ln \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right) + \\left(1 - y_i\\right) \\ln \\left(1 - \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right)\\right) \\\\\n&=& \\sum_{i=1}^n y_i \\frac{1}{\\sigma} \\sigma \\left(1 - \\sigma\\right) \\vec{x}_i + \\left(1 - y_i\\right) \\frac{1}{1 - \\sigma} \\left(-1\\right)\\sigma \\left(1 - \\sigma\\right) \\vec{x}_i \\\\\n&=& \\sum_{i=1}^n y_i \\left(1 - \\sigma\\right) \\vec{x}_i - \\left(1 - y_i\\right) \\sigma \\vec{x}_i \\\\\n&=& \\sum_{i=1}^n \\vec{x}_i \\left(y_i - \\sigma\\right)\n\\end{array}$$"},{"metadata":{},"cell_type":"markdown","source":"# Validation\n\nIf we have a few different models how can we define wich one performs better for our task? <br>\nWe can measure the quality of predictions using some metric or loss functions. <br>\n\nLet's look through the example: <br>\nWe have regression task with target variable y. We can observe x0, x1 and we assume that we can estimate y as function of x0, x1. Also, we know that there may be some \"noise\" in our data and also there may also be other factors that affect the target y. We have 100 examples in training set and 20 examples in evaluation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(42)\n\nX_train = np.random.normal(0, 1, (500, 2))\nnoise_train = np.random.normal(0, 1., 500)\n\nX_test = np.random.normal(0, 1, (100, 2))\nnoise_test = np.random.normal(0, 1., 100)\n\ny_train = (np.sin(X_train) + np.cos(2*X_train)).sum(axis=1) + noise_train\ny_test = (np.sin(X_test) + np.cos(2*X_test)).sum(axis=1) + noise_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's build some simple Linear Regression model and consider Mean Squared Error as a target metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\ny_train_preds = lr.predict(X_train)\ny_test_preds = lr.predict(X_test)\n\nprint(f'MSE on train: {mean_squared_error(y_train, y_train_preds)}\\nMSE on test: {mean_squared_error(y_test, y_test_preds)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maybe we can improve results by adding comlexity such as polynomial features to our model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\n\nX_train_poly = np.hstack([X_train, X_train**2])\nX_test_poly = np.hstack([X_test, X_test**2])\n\nlr.fit(X_train_poly, y_train)\n\ny_train_preds = lr.predict(X_train_poly)\ny_test_preds = lr.predict(X_test_poly)\n\nprint(f'MSE on train: {mean_squared_error(y_train, y_train_preds)}\\nMSE on test: {mean_squared_error(y_test, y_test_preds)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! Thats great, we've got some improvements by just adding squared features to our model. But can we improve it more? Let's add more powers up to 10!\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\n\nX_train_poly = np.hstack([X_train**(p) for p in range(10)])\nX_test_poly = np.hstack([X_test**(p) for p in range(10)])\n\nlr.fit(X_train_poly, y_train)\n\ny_train_preds = lr.predict(X_train_poly)\ny_test_preds = lr.predict(X_test_poly)\n\nprint(f'MSE on train: {mean_squared_error(y_train, y_train_preds)}\\nMSE on test: {mean_squared_error(y_test, y_test_preds)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmmm... That looks really weird isn't it? Why we've got huge improvements on train and such a terrible result on test?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ntrain_result = []\ntest_result = []\nfor pow_n in range(1, 10):\n    lr = LinearRegression()\n\n    X_train_poly = np.hstack([X_train**(p) for p in range(pow_n)])\n    X_test_poly = np.hstack([X_test**(p) for p in range(pow_n)])\n\n    lr.fit(X_train_poly, y_train)\n\n    y_train_preds = lr.predict(X_train_poly)\n    y_test_preds = lr.predict(X_test_poly)\n    \n    train_result.append(mean_squared_error(y_train, y_train_preds))\n    test_result.append(mean_squared_error(y_test, y_test_preds))\n    \nplt.plot(list(range(1, 10)), train_result, list(range(1, 10)), test_result)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In fact, the thing is, that when we use more complex models, then we can \"explain\" functional dependency in our training data, even if we actually don't have it. In real world we always face situations when we have such kind of \"noise\" in real training data, so very important thing in model validation is to validate it using unseen in train phase data."},{"metadata":{},"cell_type":"markdown","source":"# Back to the original task: Modeling and Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image('../input/nlp-lectures-images/countvectorizer.png')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image('../input/nlp-lectures-images/countvectorizer2.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split to train test for validation purposes\nX, y = train['text'], train['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#init model\ncount_vectorizer = CountVectorizer(binary=True)\nlog_reg = LogisticRegression(solver='liblinear', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit model\ncount_vectorizer.fit(X_train)\nX_train_vectorized = count_vectorizer.transform(X_train)\n\nlog_reg.fit(X_train_vectorized, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction\n\nX_test_vectorized = count_vectorizer.transform(X_test)\n\nprediction_train = log_reg.predict(X_train_vectorized)\nprediction_test = log_reg.predict(X_test_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_train, prediction_train), accuracy_score(y_test, prediction_test),","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, as we've writen before, there can be actually a huge difference between results on train and test data! And we consider test results as a benchmark ones."},{"metadata":{},"cell_type":"markdown","source":"# Scikit-Learn Pipeline API"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image('../input/nlp-lectures-images/pipeline.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\n\n#init\ncount_vectorizer = CountVectorizer()\nlog_reg = LogisticRegression(solver='liblinear', random_state=42)\n\nmodel = Pipeline([('count_vectorizer', count_vectorizer),  ('log_reg', log_reg)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit\nmodel.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict\naccuracy_score(y_test, model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Improvement\nLet's calculate tf-idf measure instead of simple counts for each term"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image('../input/nlp-lectures-images/tf_idf.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### TF-IDF calculation example:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image('../input/nlp-lectures-images/tf-idf-example.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also finetune some parameters for vectorizer in scikit-learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nword_vectorizer = TfidfVectorizer(\n    analyzer='word',\n    stop_words='english',\n    ngram_range=(1, 3),\n    lowercase=True,\n    min_df=5,\n    max_features=30000)\n\nchar_vectorizer = TfidfVectorizer(\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(3, 6),\n    lowercase=True,\n    min_df=5,\n    max_features=50000)\n\n\nlog_reg = LogisticRegression(solver='liblinear', random_state=42)\n\nfu = FeatureUnion([('word_vectorizer', word_vectorizer),  ('char_vectorizer', char_vectorizer)])\nmodel = Pipeline([('vectorizers', fu),  ('log_reg', log_reg)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Explainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\n\nlog_reg = model.named_steps['log_reg']\nvectorizers = model.named_steps['vectorizers']\nfeature_names = vectorizers.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.explain_weights(log_reg, feature_names=feature_names, top=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.iloc[18])\neli5.show_prediction(log_reg, X_test.iloc[18], vec=vectorizers, feature_names=feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final thoughts\n\nFor various text classification tasks we can achive pretty decent results without any domain knowledge, comlicated algorithms and powerful computational resources. We need just a few lines of code for scikit-learn pipeline: TF-IDF Vectorizer -> Logistic Regression, that is very easy to use."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
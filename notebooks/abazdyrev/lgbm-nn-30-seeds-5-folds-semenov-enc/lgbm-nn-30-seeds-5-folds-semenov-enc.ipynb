{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_BOOST_ROUNDS = 10000\nEARLY_STOPPING = 500\nNUM_FOLDS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\n\ndf = pd.read_csv('/kaggle/input/infopulsehackathon/train.csv', index_col='Id')\nCATEGORICAL_FEATURES = df.columns[(df.dtypes != float) & (df.nunique() <= 12) & (df.nunique() >= 4)]\n\ndef create_cat_encode(X, X_val, random_state=42, reg=10, categorical_features=CATEGORICAL_FEATURES):\n    skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=random_state)\n    bins = np.linspace(0, X.shape[0], 10)\n    y_binned = np.digitize(X['Energy_consumption'], bins)\n    \n    for f in categorical_features:\n        X[f'target_encode_{f}'] = X['Energy_consumption'].mean()\n        \n    for train_idx, test_idx in skf.split(X, y_binned):\n        abs_mean_value = X.iloc[train_idx]['Energy_consumption'].mean()\n        for f in categorical_features:\n            group_obf = X.iloc[train_idx].groupby(f)['Energy_consumption']\n            group_mean = group_obf.mean().to_dict()\n            group_count = group_obf.count().to_dict()\n            X.loc[test_idx, f'target_encode_{f}'] = X.iloc[test_idx][f].apply(lambda x: \n                (group_mean.get(x, abs_mean_value)*group_count.get(x, 0) + abs_mean_value*reg)/(group_count.get(x, 0) + reg))\n\n    \n    abs_mean_value = X['Energy_consumption'].mean()\n    for f in categorical_features:\n        X_val[f'target_encode_{f}'] = abs_mean_value\n        group_obf = X.groupby(f)['Energy_consumption']\n        group_mean = group_obf.mean().to_dict()\n        group_count = group_obf.count().to_dict()\n        X_val[f'target_encode_{f}'] = X_val[f].apply(lambda x: \n                (group_mean.get(x, abs_mean_value)*group_count.get(x, 0) + abs_mean_value*reg)/(group_count.get(x, 0) + reg))       \n    return X, X_val","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom tqdm import tqdm\nfrom copy import deepcopy\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plotImp(model, col_names , num = 20):\n    feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':col_names})\n    plt.figure(figsize=(40, 20))\n    sns.set(font_scale = 5)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:num])\n    \n    plt.title('LightGBM Features (avg over folds)')\n    plt.show()\n\nclass MyRegressor(object):\n    def __init__(self, ml_params, categoricals, cols_to_drop=[], tgt_variable='Energy_consumption'):\n        self.ml = None\n        self.ml_params = ml_params\n        \n        self.tgt_variable = tgt_variable\n        self.categoricals = categoricals\n        self.cols_to_drop = cols_to_drop\n        \n    def fit(self, X, X_val=None, plot_feature_imp=False):        \n        y = X[self.tgt_variable]\n        X = X.drop(columns=[self.tgt_variable] + self.cols_to_drop)\n        col_names = X.columns\n        \n        X = lgb.Dataset(X, y)        \n        if X_val is not None:\n            X_val = lgb.Dataset(X_val.drop(columns=[self.tgt_variable] + self.cols_to_drop), X_val[self.tgt_variable])\n            self.ml = lgb.train(self.ml_params,\n                                X,\n                                num_boost_round=MAX_BOOST_ROUNDS,\n                                valid_sets=(X, X_val),\n                                early_stopping_rounds=EARLY_STOPPING,\n                                verbose_eval = 500)\n        else:\n            self.ml = lgb.train(self.ml_params,\n                                X,\n                                valid_sets=(X),\n                                num_boost_round=MAX_BOOST_ROUNDS,\n                                verbose_eval = 500)\n        if plot_feature_imp:\n            plotImp(self.ml, col_names)\n            \n        return self\n    \n    def predict(self, X):\n        cols_to_drop = list(set(['row_id', self.tgt_variable] + self.cols_to_drop) & set(X.columns))\n            \n        return self.ml.predict(X.drop(columns=cols_to_drop))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport gc\nfrom copy import deepcopy\n\ndef time_val(data, model, metric=mean_squared_error, target_var_name='Energy_consumption', test_to_predict=None, random_state=42):\n    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=random_state)\n    bins = np.linspace(0, data.shape[0], 10)\n    y_binned = np.digitize(data[target_var_name], bins)\n    y_val_preds = deepcopy(data[target_var_name])\n    print('Starting Validation')\n    results = []\n    if test_to_predict is not None:\n        test_prediction = []\n    for train_idx, test_idx in kf.split(data, y_binned):\n        train_df, val_df = create_cat_encode(data.iloc[train_idx].reset_index(drop=True), data.iloc[test_idx].reset_index(drop=True), random_state=random_state)\n        print('New Itter')\n        model.fit(train_df, val_df)\n        pred = model.predict(val_df)\n        y_val_preds.iloc[test_idx] = pred\n        \n        train_with_ce, test_with_ce = create_cat_encode(data, test_to_predict, random_state=random_state)\n        if test_to_predict is not None:\n            test_prediction.append(model.predict(test_with_ce))\n            \n        itter_metric = metric(data.iloc[test_idx][target_var_name], pred)\n        print('Itter metric: '+str(itter_metric))\n        results.append(itter_metric)\n        \n        gc.collect()\n     \n    if test_to_predict is not None:\n        return results, sum(test_prediction)/NUM_FOLDS, y_val_preds\n    else:\n        return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgb_df():\n    train = pd.read_csv('/kaggle/input/infopulsehackathon/train.csv', index_col='Id')\n    test = pd.read_csv('/kaggle/input/infopulsehackathon/test.csv', index_col='Id')\n    def label_encode(train_df, test_df, feature_name):\n        map_dict = {k:i for i, k in enumerate(train_df[feature_name].unique())}\n        print(map_dict)\n        train_df[feature_name] = train_df[feature_name].map(map_dict)\n        test_df[feature_name] = test_df[feature_name].map(map_dict)\n        return train_df, test_df\n    \n    for f in ['feature_3', 'feature_4', 'feature_257', 'feature_258']:\n        train, test = label_encode(train, test, f)\n        \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef nn_df():\n    train = pd.read_csv('/kaggle/input/infopulsehackathon/train.csv', index_col='Id')\n    test = pd.read_csv('/kaggle/input/infopulsehackathon/test.csv', index_col='Id')\n    y_train = train.Energy_consumption\n    X_train = train.drop(columns=['Energy_consumption'])\n    X_test = test\n    \n    to_drop = (X_train.std() <= 0.01).index[(X_train.std() <= 0.01).values].values\n    X_train = X_train.drop(columns=to_drop)\n    X_test = X_test.drop(columns=to_drop)\n    \n    continuous_columns = list(X_train.columns[X_train.dtypes == float]) + list(X_train.columns[(X_train.apply(pd.Series.nunique) > 8) & (X_train.dtypes != object)])\n    continuous_columns = sorted(set(continuous_columns))\n    \n    scaler = StandardScaler()\n\n    X_train[continuous_columns] = pd.DataFrame(scaler.fit_transform(X_train[continuous_columns]), columns=continuous_columns)\n    X_test[continuous_columns] = pd.DataFrame(scaler.transform(X_test[continuous_columns]), columns=continuous_columns)\n    \n    categorical_columns = list(filter(lambda x: x not in continuous_columns, X_train.columns))\n    binary_columns = list(filter(lambda f: X_train[f].append(X_test[f]).nunique() <= 2, categorical_columns))\n    categorical_columns = list(filter(lambda x: x not in binary_columns, categorical_columns))\n    \n    for f in binary_columns:\n        le = LabelEncoder()\n        le.fit(X_train[f].append(X_test[f]))\n        X_train[f] = le.transform(X_train[f])\n        X_test[f] = le.transform(X_test[f])\n        \n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe_cols = categorical_columns\n\n    ohe_data = pd.DataFrame(ohe.fit_transform(X_train[ohe_cols]).toarray(), dtype=int)\n    X_train = pd.concat([X_train.drop(columns = ohe_cols), ohe_data], axis=1)\n\n    ohe_data = pd.DataFrame(ohe.transform(test[ohe_cols]).toarray(), dtype=int)\n    X_test = pd.concat([X_test.drop(columns = ohe_cols), ohe_data], axis=1)\n    \n    return X_train, y_train, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train, lgb_test = lgb_df()\nnn_X_train, nn_y_train, nn_X_test = nn_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, Dense, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam, Nadam\nfrom keras import callbacks\n\ndef create_model(inp_dim):\n    inps = Input(shape=(inp_dim,))\n    x = Dense(256, activation='relu')(inps)\n    x = Dropout(0.2)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1)(x)\n    model = Model(inputs=inps, outputs=x)\n    model.compile(\n        optimizer=Nadam(lr=1e-3),\n        loss=['mse']\n    )\n    #model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_cv(X_train, y_train, X_test, seed):\n    test_predictions = []\n    metric_results = []\n    cross_val_predicts = deepcopy(y_train)\n    \n    skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seed)\n    bins = np.linspace(0, y_train.shape[0], 10)\n    y_binned = np.digitize(y_train, bins)\n\n    for ind, (tr, val) in enumerate(skf.split(X_train, y_binned)):\n        X_tr = X_train.iloc[tr]\n        y_tr = y_train.iloc[tr]\n        X_vl = X_train.iloc[val]\n        y_vl = y_train.iloc[val]\n\n        model = create_model(X_train.shape[1])\n        es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=50, verbose=False, mode='auto', restore_best_weights=True)\n        rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, mode='auto', verbose=False)\n        model.fit(\n            X_tr, y_tr, epochs=500, batch_size=256, validation_data=(X_vl, y_vl), verbose=False, callbacks=[es, rlr]\n        )\n        test_predictions.append(model.predict(X_test).flatten())\n        cross_val_predicts.iloc[val] = model.predict(X_vl).flatten()\n        metric_results.append(mean_squared_error(y_vl, cross_val_predicts.iloc[val].values))\n        \n    return metric_results, sum(test_predictions)/NUM_FOLDS, cross_val_predicts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score\n\nall_predicts = []\nall_results = []\nall_seeds = [0, 42, 2906, 1999, 2019, 1204, 1207, 15, 3, 7, 29975, 77383, 95657, 58553, 34851, 31093, 69823, 46869, 98800,\n        5391, 2848, 34806, 96965, 9961, 62236, 1364, 88418, 14141, 14865, 530]\n\nfor seed in all_seeds:\n    boost_model = MyRegressor(ml_params={\n                \"objective\": \"regression\",\n                \"boosting\": \"gbdt\",\n                \"num_leaves\": 15,\n                'max_depth': 8,\n                \"learning_rate\": 0.01,\n                \"feature_fraction\": 0.6,\n                \"reg_lambda\": 2,\n                \"metric\": \"mse\",\n                'seed': seed,\n                'subsample_freq': 3,\n                'bagging_seed': seed,\n                'subsample': 0.6\n                }, categoricals=[])\n    \n    lgb_res, lgb_predicts, lgb_val_predicts = time_val(lgb_train, boost_model, test_to_predict=lgb_test, random_state=seed)\n    print(f'LGBM  Seed: {seed} Result: {round(np.mean(lgb_res),5)} +/- {round(np.std(lgb_res),5)}')\n    print(mean_squared_error(lgb_train.Energy_consumption, lgb_val_predicts))\n    \n    nn_res, nn_predicts, nn_val_predicts = nn_cv(nn_X_train, nn_y_train, nn_X_test, seed=seed)\n    print(f'NN  Seed: {seed} Result: {round(np.mean(nn_res),5)} +/- {round(np.std(nn_res),5)}')\n    print(mean_squared_error(lgb_train.Energy_consumption, nn_val_predicts))   \n    \n    weight_space = np.arange(0.5, 1.01, 0.01)\n    blend_results = []\n    for w in weight_space:\n        blend_results.append(mean_squared_error(lgb_train.Energy_consumption, w*lgb_val_predicts + (1-w)*nn_val_predicts))\n        \n    best_weight = weight_space[np.argmin(blend_results)]\n    print(min(blend_results))\n    print(best_weight)\n    best_weight = 0.7*best_weight + 0.3*0.8\n    \n    all_predicts.append(best_weight*lgb_predicts + (1 - best_weight)*nn_predicts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(all_results), np.std(all_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/infopulsehackathon/test.csv', index_col='Id')\ntest['Energy_consumption'] = sum(all_predicts)/len(all_predicts)\ntest.loc[test['Energy_consumption'] < lgb_train.Energy_consumption.min(), 'Energy_consumption'] = lgb_train.Energy_consumption.min()\ntest[['Energy_consumption']].to_csv('submission.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nsns.distplot(lgb_train.Energy_consumption, bins=30)\nsns.distplot(test.Energy_consumption, bins=30);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
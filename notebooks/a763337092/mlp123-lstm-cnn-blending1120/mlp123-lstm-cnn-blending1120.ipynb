{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport os\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA, FastICA, TruncatedSVD, FactorAnalysis, DictionaryLearning, LatentDirichletAllocation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntry:\n    from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nexcept:\n    from transformers import WarmupLinearSchedule, WarmupCosineSchedule\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nDEBUG = False\nIS_TRAIN = False\nDATA_PATH = '../input/lish-moa'\nCACHE_PATH = '../input/mlp1-torch-weights1109'\nif not os.path.exists(CACHE_PATH):\n    os.mkdir(CACHE_PATH)\ndata_process_file = f'{CACHE_PATH}/data_process.pkl'\n\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nhidden_size = 2048\n# FEAT_SAMPLE_RATE = 0.95\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n# SEED = [0]\nSEED = [0, 1, 2, 3, 4, 5, 6]\n\nif DEBUG:\n    NFOLDS = 2\n    EPOCHS = 2\n    SEED = [0]\n    CACHE_PATH = f'{CACHE_PATH}/debug'\n    if not os.path.exists(CACHE_PATH):\n        os.mkdir(CACHE_PATH)\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\ndef score_fn(y_true, y_pred):\n    losses = []\n    for idx in range(len(target_cols)):\n        loss = log_loss(y_true[:, idx], y_pred[:, idx])\n        losses.append(loss)\n    return np.mean(losses)\n\nseed_everything(seed=42)\n\nselected_cols = ['sig_id','cp_type','cp_time','cp_dose'\n                     ,'g-0','g-2','g-3','g-5','g-6','g-7','g-8'\n,'g-9','g-11','g-12','g-13','g-15','g-16','g-17','g-18','g-20','g-21'\n,'g-22','g-24','g-25','g-26','g-27','g-28','g-29','g-30','g-31','g-32'\n,'g-33','g-34','g-36','g-37','g-38','g-39','g-41','g-42','g-43','g-45'\n,'g-47','g-48','g-49','g-50','g-51','g-52','g-53','g-54','g-55','g-56'\n,'g-57','g-58','g-60','g-61','g-62','g-63','g-65','g-66','g-67','g-68'\n,'g-69','g-70','g-71','g-72','g-73','g-75','g-76','g-77','g-78','g-79'\n,'g-80','g-81','g-83','g-84','g-85','g-86','g-87','g-89','g-90','g-91'\n,'g-92','g-93','g-94','g-96','g-97','g-98','g-100','g-101','g-102','g-103'\n,'g-104','g-105','g-106','g-107','g-108','g-109','g-110','g-111','g-112'\n,'g-113','g-114','g-115','g-116','g-117','g-118','g-119','g-120','g-121'\n,'g-122','g-123','g-124','g-125','g-126','g-127','g-129','g-130','g-131'\n,'g-132','g-133','g-134','g-135','g-136','g-137','g-138','g-139','g-140'\n,'g-141','g-142','g-143','g-144','g-146','g-147','g-148','g-149','g-150'\n,'g-151','g-152','g-154','g-156','g-157','g-158','g-160','g-161','g-162'\n,'g-163','g-164','g-165','g-166','g-167','g-169','g-170','g-172','g-173'\n,'g-174','g-175','g-177','g-178','g-179','g-180','g-181','g-183','g-184'\n,'g-185','g-186','g-187','g-188','g-189','g-190','g-192','g-194','g-195'\n,'g-196','g-199','g-200','g-202','g-203','g-205','g-206','g-207','g-208'\n,'g-209','g-210','g-211','g-212','g-215','g-216','g-217','g-218','g-219'\n,'g-221','g-222','g-224','g-225','g-226','g-227','g-228','g-229','g-230'\n,'g-231','g-233','g-235','g-236','g-237','g-238','g-239','g-240','g-241'\n,'g-242','g-243','g-245','g-246','g-247','g-248','g-250','g-251','g-252'\n,'g-253','g-254','g-255','g-256','g-257','g-258','g-260','g-262','g-263'\n,'g-265','g-267','g-268','g-269','g-270','g-272','g-273','g-274','g-276'\n,'g-279','g-280','g-283','g-284','g-285','g-286','g-287','g-291','g-292'\n,'g-293','g-294','g-296','g-297','g-298','g-299','g-300','g-301','g-302'\n,'g-303','g-305','g-306','g-307','g-308','g-309','g-310','g-312','g-313'\n,'g-314','g-317','g-318','g-319','g-321','g-322','g-323','g-324','g-325'\n,'g-326','g-327','g-328','g-329','g-330','g-331','g-332','g-335','g-336'\n,'g-337','g-338','g-340','g-341','g-342','g-343','g-344','g-346','g-347'\n,'g-348','g-349','g-350','g-352','g-353','g-354','g-355','g-356','g-357'\n,'g-358','g-359','g-360','g-361','g-362','g-363','g-365','g-366','g-367'\n,'g-368','g-369','g-371','g-372','g-373','g-374','g-375','g-376','g-377'\n,'g-379','g-380','g-381','g-382','g-383','g-384','g-385','g-386','g-387'\n,'g-388','g-389','g-390','g-391','g-392','g-394','g-395','g-396','g-397'\n,'g-398','g-400','g-402','g-403','g-404','g-405','g-407','g-408','g-409'\n,'g-410','g-411','g-412','g-414','g-415','g-416','g-417','g-418','g-419'\n,'g-420','g-421','g-422','g-423','g-424','g-425','g-426','g-427','g-428'\n,'g-429','g-430','g-431','g-432','g-433','g-434','g-435','g-438','g-439'\n,'g-440','g-441','g-442','g-443','g-444','g-445','g-446','g-447','g-449'\n,'g-450','g-451','g-453','g-454','g-455','g-456','g-457','g-458','g-459'\n,'g-460','g-461','g-462','g-463','g-465','g-466','g-468','g-469','g-470'\n,'g-471','g-472','g-473','g-474','g-475','g-476','g-479','g-480','g-482'\n,'g-483','g-484','g-485','g-486','g-488','g-489','g-491','g-492','g-493'\n,'g-497','g-498','g-499','g-500','g-502','g-503','g-504','g-506','g-507'\n,'g-508','g-509','g-510','g-511','g-513','g-514','g-515','g-516','g-518'\n,'g-520','g-522','g-523','g-524','g-525','g-526','g-527','g-528','g-529'\n,'g-530','g-531','g-533','g-534','g-535','g-536','g-537','g-538','g-539'\n,'g-540','g-541','g-542','g-543','g-544','g-546','g-547','g-550','g-551'\n,'g-552','g-553','g-554','g-555','g-556','g-557','g-558','g-559','g-560'\n,'g-561','g-562','g-563','g-564','g-566','g-567','g-568','g-569','g-570'\n,'g-571','g-572','g-574','g-577','g-578','g-579','g-580','g-583','g-584'\n,'g-587','g-588','g-589','g-590','g-592','g-593','g-594','g-595','g-596'\n,'g-597','g-598','g-599','g-600','g-602','g-604','g-605','g-606','g-608'\n,'g-609','g-610','g-611','g-612','g-613','g-614','g-616','g-619','g-620'\n,'g-622','g-624','g-627','g-628','g-629','g-630','g-631','g-632','g-634'\n,'g-635','g-636','g-639','g-640','g-641','g-642','g-643','g-644','g-646'\n,'g-647','g-648','g-649','g-651','g-652','g-655','g-656','g-657','g-658'\n,'g-659','g-660','g-661','g-663','g-664','g-665','g-666','g-667','g-669'\n,'g-671','g-672','g-673','g-674','g-675','g-677','g-678','g-679','g-681'\n,'g-682','g-683','g-684','g-685','g-686','g-688','g-689','g-691','g-692'\n,'g-693','g-694','g-696','g-697','g-698','g-699','g-700','g-701','g-702'\n,'g-704','g-705','g-706','g-708','g-709','g-710','g-711','g-712','g-713'\n,'g-714','g-720','g-722','g-724','g-725','g-726','g-727','g-728','g-729'\n,'g-731','g-733','g-734','g-735','g-736','g-737','g-738','g-739','g-740'\n,'g-741','g-742','g-743','g-744','g-745','g-746','g-747','g-748','g-749'\n,'g-750','g-751','g-752','g-753','g-755','g-756','g-757','g-758','g-759'\n,'g-760','g-761','g-762','g-763','g-764','g-766','g-767','g-768','g-769'\n,'g-771','c-0','c-5','c-6','c-7','c-8','c-9','c-10','c-12','c-13','c-15'\n,'c-18','c-20','c-22','c-24','c-25','c-26','c-30','c-33','c-34','c-36'\n,'c-37','c-38','c-41','c-44','c-45','c-46','c-47','c-48','c-50','c-51'\n,'c-52','c-54','c-56','c-57','c-58','c-59','c-60','c-62','c-63','c-64'\n,'c-65','c-66','c-67','c-69','c-70','c-71','c-72','c-73','c-75','c-76'\n,'c-77','c-79','c-80','c-81','c-83','c-85','c-86','c-87','c-89','c-92'\n,'c-93','c-95','c-96','c-98','c-99']\n\nif IS_TRAIN:\n    data_features = pd.read_csv(f'{DATA_PATH}/train_features.csv')[selected_cols]\n    test_features = pd.read_csv(f'{DATA_PATH}/test_features.csv')[selected_cols]\n    train_targets_scored = pd.read_csv(f'{DATA_PATH}/train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv(f'{DATA_PATH}/train_targets_nonscored.csv')\n    print(f'Train: {len(data_features)}')\n\n    target_cols = [col for col in train_targets_scored.columns if col != 'sig_id']\n    data_process_dict = {}\n    data_process_dict['target_cols'] = target_cols\n\n    # from sklearn.preprocessing import QuantileTransformer, MinMaxScaler\n    #\n    # weights_arr = np.array(len(data_features) / train_targets_scored[target_cols].sum(axis=0))\n    # scaler = MinMaxScaler((1., 3.))\n    # scaler.fit(weights_arr.reshape(-1, 1))\n    # weights_arr_out = scaler.transform(weights_arr.reshape(-1, 1))\n    # weights_arr_out = weights_arr_out.reshape(-1)\n\nelse:\n    data_features = pd.read_csv(f'{DATA_PATH}/test_features.csv')[selected_cols]\n    sample_submission = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n    print(f'Test: {len(data_features)}')\n\n    data_process_dict = load_pickle(data_process_file)\n    target_cols = data_process_dict['target_cols']\n\nid_col = 'sig_id'\nGENES = [col for col in data_features.columns if col.startswith('g-')]\nCELLS = [col for col in data_features.columns if col.startswith('c-')]\nfeature_cols = GENES + CELLS\n\n########################################################################################################################\n##### process data\n\n#RankGauss\nvec_len = len(data_features)\nif IS_TRAIN:\n    rankGauss_dict = {}\n    for col in tqdm(GENES + CELLS, desc='GENES + CELLS cols rankgauss fitting'):\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(data_features[col].values.reshape(vec_len, 1))\n        rankGauss_dict[col] = transformer\n    data_process_dict['rankGauss_dict'] = rankGauss_dict\nelse:\n    rankGauss_dict = data_process_dict['rankGauss_dict']\n\nfor col in tqdm(GENES + CELLS, desc='GENES + CELLS cols rankgauss transforming'):\n    raw_vec = data_features[col].values.reshape(vec_len, 1)\n    data_features[col] = rankGauss_dict[col].transform(raw_vec).reshape(1, vec_len)[0]\n\n### PCA features + Existing features\n# GENES\nn_comp = 50\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=42)\n    pca.fit(data_features[GENES])\n    data_process_dict['g_pca'] = pca\npca = data_process_dict['g_pca']\ndata2 = pca.transform(data_features[GENES])\n\ndata2 = pd.DataFrame(data2, columns=[f'pca_G-{i}' for i in range(n_comp)])\nfeature_cols.extend([f'pca_G-{i}' for i in range(n_comp)])\n\ndata_features = pd.concat((data_features, data2), axis=1)\n\n#CELLS\nn_comp = 15\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=42)\n    pca.fit(data_features[CELLS])\n    data_process_dict['c_pca'] = pca\npca = data_process_dict['c_pca']\ndata2 = pca.transform(data_features[CELLS])\n\ndata2 = pd.DataFrame(data2, columns=[f'pca_C-{i}' for i in range(n_comp)])\nfeature_cols.extend([f'pca_C-{i}' for i in range(n_comp)])\n\ndata_features = pd.concat((data_features, data2), axis=1)\n\n### TruncatedSVD features + Existing features\n# GENES\nn_comp = 50\nif IS_TRAIN:\n    svd = TruncatedSVD(n_components=n_comp, random_state=42)\n    svd.fit(data_features[GENES])\n    data_process_dict['g_svd'] = svd\nsvd = data_process_dict['g_svd']\ndata2 = svd.transform(data_features[GENES])\n\ndata2 = pd.DataFrame(data2, columns=[f'TruncatedSVD_G-{i}' for i in range(n_comp)])\nfeature_cols.extend([f'TruncatedSVD_G-{i}' for i in range(n_comp)])\n\ndata_features = pd.concat((data_features, data2), axis=1)\n\n#CELLS\nn_comp = 15\nif IS_TRAIN:\n    svd = TruncatedSVD(n_components=n_comp, random_state=42)\n    svd.fit(data_features[CELLS])\n    data_process_dict['c_svd'] = svd\nsvd = data_process_dict['c_svd']\ndata2 = svd.transform(data_features[CELLS])\n\ndata2 = pd.DataFrame(data2, columns=[f'TruncatedSVD_C-{i}' for i in range(n_comp)])\nfeature_cols.extend([f'TruncatedSVD_C-{i}' for i in range(n_comp)])\n\ndata_features = pd.concat((data_features, data2), axis=1)\n\n### FactorAnalysis features + Existing features\n# GENES\nn_comp = 50\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42)\n    fa.fit(data_features[GENES])\n    data_process_dict['g_fa'] = fa\nfa = data_process_dict['g_fa']\ndata2 = fa.transform(data_features[GENES])\n\ndata2 = pd.DataFrame(data2, columns=[f'FA_G-{i}' for i in range(n_comp)])\nfeature_cols.extend([f'FA_G-{i}' for i in range(n_comp)])\n\ndata_features = pd.concat((data_features, data2), axis=1)\n\n#CELLS\nn_comp = 15\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42)\n    fa.fit(data_features[CELLS])\n    data_process_dict['c_fa'] = fa\nfa = data_process_dict['c_fa']\ndata2 = fa.transform(data_features[CELLS])\n\ndata2 = pd.DataFrame(data2, columns=[f'FA_C-{i}' for i in range(n_comp)])\nfeature_cols.extend([f'FA_C-{i}' for i in range(n_comp)])\n\ndata_features = pd.concat((data_features, data2), axis=1)\n\nif IS_TRAIN:\n    cate_dict = {}\n    for col in ['cp_time', 'cp_dose']:\n        cate_dict[col] = []\n        for mod in data_features[col].unique():\n            cate_dict[col].append(mod)\n    data_process_dict['cate_dict'] = cate_dict\ncate_dict = data_process_dict['cate_dict']\n\nfor col in cate_dict:\n    for mod in cate_dict[col]:\n        data_features[str(mod)] = (data_features[col] == mod).astype(int)\n        feature_cols.append(str(mod))\n\n########################################################################################################################\n##### prepare models\nclass MoADataset:\n    def __init__(self, features, targets, isTrain=True):\n        self.features = features\n        self.targets = targets\n        self.isTrain = isTrain\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x': torch.tensor(self.features[idx, :], dtype=torch.float),\n        }\n        if self.isTrain:\n            dct['y'] = torch.tensor(self.targets[idx, :], dtype=torch.float)\n        return dct\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        #         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n\n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds)\n\n    return preds\n\nfrom torch.nn.modules.loss import _WeightedLoss\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\ndef MultilabelStratifiedKFold_run(\n        model_fn,\n        train_feat,\n        train_labels,\n        test_feat,\n        seed,\n        model_name,\n        input_dim,\n        output_dim,\n        nfolds=NFOLDS,\n        verbose=0,\n        isTrain=IS_TRAIN,\n):\n    seed_everything(seed)\n    if IS_TRAIN:\n        train_pred = np.zeros((len(train_feat), output_dim))\n        kf = MultilabelStratifiedKFold(n_splits=nfolds, shuffle=True, random_state=seed)\n        from sklearn.cluster import KMeans\n        cluster_num = 250\n        kmeans = KMeans(n_clusters=cluster_num, random_state=0).fit(train_labels)\n        label_cluster = np.eye(cluster_num)[kmeans.predict(train_labels)]\n        # kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42)\n        # for _fold, (trn_idx, val_idx) in enumerate(kf.split(train_feat, train_labels)):\n        for _fold, (trn_idx, val_idx) in enumerate(kf.split(train_feat, label_cluster)):\n            start_time = time.time()\n\n            # feat_num = train_feat.shape[1]\n            # sample_num = int(FEAT_SAMPLE_RATE * feat_num)\n            # print(f'feat_num: {feat_num}, sample_num: {sample_num}')\n            # sample_cols_indexs = sorted(random.sample([feat_i for feat_i in range(feat_num)], sample_num))\n            # data_process_dict[f'Seed{seed}Fold{_fold}_cols'] = sample_cols_indexs\n\n            train_dataset = MoADataset(train_feat[trn_idx], train_labels[trn_idx], True)\n            valid_dataset = MoADataset(train_feat[val_idx], train_labels[val_idx], True)\n\n            trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n            validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n            model = model_fn(\n                num_features=input_dim,\n                num_targets=output_dim,\n                hidden_size=hidden_size,\n            )\n\n            model.to(DEVICE)\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n            scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                                      max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n            # loss_fn = nn.BCEWithLogitsLoss()\n            # loss_fn = BCEWithLogitsLoss(smooth_eps=1e-3)\n            # loss_fn = SmoothBCEwLogits(weight=torch.tensor(weights_arr_out), smoothing=1e-3).to(DEVICE)\n            loss_fn = SmoothBCEwLogits(smoothing=1e-3)\n\n            early_stopping_steps = EARLY_STOPPING_STEPS\n            early_step = 0\n\n            best_loss = np.inf\n\n            model_weights = f\"{CACHE_PATH}/{model_name}_SEED{seed}_FOLD{_fold}.pth\"\n            for epoch in range(EPOCHS):\n\n                train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n                valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n                if verbose:\n                    print(f\"SEED: {seed}, FOLD: {_fold}, EPOCH: {epoch:3}, train_loss: {train_loss:.5f}, valid_loss: {valid_loss:.5f}, \"\n                          f\"time: {(time.time() - start_time) / 60:.2f}min\")\n\n                if valid_loss < best_loss:\n\n                    best_loss = valid_loss\n                    torch.save(model.state_dict(), model_weights)\n\n                elif (EARLY_STOP == True):\n\n                    early_step += 1\n                    if (early_step >= early_stopping_steps):\n                        break\n\n            model.load_state_dict(torch.load(model_weights))\n            train_pred[val_idx] = inference_fn(model, validloader, DEVICE)\n        return train_pred\n    else:\n        test_pred = np.zeros((len(test_feat), output_dim))\n        for _fold in range(nfolds):\n            # sample_cols_indexs = data_process_dict[f'Seed{seed}Fold{_fold}_cols']\n\n            model = Model(\n                num_features=input_dim,\n                num_targets=output_dim,\n                hidden_size=hidden_size,\n            )\n            model.to(DEVICE)\n            model_weights = f\"{CACHE_PATH}/{model_name}_SEED{seed}_FOLD{_fold}.pth\"\n            model.load_state_dict(torch.load(model_weights))\n\n            testdataset = MoADataset(test_feat, None, False)\n            testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n            test_pred += inference_fn(model, testloader, DEVICE) / nfolds\n        return test_pred\n\n# ########################################################################################################################\n# ##### make meta features\n# print('Making nonscored feat...')\n# if IS_TRAIN:\n#     nonscored_drop_cols = [id_col]\n#     for col in [var for var in train_targets_nonscored.columns if var != 'sig_id']:\n#         if train_targets_nonscored[col].sum() == 0:\n#             nonscored_drop_cols.append(col)\n#     nonscored_cols = [col for col in train_targets_nonscored.columns if col not in nonscored_drop_cols]\n#     data_process_dict['nonscored_cols'] = nonscored_cols\n# else:\n#     nonscored_cols = data_process_dict['nonscored_cols']\n# print('nonscored_cols: ', len(nonscored_cols))\n# non_scored_feat = data_features[feature_cols].values\n# if IS_TRAIN:\n#     non_scored_labels = train_targets_nonscored[nonscored_cols].values\n#\n# if IS_TRAIN:\n#     data_meta_feat = MultilabelStratifiedKFold_run(\n#         train_feat=non_scored_feat,\n#         train_labels=non_scored_labels,\n#         test_feat=None,\n#         seed=0,\n#         model_name='nonscored_model',\n#         input_dim=len(feature_cols),\n#         output_dim=len(nonscored_cols),\n#         nfolds=5,\n#         verbose=0,\n#         isTrain=IS_TRAIN,\n#     )\n#     oof_score = score_fn(y_true=non_scored_labels, y_pred=data_meta_feat)\n#     print(f'Nonscored OOF score={oof_score:.5f}')\n# else:\n#     data_meta_feat, _ = MultilabelStratifiedKFold_run(\n#         train_feat=None,\n#         train_labels=None,\n#         test_feat=non_scored_feat,\n#         seed=0,\n#         model_name='nonscored_model',\n#         input_dim=len(feature_cols),\n#         output_dim=len(nonscored_cols),\n#         nfolds=5,\n#         verbose=0,\n#         isTrain=IS_TRAIN,\n#     )\n# for idx, col in enumerate(tqdm(nonscored_cols, desc='Appending unscored feat')):\n#     data_features[col] = data_meta_feat[:, idx]\n#     feature_cols.append(col)\n# del data_meta_feat\n\n########################################################################################################################\n##### prepare data and train\nif IS_TRAIN:\n    data = data_features.merge(train_targets_scored, on='sig_id')\n    data = data[data['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n\n    data_labels = data[target_cols].values\nelse:\n    data = data_features[data_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n\n# external_feature_chosen = pd.read_csv('../input/feature_selecting/chosen_feat_df.csv').top_feat_cols.values.tolist()\n# feature_cols = [col for col in feature_cols if col in external_feature_chosen]\n\nfeature_cols = pd.read_csv('../input/feature-selecting1105/chosen_feat_df.csv').top_feat_cols.values.tolist()\n\ndata_feat = data[feature_cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MLP1 OOF log_loss:  0.014429373656645358 7seed7cv LB 0.01834\nCACHE_PATH = '../input/mlp1-torch-weights1109'\nhidden_size = 2048\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.4)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.4)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        # x = self.dropout1(x)\n        x = self.dense1(x)\n        # x = F.relu(x)\n        x = self.PReLU(x)\n        # x = self.LeakyReLU(x)\n        # x = self.GeLU(x)\n        # x = self.RReLU(x)\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.dense2(x)\n        # x = F.relu(x)\n        x = self.PReLU(x)\n        # x = self.LeakyReLU(x)\n        # x = self.GeLU(x)\n        # x = self.RReLU(x)\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x\n\n\n# Averaging on multiple SEEDS\noof_pred1 = pd.read_csv(f'{CACHE_PATH}/oof_pred.csv')[target_cols].values\nnpy_pred1 = np.zeros((len(data), len(target_cols)))\nfor seed in SEED:\n    if IS_TRAIN:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=data_feat,\n            train_labels=data_labels,\n            test_feat=None,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    else:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=None,\n            train_labels=None,\n            test_feat=data_feat,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    npy_pred1 += seed_npy_pred / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MLP2 OOF log_loss:  0.0144581231992106 7seed7cv LB ???\nCACHE_PATH = '../input/mlp2-torch-weights1110'\nhidden_size = 1024\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n\n        hidden_size1 = 1024\n        hidden_size2 = 1024\n        hidden_size3 = 512\n\n        self.l1_tanh = nn.Sequential(\n            nn.BatchNorm1d(num_features),\n            nn.Dropout(0.2),\n            nn.Linear(num_features, hidden_size3, bias=False),\n            nn.Tanh())\n        self.l1_sigm = nn.Sequential(\n            nn.BatchNorm1d(num_features),\n            nn.Dropout(0.2),\n            nn.Linear(num_features, hidden_size3, bias=False),\n            nn.RReLU())\n\n        self.l1_nn = nn.Sequential(\n            nn.BatchNorm1d(hidden_size3),\n            nn.LayerNorm(hidden_size3),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_size3, hidden_size3, bias=False),\n            nn.RReLU())\n\n        self.l1 = nn.Sequential(\n            nn.BatchNorm1d(num_features),\n            nn.LayerNorm(num_features),\n            nn.Dropout(0.2),\n            nn.Linear(num_features, hidden_size1),\n            nn.PReLU(hidden_size1))\n\n        self.tanh = nn.Sequential(\n            nn.BatchNorm1d(hidden_size1),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_size1, hidden_size2, bias=False),\n            nn.Tanh())\n\n        self.sigm = nn.Sequential(\n            nn.BatchNorm1d(hidden_size1),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_size1, hidden_size2, bias=False),\n            nn.RReLU())\n\n        self.f_layer = nn.Sequential(\n            nn.BatchNorm1d(hidden_size2 + hidden_size3),\n            nn.Dropout(0.3),\n            nn.RReLU(),\n            nn.Linear(hidden_size2 + hidden_size3, num_targets)\n        )\n\n    def forward(self, x):\n        x_tanh_l1 = self.l1_tanh(x)\n        x_sigm_l1 = self.l1_sigm(x)\n        x_att_l1 = x_tanh_l1 * x_sigm_l1\n        x_att_l1 = self.l1_nn(x_att_l1)\n\n        x = self.l1(x)\n\n        x_tanh = self.tanh(x)\n        x_sigm = self.sigm(x)\n\n        x = x_tanh * x_sigm\n\n        x = torch.cat((x, x_att_l1), dim=1)\n        x = self.f_layer(x)\n\n        return x\n\n# Averaging on multiple SEEDS\noof_pred2 = pd.read_csv(f'{CACHE_PATH}/oof_pred.csv')[target_cols].values\nnpy_pred2 = np.zeros((len(data), len(target_cols)))\nfor seed in SEED:\n    if IS_TRAIN:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=data_feat,\n            train_labels=data_labels,\n            test_feat=None,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    else:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=None,\n            train_labels=None,\n            test_feat=data_feat,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    npy_pred2 += seed_npy_pred / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MLP3 OOF log_loss:  0.014490372476255158 7seed7cv LB ???\nCACHE_PATH = '../input/mlp3-torch-weights1110'\nhidden_size = 1024\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.BN0 = nn.BatchNorm1d(num_features)\n        self.dropout0 = nn.Dropout(0.2)\n\n        drop_rate = 0.4\n        self.dense1 = nn.Linear(num_features, 2048)\n        self.BN1 = nn.BatchNorm1d(2048)\n        self.dropout1 = nn.Dropout(drop_rate)\n\n        self.dense2 = nn.Linear(2048, 2048)\n        self.BN2 = nn.BatchNorm1d(2048)\n        self.dropout2 = nn.Dropout(drop_rate)\n\n        self.dense3 = nn.Linear(2048, 2048)\n        self.BN3 = nn.BatchNorm1d(2048)\n        self.dropout3 = nn.Dropout(drop_rate)\n\n        self.dense4 = nn.Linear(2048, num_targets)\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n\n    def forward(self, inp):\n        x = self.BN0(inp)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        # x = F.relu(x)\n        x = self.PReLU(x)\n        x = self.BN1(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        # x = F.relu(x)\n        x = self.PReLU(x)\n        x = self.BN2(x)\n        x = self.dropout2(x)\n\n        x = self.dense3(x)\n        # x = F.relu(x)\n        x = self.PReLU(x)\n        x = self.BN3(x)\n        x = self.dropout3(x)\n\n        x = self.dense4(x)\n\n        return x\n\n# Averaging on multiple SEEDS\noof_pred3 = pd.read_csv(f'{CACHE_PATH}/oof_pred.csv')[target_cols].values\nnpy_pred3 = np.zeros((len(data), len(target_cols)))\nfor seed in SEED:\n    if IS_TRAIN:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=data_feat,\n            train_labels=data_labels,\n            test_feat=None,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    else:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=None,\n            train_labels=None,\n            test_feat=data_feat,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    npy_pred3 += seed_npy_pred / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM OOF log_loss:  0.014677774191343845 7seed7cv LB 0.01854\nCACHE_PATH = '../input/lstm-torch1116-weights'\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        hidden_size = 200\n        self.Lstm = nn.LSTM(num_features, hidden_size // 2, bidirectional=True, batch_first=True, dropout=0.2)\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        # self.dropout1 = nn.Dropout(0.2)\n        # self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        #\n        # self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        # self.dropout2 = nn.Dropout(0.4)\n        # self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        #\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.4)\n        self.dense3 = nn.Linear(hidden_size, num_targets)\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        inp = x.unsqueeze(-2)\n\n        self.Lstm.flatten_parameters()\n        lstm, _ = self.Lstm(inp)\n\n        max_pool, _ = torch.max(lstm, 1)\n        avg_pool = torch.mean(lstm, 1)\n\n        pooled_output = self.dropout(max_pool)\n        pooled_output = self.RReLU(pooled_output)\n\n        x = self.batch_norm3(pooled_output)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x\n\n# Averaging on multiple SEEDS\noof_pred4 = pd.read_csv(f'{CACHE_PATH}/oof_pred.csv')[target_cols].values\nnpy_pred4 = np.zeros((len(data), len(target_cols)))\nfor seed in SEED:\n    if IS_TRAIN:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=data_feat,\n            train_labels=data_labels,\n            test_feat=None,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    else:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=None,\n            train_labels=None,\n            test_feat=data_feat,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    npy_pred4 += seed_npy_pred / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN OOF log_loss:  0.014720567819513208 7seed7cv LB 0.01862\nCACHE_PATH = '../input/cnn-torch1120-weights'\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass SEModule(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, padding, reduction=2):\n        super(SEModule, self).__init__()\n        self.conv0 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n        self.conv1 = nn.Conv1d(in_channels, in_channels//reduction, kernel_size=kernel_size, padding=padding)\n        self.conv2 = nn.Conv1d(in_channels//reduction, out_channels, kernel_size=kernel_size, padding=padding)\n    def forward(self, x):\n        s = F.adaptive_avg_pool1d(x, 1)\n        #s = self.conv1(s)\n        #s = F.relu(s, inplace=True)\n        s = self.conv0(s)\n        x *= torch.sigmoid(s)\n        return x\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n\n        conv_size = 128\n        dropout_rate = 0.2\n\n        self.conv1d_1 = nn.Conv1d(num_features, conv_size, kernel_size=1, stride=1, dilation=1, padding=0, padding_mode='replicate')\n        self.batch_norm_conv_1 = nn.BatchNorm1d(conv_size)\n        self.dropout_conv_1 = nn.Dropout(dropout_rate)\n\n        self.conv1d_2 = nn.Conv1d(num_features, conv_size, kernel_size=3, stride=1, dilation=1, padding=1, padding_mode='replicate')\n        self.batch_norm_conv_2 = nn.BatchNorm1d(conv_size)\n        self.dropout_conv_2 = nn.Dropout(dropout_rate)\n\n        self.conv1d_3 = nn.Conv1d(num_features, conv_size, kernel_size=5, stride=1, dilation=1, padding=2, padding_mode='replicate')\n        self.batch_norm_conv_3 = nn.BatchNorm1d(conv_size)\n        self.dropout_conv_3 = nn.Dropout(dropout_rate)\n\n        self.conv1d_4 = nn.Conv1d(num_features, conv_size, kernel_size=15, stride=1, dilation=1, padding=7, padding_mode='replicate')\n        self.batch_norm_conv_4 = nn.BatchNorm1d(conv_size)\n        self.dropout_conv_4 = nn.Dropout(dropout_rate)\n\n        self.conv1d_5 = nn.Conv1d(num_features, conv_size, kernel_size=31, stride=1, dilation=1, padding=15, padding_mode='replicate')\n        self.batch_norm_conv_5 = nn.BatchNorm1d(conv_size)\n        self.dropout_conv_5 = nn.Dropout(dropout_rate)\n\n        self.batch_norm3 = nn.BatchNorm1d(conv_size * 5)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.Linear(conv_size * 5, num_targets)\n\n        self.ReLU = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n        self.dropout = nn.Dropout(0.2)\n        self.spatial_dropout = SpatialDropout(0.2)\n\n    def forward(self, x):\n        # x = self.batch_norm1(x)\n        # x = self.dropout(x)\n        # x = self.linear1(x)\n        x = x.unsqueeze(-1)\n        # x = self.spatial_dropout(x)\n\n        x1 = self.conv1d_1(x)\n        x1 = F.relu(x1)\n        # x1 = self.PReLU(x1)\n        x1 = self.batch_norm_conv_1(x1)\n        x1 = self.dropout_conv_1(x1)\n        x1 = x1.squeeze(-1)\n\n        x2 = self.conv1d_2(x)\n        x2 = F.relu(x2)\n        # x2 = self.PReLU(x2)\n        x2 = self.batch_norm_conv_2(x2)\n        x2 = self.dropout_conv_2(x2)\n        x2 = x2.squeeze(-1)\n\n        x3 = self.conv1d_3(x)\n        x3 = F.relu(x3)\n        # x3 = self.PReLU(x3)\n        x3 = self.batch_norm_conv_3(x3)\n        x3 = self.dropout_conv_3(x3)\n        x3 = x3.squeeze(-1)\n\n        x4 = self.conv1d_4(x)\n        x4 = F.relu(x4)\n        # x4 = self.PReLU(x4)\n        x4 = self.batch_norm_conv_4(x4)\n        x4 = self.dropout_conv_4(x4)\n        x4 = x4.squeeze(-1)\n\n        # x5 = self.conv1d_5(x)\n        # x5 = F.relu(x5)\n        # # x5 = self.PReLU(x5)\n        # x5 = self.batch_norm_conv_5(x5)\n        # x5 = self.dropout_conv_5(x5)\n        # x5 = x5.squeeze(-1)\n        cross1 = x1 * x2\n        # cross2 = x2 * x3\n\n        conv_output = torch.cat((x1, x2, x3, x4, cross1), 1)\n\n        x = self.batch_norm3(conv_output)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x\n\n# Averaging on multiple SEEDS\noof_pred5 = pd.read_csv(f'{CACHE_PATH}/oof_pred.csv')[target_cols].values\nnpy_pred5 = np.zeros((len(data), len(target_cols)))\nfor seed in SEED:\n    if IS_TRAIN:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=data_feat,\n            train_labels=data_labels,\n            test_feat=None,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    else:\n        seed_npy_pred = MultilabelStratifiedKFold_run(\n            model_fn=Model,\n            train_feat=None,\n            train_labels=None,\n            test_feat=data_feat,\n            seed=seed,\n            model_name='mlp',\n            input_dim=len(feature_cols),\n            output_dim=len(target_cols),\n            nfolds=NFOLDS,\n            verbose=1,\n            isTrain=IS_TRAIN,\n        )\n    npy_pred5 += seed_npy_pred / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_pred = oof_pred1 * 0.3 + oof_pred2 * 0.25 + oof_pred3 * 0.2  + oof_pred4 * 0.15  + oof_pred5 * 0.1\n\ntrain_targets_scored = pd.read_csv(f'{DATA_PATH}/train_targets_scored.csv')\ny_true = train_targets_scored[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], oof_pred[:, i])\n    score += score_ / len(target_cols)\n\nprint(\"OOF log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OOF log_loss:  0.014349538866134236","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"npy_pred = npy_pred1 * 0.3 + npy_pred2 * 0.25 + npy_pred3 * 0.2 + npy_pred4 * 0.15 + npy_pred5 * 0.1\n\ntest_pred = pd.DataFrame(npy_pred, columns=target_cols)\ntest_pred[id_col] = data[id_col].values\n\nsub = sample_submission.drop(columns=target_cols).merge(test_pred[['sig_id'] + target_cols], on='sig_id',\n                                                            how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)\nprint(sub.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
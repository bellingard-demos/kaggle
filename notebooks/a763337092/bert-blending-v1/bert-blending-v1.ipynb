{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold, KFold, RepeatedKFold\nfrom tqdm import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport re\nimport os\nimport sys\nimport six\nimport collections\nfrom scipy.stats import spearmanr, rankdata\nfrom math import floor, ceil\n\nsys.path.extend(['../input/bert-joint-baseline/'])\n# from transformers import *\nimport tokenization\nDEBUG = False\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# print('tokenizer: ', tokenizer)\n\nif not DEBUG:\n    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n    np.set_printoptions(suppress=True)\n\nPATH = '../input/google-quest-challenge/'\nBERT_PATH = '../input/bert-en-uncased-l12-h768-a12'\n# BERT_PATH = '../input/bertenuncasedl24h1024a16'\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', False)\nMAX_SEQUENCE_LENGTH = 512\n\nif DEBUG:\n    df_train = pd.read_csv(PATH+'train.csv', nrows=30)\n    df_test = pd.read_csv(PATH+'test.csv', nrows=30)\nelse:\n    df_train = pd.read_csv(PATH + 'train.csv')\n    df_test = pd.read_csv(PATH + 'test.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\nprint('\\noutput categories:\\n\\t', output_categories)\n\nq_targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n    ]\na_targets = [\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'\n    ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class MyLabelEncoder(object):\n    \"\"\"safely handle unknown label\"\"\"\n    def __init__(self):\n        self.mapper = {}\n\n    def fit(self, X):\n        uniq_X = np.unique(X)\n        # reserve 0 for unknown\n        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n        return self\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)\n\n    def _map(self, x):\n        return self.mapper.get(x, 0)\n\n    def transform(self, X):\n        return list(map(self._map, X))\ndef get_cate_feat(df, isTrain):\n    if isTrain:\n        label_encoder[\"category\"] = MyLabelEncoder()\n        category_feat = np.array(label_encoder[\"category\"].fit_transform(df[\"category\"]))\n        label_encoder[\"host\"] = MyLabelEncoder()\n        host_feat = np.array(label_encoder[\"host\"].fit_transform(df[\"host\"]))\n    else:\n        category_feat = np.array(label_encoder[\"category\"].transform(df[\"category\"]))\n        host_feat = np.array(label_encoder[\"host\"].transform(df[\"host\"]))\n    return [category_feat, host_feat]\nlabel_encoder = {}\ncate_feat_tr = get_cate_feat(df_train, isTrain=True)\ncate_feat_te = get_cate_feat(df_test, isTrain=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n    return input_ids\n\ndef my_pad(text, max_length, tokenizer):\n    res = tokenizer.tokenize(text)\n    if len(res) > max_length:\n        head_length = int(0.25 * max_length)\n        tail_length = max_length - head_length\n        res = res[:head_length] + res[-tail_length:]\n    return res\n\ndef my_padQ(q, a, max_length, tokenizer):\n    q_ = tokenizer.tokenize(q)\n    a_ = tokenizer.tokenize(a)\n\n    if len(q_)+10 >= max_length:\n        head_length = int(0.25 * (max_length-10))\n        tail_length = (max_length-10) - head_length\n        q_ = q_[:head_length] + q_[-tail_length:]\n\n    if len(q_) + len(a_) > max_length:\n        a_length = max_length - len(q_)\n        head_a_length = int(0.25*a_length)\n        tail_a_length = a_length - head_a_length\n        a_ = a_[:head_a_length] + a_[-tail_a_length:]\n    # print(len(q_), len(a_))\n    return q_, a_\ndef my_padA(q, a, max_length, tokenizer):\n    q_ = tokenizer.tokenize(q)\n    a_ = tokenizer.tokenize(a)\n\n    if len(a_)+10 >= max_length:\n        head_length = int(0.25 * (max_length-10))\n        tail_length = (max_length-10) - head_length\n        a_ = a_[:head_length] + a_[-tail_length:]\n\n    if len(q_) + len(a_) > max_length:\n        q_length = max_length - len(a_)\n        head_q_length = int(0.25*q_length)\n        tail_q_length = q_length - head_q_length\n        q_ = q_[:head_q_length] + q_[-tail_q_length:]\n    return q_, a_\n\ndef prepare_Qdata(df, tokenizer):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[['question_title', 'question_body', 'answer']].iterrows()):\n        question_title_ = str(instance['question_title']).lower()\n        question_body_ = str(instance['question_body']).lower()\n        answer_ = str(instance['answer']).lower()\n\n        question = question_title_ + ' ' + question_body_\n        # question, answer = my_padQ(question, answer_, MAX_SEQUENCE_LENGTH-3, tokenizer)\n        question = my_pad(question, MAX_SEQUENCE_LENGTH-3, tokenizer)\n        \n        stoken = [\"[CLS]\"] + question + [\"[SEP]\"]\n        # print(len(stoken))\n        # stoken = [\"[CLS]\"] + question_title_ + [\",\"] + question_body_ + [\"[SEP]\"] + answer_ + [\"[SEP]\"]\n\n        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n\n        input_ids.append(input_ids_)\n        input_masks.append(input_masks_)\n        input_segments.append(input_segments_)\n\n    return [np.asarray(input_ids, dtype=np.int32),\n            np.asarray(input_masks, dtype=np.int32),\n            np.asarray(input_segments, dtype=np.int32)]\n\ndef prepare_Adata(df, tokenizer):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[['question_title', 'question_body', 'answer']].iterrows()):\n        question_title_ = str(instance['question_title']).lower()\n        question_body_ = str(instance['question_body']).lower()\n        answer_ = str(instance['answer']).lower()\n\n        question = question_title_ + ' ' + question_body_\n        question, answer = my_padA(question, answer_, MAX_SEQUENCE_LENGTH-3, tokenizer)\n        \n        stoken = [\"[CLS]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n        # stoken = [\"[CLS]\"] + question_title_ + [\",\"] + question_body_ + [\"[SEP]\"] + answer_ + [\"[SEP]\"]\n\n        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n\n        input_ids.append(input_ids_)\n        input_masks.append(input_masks_)\n        input_segments.append(input_segments_)\n\n    return [np.asarray(input_ids, dtype=np.int32),\n            np.asarray(input_masks, dtype=np.int32),\n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Qinputs_tr = prepare_Qdata(df_train, tokenizer)\nQinputs_te = prepare_Qdata(df_test, tokenizer)\nQinputs_tr.extend(cate_feat_tr)\nQinputs_te.extend(cate_feat_te)\nQoutputs = compute_output_arrays(df_train, q_targets)\n\nAinputs_tr = prepare_Adata(df_train, tokenizer)\nAinputs_te = prepare_Adata(df_test, tokenizer)\nAinputs_tr.extend(cate_feat_tr)\nAinputs_te.extend(cate_feat_te)\nAoutputs = compute_output_arrays(df_train, a_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\nfrom tensorflow.python import ops, math_ops, state_ops, control_flow_ops\nfrom tensorflow.python.keras import backend_config\n\n__all__ = ['AdamWarmup']\n\n\nclass AdamWarmup(OptimizerV2):\n    \"\"\"Adam optimizer with warmup.\"\"\"\n\n    def __init__(self,\n                 decay_steps,\n                 warmup_steps,\n                 min_lr=0.0,\n                 learning_rate=0.001,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-7,\n                 weight_decay=0.,\n                 weight_decay_pattern=None,\n                 amsgrad=False,\n                 name='Adam',\n                 **kwargs):\n        r\"\"\"Construct a new Adam optimizer.\n\n        Args:\n            decay_steps: Learning rate will decay linearly to zero in decay steps.\n            warmup_steps: Learning rate will increase linearly to lr in first warmup steps.\n            lr: float >= 0. Learning rate.\n            beta_1: float, 0 < beta < 1. Generally close to 1.\n            beta_2: float, 0 < beta < 1. Generally close to 1.\n            epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n            weight_decay: float >= 0. Weight decay.\n            weight_decay_pattern: A list of strings. The substring of weight names to be decayed.\n                                  All weights will be decayed if it is None.\n            amsgrad: boolean. Whether to apply the AMSGrad variant of this\n                algorithm from the paper \"On the Convergence of Adam and\n                Beyond\".\n        \"\"\"\n\n        super(AdamWarmup, self).__init__(name, **kwargs)\n        self._set_hyper('decay_steps', float(decay_steps))\n        self._set_hyper('warmup_steps', float(warmup_steps))\n        self._set_hyper('min_lr', min_lr)\n        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n        self._set_hyper('decay', self._initial_decay)\n        self._set_hyper('beta_1', beta_1)\n        self._set_hyper('beta_2', beta_2)\n        self._set_hyper('weight_decay', weight_decay)\n        self.epsilon = epsilon or backend_config.epsilon()\n        self.amsgrad = amsgrad\n        self._initial_weight_decay = weight_decay\n        self._weight_decay_pattern = weight_decay_pattern\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, 'm')\n        for var in var_list:\n            self.add_slot(var, 'v')\n        if self.amsgrad:\n            for var in var_list:\n                self.add_slot(var, 'vhat')\n\n    def set_weights(self, weights):\n        params = self.weights\n        num_vars = int((len(params) - 1) / 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[:len(params)]\n        super(AdamWarmup, self).set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        m = self.get_slot(var, 'm')\n        v = self.get_slot(var, 'v')\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        decay_steps = self._get_hyper('decay_steps', var_dtype)\n        warmup_steps = self._get_hyper('warmup_steps', var_dtype)\n        min_lr = self._get_hyper('min_lr', var_dtype)\n        lr_t = tf.where(\n            local_step <= warmup_steps,\n            lr_t * (local_step / warmup_steps),\n            min_lr + (lr_t - min_lr) * (1.0 - tf.minimum(local_step, decay_steps) / decay_steps),\n        )\n        lr_t = (lr_t * math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power))\n\n        m_t = state_ops.assign(m,\n                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n                               use_locking=self._use_locking)\n\n        v_t = state_ops.assign(v,\n                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n                               use_locking=self._use_locking)\n\n        if self.amsgrad:\n            v_hat = self.get_slot(var, 'vhat')\n            v_hat_t = math_ops.maximum(v_hat, v_t)\n            var_update = m_t / (math_ops.sqrt(v_hat_t) + epsilon_t)\n        else:\n            var_update = m_t / (math_ops.sqrt(v_t) + epsilon_t)\n\n        if self._initial_weight_decay > 0.0:\n            weight_decay = self._get_hyper('weight_decay', var_dtype)\n            var_update += weight_decay * var\n        var_update = state_ops.assign_sub(var, lr_t * var_update, use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(v_hat_t)\n        return control_flow_ops.group(*updates)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        decay_steps = self._get_hyper('decay_steps', var_dtype)\n        warmup_steps = self._get_hyper('warmup_steps', var_dtype)\n        min_lr = self._get_hyper('min_lr', var_dtype)\n        lr_t = tf.where(\n            local_step <= warmup_steps,\n            lr_t * (local_step / warmup_steps),\n            min_lr + (lr_t - min_lr) * (1.0 - tf.minimum(local_step, decay_steps) / decay_steps),\n        )\n        lr_t = (lr_t * math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power))\n\n        m = self.get_slot(var, 'm')\n        m_scaled_g_values = grad * (1 - beta_1_t)\n        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n\n        v = self.get_slot(var, 'v')\n        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n\n        if self.amsgrad:\n            v_hat = self.get_slot(var, 'vhat')\n            v_hat_t = math_ops.maximum(v_hat, v_t)\n            var_update = m_t / (math_ops.sqrt(v_hat_t) + epsilon_t)\n        else:\n            var_update = m_t / (math_ops.sqrt(v_t) + epsilon_t)\n\n        if self._initial_weight_decay > 0.0:\n            weight_decay = self._get_hyper('weight_decay', var_dtype)\n            var_update += weight_decay * var\n        var_update = state_ops.assign_sub(var, lr_t * var_update, use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(v_hat_t)\n        return control_flow_ops.group(*updates)\n\n    def get_config(self):\n        config = super(AdamWarmup, self).get_config()\n        config.update({\n            'decay_steps': self._serialize_hyperparameter('decay_steps'),\n            'warmup_steps': self._serialize_hyperparameter('warmup_steps'),\n            'min_lr': self._serialize_hyperparameter('min_lr'),\n            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n            'decay': self._serialize_hyperparameter('decay'),\n            'beta_1': self._serialize_hyperparameter('beta_1'),\n            'beta_2': self._serialize_hyperparameter('beta_2'),\n            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n        })\n        return config\n\ndef calc_train_steps(num_example, batch_size, epochs, warmup_proportion=0.1):\n    \"\"\"Calculate the number of total and warmup steps.\n    (320, 32)\n    :param num_example: Number of examples in one epoch.\n    :param batch_size: Batch size.\n    :param epochs: Number of epochs.\n    :param warmup_proportion: The proportion of warmup steps.\n    :return: Total steps and warmup steps.\n    \"\"\"\n    steps = (num_example + batch_size - 1) // batch_size\n    total = steps * epochs\n    warmup = int(total * warmup_proportion)\n    return total, warmup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpearmanRhoCallback(tf.keras.callbacks.Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind],\n                                     y_pred_val[:, ind]).correlation\n                           for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n            self.bad_epochs = 0\n            self.model.save_weights(self.model_name)\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100 * ' ' + '\\n')\n        logs['val_rho'] = rho_val\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_model(bert_trainabel, learning_rate, len_tr, BATCH_SIZE, NUM_EPOCHS, out_dim):\n    EMB_SIZE = 32\n\n    input_word_ids = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n\n    input_category = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_category')\n    input_host = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_host')\n\n    category_emb = tf.keras.layers.SpatialDropout1D(0.1)(\n        tf.keras.layers.Embedding(input_dim=6, output_dim=EMB_SIZE)(input_category))\n    host_emb = tf.keras.layers.SpatialDropout1D(0.1)(\n        tf.keras.layers.Embedding(input_dim=65, output_dim=EMB_SIZE)(input_host))\n    features_dense = tf.keras.layers.concatenate([category_emb, host_emb], axis=1)\n    features_dense = tf.keras.layers.Flatten()(features_dense)\n\n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=bert_trainabel)\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n\n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.concatenate([x, features_dense])\n\n    out = tf.keras.layers.Dense(out_dim, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(inputs=[input_word_ids, input_masks, input_segments, input_category, input_host],\n                                  outputs=out)\n    decay_steps, warmup_steps = calc_train_steps(\n        len_tr,\n        batch_size=BATCH_SIZE,\n        epochs=NUM_EPOCHS,\n        )\n    adamW_opt = AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=learning_rate, min_lr=0,)\n    # Nadam = tf.keras.optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n    model.compile(loss='binary_crossentropy', optimizer=adamW_opt)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/qa2bert1230","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nMODEL_PATH = '../input/qa2bert1230'\nall_predictions_q = []\nstart_time = time.time()\nfor fold_ in range(0, 5):\n    print('fold: ', fold_)\n    K.clear_session()\n    \n    model_name = f'{MODEL_PATH}/Qbert_fold{fold_}.h5'\n    \n    model = bert_model(bert_trainabel=False, learning_rate=5e-5, len_tr=len(df_test), BATCH_SIZE=4, NUM_EPOCHS=4, out_dim=len(q_targets))\n    model.load_weights(model_name)\n\n    all_predictions_q.append(model.predict(Qinputs_te))\n    print(\"time elapsed: {:<5.2}m\".format((time.time() - start_time) / 60))\n\nall_predictions_a = []\nfor fold_ in range(0, 5):\n    print('fold: ', fold_)\n    K.clear_session()\n    \n    model_name = f'{MODEL_PATH}/Abert_fold{fold_}.h5'\n    \n    model = bert_model(bert_trainabel=False, learning_rate=5e-5, len_tr=len(df_test), BATCH_SIZE=4, NUM_EPOCHS=4, out_dim=len(a_targets))\n    model.load_weights(model_name)\n\n    all_predictions_a.append(model.predict(Ainputs_te))\n    print(\"time elapsed: {:<5.2}m\".format((time.time() - start_time) / 60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_q = np.mean(all_predictions_q, axis=0)\ntest_preds_a = np.mean(all_predictions_a, axis=0)\nprint(test_preds_q.shape, test_preds_a.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds1 = np.concatenate([test_preds_q, test_preds_a], axis=1)\nprint(test_preds1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = df_train.columns.tolist()[11:42]\n\noutput_categories = list(df_train.columns[11:])\nprint('\\noutput categories:\\n\\t', output_categories)\n\ntargets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n    return input_ids\n\ndef my_pad(text, max_length, tokenizer):\n    res = tokenizer.tokenize(text)\n    if len(res) > max_length:\n        head_length = int(0.25 * max_length)\n        tail_length = max_length - head_length\n        res = res[:head_length] + res[-tail_length:]\n    return res\n\ndef _trim_input(title, question, answer, max_sequence_length,\n                t_max_len=30, q_max_len=239, a_max_len=239):\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n\n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len + q_len + a_len + 4) > max_sequence_length:\n\n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len) / 2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len) / 2)\n        else:\n            t_new_len = t_max_len\n\n        if a_max_len > a_len:\n            a_new_len = a_len\n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n\n        if t_new_len + a_new_len + q_new_len + 4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"\n                             % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n\n        head_t_new_len = int(0.25 * t_new_len)\n        tail_t_new_len = t_new_len - head_t_new_len\n\n        head_q_new_len = int(0.25 * q_new_len)\n        tail_q_new_len = q_new_len - head_q_new_len\n\n        head_a_new_len = int(0.25 * a_new_len)\n        tail_a_new_len = a_new_len - head_a_new_len\n\n        t = t[:head_t_new_len] + t[-tail_t_new_len:]\n        q = q[:head_q_new_len] + q[-tail_q_new_len:]\n        a = a[:head_a_new_len] + a[-tail_a_new_len:]\n\n    return t, q, a\n\ndef prepare_data(df, tokenizer):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n        question_title_ = str(instance['question_title']).lower()\n        question_body_ = str(instance['question_body']).lower()\n        answer_ = str(instance['answer']).lower()\n#         question_user_name_ = instance['question_user_name']\n#         answer_user_name_ = instance['answer_user_name']\n\n#         question_title_ = my_pad(question_title_, max_length=15, tokenizer=tokenizer)\n#         question_body_ = my_pad(question_body_, max_length=245, tokenizer=tokenizer)\n#         answer_ = my_pad(answer_, max_length=248, tokenizer=tokenizer)\n#         question_user_name_ = my_pad(question_user_name_, max_length=2, tokenizer=tokenizer)\n#         answer_user_name_ = my_pad(answer_user_name_, max_length=2, tokenizer=tokenizer)\n\n        question_title_, question_body_, answer_ = _trim_input(question_title_, question_body_, answer_, MAX_SEQUENCE_LENGTH,\n                                                               t_max_len=15, q_max_len=245, a_max_len=248)\n\n        stoken = [\"[CLS]\"] + question_title_ + [\",\"] + question_body_ + [\"[SEP]\"] + answer_ + [\"[SEP]\"]\n\n        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n\n        input_ids.append(input_ids_)\n        input_masks.append(input_masks_)\n        input_segments.append(input_segments_)\n\n    return [np.asarray(input_ids, dtype=np.int32),\n            np.asarray(input_masks, dtype=np.int32),\n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])\n\noutputs = compute_output_arrays(df_train, output_categories)\n\n# inputs = prepare_data(df_train, tokenizer)\ntest_inputs = prepare_data(df_test, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyLabelEncoder(object):\n    \"\"\"safely handle unknown label\"\"\"\n    def __init__(self):\n        self.mapper = {}\n\n    def fit(self, X):\n        uniq_X = np.unique(X)\n        # reserve 0 for unknown\n        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n        return self\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)\n\n    def _map(self, x):\n        return self.mapper.get(x, 0)\n\n    def transform(self, X):\n        return list(map(self._map, X))\ndef get_cate_feat(df, isTrain):\n    if isTrain:\n        label_encoder[\"category\"] = MyLabelEncoder()\n        category_feat = np.array(label_encoder[\"category\"].fit_transform(df[\"category\"]))\n        label_encoder[\"host\"] = MyLabelEncoder()\n        host_feat = np.array(label_encoder[\"host\"].fit_transform(df[\"host\"]))\n    else:\n        category_feat = np.array(label_encoder[\"category\"].transform(df[\"category\"]))\n        host_feat = np.array(label_encoder[\"host\"].transform(df[\"host\"]))\n    return [category_feat, host_feat]\nlabel_encoder = {}\ncate_feat_tr = get_cate_feat(df_train, isTrain=True)\ncate_feat_te = get_cate_feat(df_test, isTrain=False)\n\n# inputs.extend(cate_feat_tr)\ntest_inputs.extend(cate_feat_te)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpearmanRhoCallback(tf.keras.callbacks.Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind],\n                                     y_pred_val[:, ind]).correlation\n                           for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n            self.bad_epochs = 0\n            self.model.save_weights(self.model_name)\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100 * ' ' + '\\n')\n        logs['val_rho'] = rho_val\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\nimport itertools\n\ndef spearman_loss(y_true, y_pred):\n  return K.var(y_pred - y_true, axis=-1) / (K.std(y_pred, axis=-1)*K.std(y_true, axis=-1))# +K.random_uniform(shape=y_true.shape, minval=0.0, maxval=0.0001))\n\ndef bert_model(bert_trainabel, learning_rate, len_tr, BATCH_SIZE, NUM_EPOCHS):\n    EMB_SIZE = 32\n\n    input_word_ids = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n\n    input_category = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_category')\n    input_host = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_host')\n\n    category_emb = tf.keras.layers.SpatialDropout1D(0.1)(\n        tf.keras.layers.Embedding(input_dim=6, output_dim=EMB_SIZE)(input_category))\n    host_emb = tf.keras.layers.SpatialDropout1D(0.1)(\n        tf.keras.layers.Embedding(input_dim=65, output_dim=EMB_SIZE)(input_host))\n    features_dense = tf.keras.layers.concatenate([category_emb, host_emb], axis=1)\n    features_dense = tf.keras.layers.Flatten()(features_dense)\n\n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=bert_trainabel)\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n\n    print('pooled_output: ', pooled_output)\n    print('sequence_output: ', sequence_output)\n\n    # bert_feat = tf.keras.layers.Dense(EMB_SIZE, activation=\"relu\")(pooled_output)\n    # fm_emb = [bert_feat, category_emb, host_emb]\n    # sum_add = tf.keras.layers.add(fm_emb)\n    # sum_add = tf.keras.layers.multiply([sum_add,sum_add])\n    # add_sum = []\n    # for layer in fm_emb:\n    #     add_sum.append(tf.keras.layers.multiply([layer,layer]))\n    # add_sum = tf.keras.layers.add(add_sum)\n        \n    # subtract_layer = tf.keras.layers.Lambda(lambda inputs: inputs[0] - inputs[1],output_shape=lambda shapes: shapes[0])\n    # fm_part = subtract_layer([sum_add, add_sum])\n    # fm_part  = tf.keras.layers.Lambda(lambda x: x * 0.5)(fm_part)\n    # fm_part = tf.keras.layers.Dropout(0.5)(fm_part)\n    # fm_part = tf.keras.layers.Flatten()(fm_part)\n    # print(fm_part)\n\n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.concatenate([x, features_dense])\n\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(inputs=[input_word_ids, input_masks, input_segments, input_category, input_host],\n                                  outputs=out)\n    decay_steps, warmup_steps = calc_train_steps(\n        len_tr,\n        batch_size=BATCH_SIZE,\n        epochs=NUM_EPOCHS,\n        )\n    adamW_opt = AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=learning_rate, min_lr=0,)\n    # Nadam = tf.keras.optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n    model.compile(loss='binary_crossentropy', optimizer=adamW_opt)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/bert-weights-repeatkfold1224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nMODEL_PATH = '../input/bert-weights-repeatkfold1224'\ny_test = np.zeros((len(test_inputs), len(targets)))\nall_predictions = []\nstart_time = time.time()\nfor fold_ in range(1, 11):\n    print('fold: ', fold_)\n    K.clear_session()\n    \n    model_name = f'{MODEL_PATH}/bert_fold{fold_}.h5'\n    \n    model = bert_model(bert_trainabel=False, learning_rate=5e-5, len_tr=len(y_test), BATCH_SIZE=4, NUM_EPOCHS=4)\n    model.load_weights(model_name)\n\n    all_predictions.append(model.predict(test_inputs))\n    print(\"time elapsed: {:<5.2}m\".format((time.time() - start_time) / 60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds2 = np.mean(all_predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = test_preds1 * 0.5 + test_preds2 * 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.copy(test_preds)\n\nhyper = pd.read_csv('../input/bert-base/hyper2.csv')\nsubmission = pd.read_csv(PATH+'sample_submission.csv')\ncol1 = hyper.loc[hyper[['score1', 'score2']].max(axis=1) == hyper.score1, 'col'].tolist()\n\nfor col in col1:\n    colidx = target_col.index(col)\n    pred[:, colidx] = (pred[:, colidx]//(1/90))/90\n\nfor col in [x for x in target_col if x not in col1]:\n    if hyper.loc[hyper.col==col, 'pct'].values == 1:\n        pct = hyper.loc[hyper.col==col, 'pct'] - 0.005\n    elif hyper.loc[hyper.col==col, 'pct'].values == 0:\n        pct = hyper.loc[hyper.col==col, 'pct'] + 0.005\n    else:\n        pct = hyper.loc[hyper.col==col, 'pct']\n        \n    changerow = int(len(df_test) * pct)\n    colidx = target_col.index(col)\n    \n    if hyper.loc[hyper.col==col, 'choice'].values =='low':\n        rowidx = pred[:, colidx].argsort()[:changerow]\n        pred[rowidx, colidx] = 0\n    elif hyper.loc[hyper.col==col, 'choice'].values =='up':\n        rowidx = pred[:, colidx].argsort()[-changerow:]\n        pred[rowidx, colidx] = 1\n    else:\n        print('Wrong!')\n\nsubmission[target_col] = pred\n\n\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align:center;font-size:200%;;\">Real or Not? NLP with Disaster Tweets</h1>\n<img src=\"https://www.kdnuggets.com/wp-content/uploads/slideshare-data-mining-wordle.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"## Work path in this notebook, (inspired by [@shahules](https://www.kaggle.com/shahules/tweets-complete-eda-and-basic-modeling) and [@marcovasquez](https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud/notebook)).\n- Basic EDA\n- Data Cleaning\n- Feature engineering \n- Machine learning models"},{"metadata":{},"cell_type":"markdown","source":"* # util Libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 10000)\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,GRU,Dense\nfrom keras.initializers import Constant\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data and print head and get in touch"},{"metadata":{"trusted":true},"cell_type":"code","source":"rawtrain = pd.read_csv('../input/nlp-getting-started/train.csv')\nrawtest = pd.read_csv('../input/nlp-getting-started/test.csv')\nrawtrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets convert target to object type.\nrawtrain['target'] = rawtrain['target'].astype(object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(rawtrain.shape[0],rawtrain.shape[1]))\nprint('There are {} rows and {} columns in train'.format(rawtest.shape[0],rawtest.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NaN values\nrawtrain.isnull().sum()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets make some world Cloud\ntext = rawtrain.text.values\nwordcloud = WordCloud(width = 5000, height = 2500,background_color = 'white',\n                      stopwords = STOPWORDS).generate(str(text))\n\nfig = plt.figure(figsize = (16, 10), facecolor = 'k', edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keyword countvalues\nrawtrain.keyword.value_counts()[:10].plot.bar(color='green',);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration\nClass distribution"},{"metadata":{},"cell_type":"markdown","source":"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"rawtrain.groupby('target').target.value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets), And sample is not unballanced"},{"metadata":{},"cell_type":"markdown","source":"### Number of characters in tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize = (18,6))\n\ntweet_len = rawtrain[rawtrain['target'] == 1]['text'].str.len()\nax1.hist(tweet_len,color = 'black')\nax1.set_title('disaster tweets')\n\n\ntweet_len = rawtrain[rawtrain['target'] == 0]['text'].str.len()\nax2.hist(tweet_len,color = 'green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both."},{"metadata":{},"cell_type":"markdown","source":"### Length of text in tweets, "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize = (18,5))\ntweet_len = rawtrain[rawtrain['target'] == 1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color = 'orange')\nax1.set_title('disaster tweets')\n\ntweet_len = rawtrain[rawtrain['target'] ==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color = 'purple')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sounds we've good distribtion here."},{"metadata":{},"cell_type":"markdown","source":"###  Average word length in a tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(18,6))\nword1 = rawtrain[rawtrain['target'] == 1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\n\nword0 = rawtrain[rawtrain['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word0.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering \nBased on what we've in the above we can make some interesting feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([rawtrain,  rawtest])\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['body_len'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count/(len(text) - text.count(\" \")), 3)*100\n\ndata['punct%'] = data['text'].apply(lambda x: count_punct(x))\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate created features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(1,figsize = (18,8))\nbins = np.linspace(0, 200, 40)\nplt.hist(data[data['target'] == 1]['body_len'], bins, alpha=0.5, density=True, label='1')\nplt.hist(data[data['target'] == 0]['body_len'], bins, alpha=0.5, density=True, label='0')\nplt.legend(loc='upper left')\nplt.xlim(0,150)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target equal to 0 est quite bite longuer than 1. Maybe in urgent situation people do not have time to write lot, or are hurry to post first! But when the text len is more than 60 we have target 1 as longuer!! Intresting."},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = np.linspace(0, 50, 40)\nplt.subplots(1,figsize = (18,8))\nplt.hist(data[data['target']==1]['punct%'], bins, alpha=0.5, density=True, label='1')\nplt.hist(data[data['target']==0]['punct%'], bins, alpha=0.5, density=True, label='0')\nplt.legend(loc='upper right')\nplt.xlim(0,45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have also more punct, target 0, but notice the peak for 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['punct%', 'body_len']].kurt()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we noticed our data are tailled Kurtosis important [ref.](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm), we will deal with this in the next step"},{"metadata":{},"cell_type":"markdown","source":"### Data transformation"},{"metadata":{},"cell_type":"markdown","source":"### Box-Cox Power Transformation\n\n**Base Form**: $$ y^x $$\n\n| X    | Base Form           |           Transformation               |\n|------|--------------------------|--------------------------|\n| -2   | $$ y ^ {-2} $$           | $$ \\frac{1}{y^2} $$      |\n| -1   | $$ y ^ {-1} $$           | $$ \\frac{1}{y} $$        |\n| -0.5 | $$ y ^ {\\frac{-1}{2}} $$ | $$ \\frac{1}{\\sqrt{y}} $$ |\n| 0    | $$ y^{0} $$              | $$ log(y) $$             |\n| 0.5  | $$ y ^ {\\frac{1}{2}}  $$ | $$ \\sqrt{y} $$           |\n| 1    | $$ y^{1} $$              | $$ y $$                  |\n| 2    | $$ y^{2} $$              | $$ y^2 $$                |\n\n\n**Process**\n1. Determine what range of exponents to test\n2. Apply each transformation to each value of your chosen feature\n3. Use some criteria to determine which of the transformations yield the best distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,7):\n    fig = plt.subplots(figsize=(10,4))\n    plt.hist((data['punct%'])**(1/i), bins=35)\n    plt.hist((data['body_len'])**(1/i), bins=35)\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we go transformation become better and better, I'll chose the third one for punct%. Body_len seem to not respond.\n- We notice also stacked bar in left, that just mean 0, for tweets without punctuations."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['punct%tr'] = data['punct%']**(1/3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    links = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = links.sub(r'',text)\n    tags = re.compile(r'<.*?>')\n    text = tags.sub(r'',text)\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text =  emoji_pattern.sub(r'', text)\n    tokens = re.split('\\W+', text)\n    #text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cleaned_text'] = data['text'].apply(lambda x: clean_text(x.lower()))\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's stem tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nps = nltk.PorterStemmer()\nstopword = nltk.corpus.stopwords.words('english')\ndef stemming(text):\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopword]\n    return text\ndata['stem_text'] = data['cleaned_text'].apply(lambda x: stemming(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntfidf_vect = TfidfVectorizer(analyzer=stemming)\nX_tfidf = tfidf_vect.fit_transform(data['cleaned_text'])\ntfidframe = pd.DataFrame(X_tfidf.toarray())\n\n# For concate\ndata = data.reset_index(drop=True)\nX_features = pd.concat([data['body_len'], data['punct%'], tfidframe], axis=1)\nX_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Re organize data for algoritms\ntrain = X_features[:rawtrain.shape[0]]\ntest = X_features[rawtrain.shape[0]:]\ny_train = rawtrain['target'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(y_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n\n\n# Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf = RandomForestClassifier()\nparam = {'n_estimators': [10, 50, 100],\n        'max_depth': [10, 20,  None]}\n\ngs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\ngs_fit = gs.fit(train, y_train)\npd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To be continued... pleas like if it help, and correct me if I'm wrong or doing un-necessary things.\n- keeplearning"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":1}
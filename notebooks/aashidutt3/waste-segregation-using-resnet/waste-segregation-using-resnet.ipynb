{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\n\n# Check for CPU/GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_transform = transforms.Compose([transforms.Resize(255),\n                                      transforms.CenterCrop(224),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.RandomRotation(20),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\ntrain_data = datasets.ImageFolder(root=\"../input/waste-segregation-image-dataset/Dataset/train\", transform=train_transform)\ntrainDataLoader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n\n\ntest_transform = transforms.Compose([transforms.Resize(255),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\ntest_data = datasets.ImageFolder(root=\"../input/waste-segregation-image-dataset/Dataset/val\", transform=test_transform)\ntestDataLoader = DataLoader(dataset=test_data, batch_size=32, shuffle=True)\n\n# Load Pre-trained model\nmodel = models.resnet50(pretrained=True)\n\nprint(\"Before: \\n\", model, \"\\n\\n\")\n\n# Freeze our Feature parameters of the model\n# Model Parameters: Pre-Learned Weights & Biases\nfor param in model.parameters():\n    # Don't allow gradients/features to update.\n    param.requires_grad = False\n\n# Replace the Classification layer with our custom classification layer\nfrom collections import OrderedDict\n\n# Define a sequence of custom layers\n# Pass in a OrderedDict to name each of these layers and corresponding functions\nclassifier = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(2048, 512)),\n    ('relu', nn.ReLU()),\n    ('dropout', nn.Dropout(p=0.2)),\n    ('fc2', nn.Linear(512, 2)),\n    ('output', nn.LogSoftmax(dim=1))\n]))\n\n# Add custom layers to the pre-trained model\nmodel.fc = classifier\n\nprint(\"After: \\n\", model, \"\\n\")\n\n# Train the model with newly added classifier layer\nmodel.to(device)\n\n# Epochs\nepochs = 10\n\n# Loss Criterion\ncriterion = nn.NLLLoss()\n\n# Optimizer\n# Since, we only need to update the parameters for the Model classifier.\n# So, we use \"model.classifier.parameters()\" instead of \"model.parameters()\".\noptimizer = optim.Adam(params=model.fc.parameters(), lr=0.001)\n\ntrain_losses, test_losses = [], []\n\nprint(\"Starting Model Training...\")\n\n# ------------- Training Loop ------------\nfor e in range(epochs):\n    running_loss = 0\n    # ------------- Training Loop -----------\n    for images, labels in trainDataLoader:\n        images, labels = Variable(images), Variable(labels)\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model.forward(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    # ------------ Validation Loop -----------\n    else:\n        test_loss = 0\n        accuracy = 0\n        with torch.no_grad():\n            model.eval()\n            for images, labels in testDataLoader:\n                images, labels = Variable(images), Variable(labels)\n                images, labels = images.to(device), labels.to(device)\n                logps = model.forward(images)\n                test_loss += criterion(logps, labels)\n                # Actual Probability distribution from Log Probabilities\n                ps = torch.exp(logps)\n                # topk: returns K largest elements of the given input tensor along a given dimension\n                top_k, top_class = ps.topk(1, dim=1)\n                # Check if Predicted Output is equal to the Actual Labels\n                equals = top_class == labels.view(*top_class.shape)\n                # Calculate Accuracy on the Test Dataset\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n\n        # ** Trun back ON the Dropouts for Model Training **\n        model.train()\n\n        # Keep track of Training and Test Loss\n        train_losses.append(running_loss / len(trainDataLoader))\n        test_losses.append(test_loss / len(testDataLoader))\n\n        print(\"Epoch: {}/{}\".format(e + 1, epochs),\n              \"Training Loss: {:.3f}\\t\".format(running_loss / len(trainDataLoader)),\n              \"Test Loss: {:.3f}\\t\".format(test_loss / len(testDataLoader)),\n              \"Test Accuracy: {:.3f}\\t\".format(accuracy / len(testDataLoader)))\n\nprint(\"\\nDone...\")","metadata":{"execution":{"iopub.status.busy":"2022-09-22T06:08:00.404619Z","iopub.execute_input":"2022-09-22T06:08:00.404949Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
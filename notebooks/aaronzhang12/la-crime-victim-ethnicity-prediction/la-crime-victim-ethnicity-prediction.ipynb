{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')\npd.set_option('display.max_columns', None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('data.csv').drop('Unnamed: 0', axis=1)\n\n#import data_prediction.csv. it's subset of the original file and will be be sufficient for testing.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Year'] = data.Year.astype('str')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Vict Descent'].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Spliting the data to data_num and data_cat\n\ndata_num = data[[x for x in data.columns if data[x].dtype != 'O']]\ndata_cat = data[[x for x in data.columns if data[x].dtype == 'O']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vict Descent to number\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\nvict_descent = label_encoder.fit_transform(data_cat['Vict Descent'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_cat['Vict Descent'] = vict_descent\n\ndata_cat['Vict Descent'].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature engineer(mean) on data_cat\n\nfor x in data_cat.columns:\n    if x != 'Vict Descent':\n        dict1 = data_cat.groupby([x])['Vict Descent'].mean().to_dict()\n        data_cat[x] = data_cat[x].map(dict1)\n        \ndf = pd.concat([data_cat, data_num], axis=1)\n\ndf.drop(['Population', 'Crime Cnt', 'Crime Rate'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation\ndf.corr()['Vict Descent'].sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 7))\nsns.heatmap(df.corr(), annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Select the best column use for analysis \n#Option1 \n\nX = df.drop('Vict Descent', axis=1)\ny = df.loc[:, 'Vict Descent']\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel = SelectFromModel(Lasso(0.005))\nmodel.fit(X, y)\nmodel.get_support()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feature = X.columns[model.get_support()]\nX[selected_feature]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Option2 \n\nfrom sklearn.feature_selection import mutual_info_regression\n\nimp = mutual_info_regression(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(imp, index=X.columns).sort_values(0, ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Option3\n\nfrom sklearn.feature_selection import SelectKBest\n\nfrom sklearn.feature_selection import chi2\n\norderd_rank_features = SelectKBest(score_func=chi2, k='all')\n\nordered_feature = orderd_rank_features.fit(X.drop(['LON'], axis=1), y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordered_feature.scores_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordered_df = pd.concat(\n    [\n     pd.DataFrame(X.drop('LON', axis=1).columns),\n     pd.DataFrame(ordered_feature.scores_, columns=['score'])\n    ], axis=1\n).sort_values('score', ascending=False)\n\nselected_features = ordered_df[:8][0].values\n\nselected_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X[selected_features]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display score\n\ndef display_scores(scores):\n    print('===============================================')\n    print('Scores: {}'.format(scores))\n    print('===============================================')\n    print('Mean Score: {}'.format(scores.mean()))\n    print('===============================================')\n    print('Standard Deviation of Scores: {}'.format(scores.std()))\n    print('===============================================')\n    \n    return None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creat the predict function \n\ndef predict(ml_model):\n    model = ml_model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    print(f'Predictions: {pred}')\n    print(f'Training Score: {model.score(X_train, y_train)}')\n    print('\\n')\n    print(f'{confusion_matrix(pred, y_test)}')\n    print('\\n')\n    print(f'Accuracy Score: {accuracy_score(pred, y_test)}')\n    print(f'Mean Squared Error: {mean_squared_error(pred, y_test)}')\n    \n    scores = cross_val_score(model,\n               X_train,\n               y_train,\n#                scoring='neg_mean_squared_error',\n               cv=10)\n    print('\\n')\n    display_scores(scores)\n    \n    plt.figure(figsize=(4,2))\n    sns.kdeplot(pred, shade=True)\n    sns.kdeplot(y_test, shade=True)\n    plt.legend(['pred', 'y_test'])\n    \n    print('\\n')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predictions - Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier()\n\npredict(tree)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predictions - forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier()\n\npredict(forest)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predictions - KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\npredict(knn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n\npredict(xgb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Parameter Tunning on XGBClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nparams={\n    'learning_rate':[0.05, 0.20, 0.25],\n    'max_depth':[5, 8, 10],\n    'min_child_weight':[1, 3, 5, 7],\n    'gamma':[0.0, 0.1, 0.2, 0.4],\n    'colsample_bytree':[0.3, 0.4, 0.7]\n}\n\nrandom_search = RandomizedSearchCV(xgb, \n                  param_distributions=params, \n                  n_iter=5,\n                  scoring='roc_auc',\n                  n_jobs=-1, \n                  cv=5,\n                  verbose=3)\n\nrandom_search.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.DataFrame(random_search.best_estimator_.feature_importances_, index=X.columns).reset_index()\n\nsns.barplot(data=features, y='index', x=0)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(random_search.best_estimator_)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Parameter Tunning on Decision Tree \n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'criterion': ['gini', 'entropy'],\n          'max_depth': [None, 10, 20, 30],\n          'min_samples_split': [2, 5, 10],\n          'min_samples_leaf': [1, 2, 4],\n          'max_features': [None, 'sqrt', 'log2']}\n\ngrid_search  = GridSearchCV(tree,\n                            params, \n                            cv=5, \n                            scoring='accuracy', \n                            n_jobs=-1)\n\ngrid_search.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_features = pd.DataFrame(grid_search.best_estimator_.feature_importances_, index=X.columns).reset_index()\n\nsns.barplot(data = tree_features, x=0, y='index')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(grid_search.best_estimator_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
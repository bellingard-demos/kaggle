{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-19T13:37:13.461104Z","iopub.execute_input":"2023-04-19T13:37:13.461443Z","iopub.status.idle":"2023-04-19T13:37:13.497516Z","shell.execute_reply.started":"2023-04-19T13:37:13.461408Z","shell.execute_reply":"2023-04-19T13:37:13.496014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e13/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:22.898616Z","iopub.execute_input":"2023-04-19T13:37:22.899053Z","iopub.status.idle":"2023-04-19T13:37:22.922628Z","shell.execute_reply.started":"2023-04-19T13:37:22.899014Z","shell.execute_reply":"2023-04-19T13:37:22.92133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:24.663541Z","iopub.execute_input":"2023-04-19T13:37:24.663966Z","iopub.status.idle":"2023-04-19T13:37:24.714111Z","shell.execute_reply.started":"2023-04-19T13:37:24.663931Z","shell.execute_reply":"2023-04-19T13:37:24.71248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:30.954355Z","iopub.execute_input":"2023-04-19T13:37:30.955274Z","iopub.status.idle":"2023-04-19T13:37:30.993077Z","shell.execute_reply.started":"2023-04-19T13:37:30.955196Z","shell.execute_reply":"2023-04-19T13:37:30.991456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.prognosis.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:35.263269Z","iopub.execute_input":"2023-04-19T13:37:35.264595Z","iopub.status.idle":"2023-04-19T13:37:35.272752Z","shell.execute_reply.started":"2023-04-19T13:37:35.264529Z","shell.execute_reply":"2023-04-19T13:37:35.271475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:36.043277Z","iopub.execute_input":"2023-04-19T13:37:36.043838Z","iopub.status.idle":"2023-04-19T13:37:36.214333Z","shell.execute_reply.started":"2023-04-19T13:37:36.043788Z","shell.execute_reply":"2023-04-19T13:37:36.213378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n1. There are 707 rows nd 66 columns. \n2. No null value is present in the data.\n3. Except id and prognosis column every other column is float datatype. \n4. Target is multiclass with 11 vector borne diseases. \n5. All the float column values are of binary type, 0.0 or 1.0.","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:40.188172Z","iopub.execute_input":"2023-04-19T13:37:40.188579Z","iopub.status.idle":"2023-04-19T13:37:40.886013Z","shell.execute_reply.started":"2023-04-19T13:37:40.188542Z","shell.execute_reply":"2023-04-19T13:37:40.884595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(20, 15))\nmask = np.zeros_like(train.corr())\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(train.corr(),mask=mask, linewidths=.5, cmap='Reds', annot=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:40.888409Z","iopub.execute_input":"2023-04-19T13:37:40.889394Z","iopub.status.idle":"2023-04-19T13:37:43.864547Z","shell.execute_reply.started":"2023-04-19T13:37:40.889338Z","shell.execute_reply":"2023-04-19T13:37:43.863386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n1. Shows most of the columns are not correlated to each other. ","metadata":{}},{"cell_type":"code","source":"labels = train.prognosis.unique()\nsizes = train.groupby('prognosis').count()['id']\nexplode = [0.1] * 11\nplt.pie(sizes, labels=labels,\n        autopct='%1.1f%%', pctdistance=0.85, explode = explode)\ncircle = plt.Circle( (0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(circle);","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:37:43.866337Z","iopub.execute_input":"2023-04-19T13:37:43.866948Z","iopub.status.idle":"2023-04-19T13:37:44.11119Z","shell.execute_reply.started":"2023-04-19T13:37:43.866909Z","shell.execute_reply":"2023-04-19T13:37:44.109715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n1. Kinda balanced dataset.","metadata":{}},{"cell_type":"markdown","source":"As there are 66 column. It will be really lengthy and time consuming to do analysis on each feature. \nSo before diving into modelling, lets do dimensionallity reduction. ","metadata":{}},{"cell_type":"markdown","source":"## Dimenisonality Reduction","metadata":{}},{"cell_type":"markdown","source":"Here I am using PCA for dimensionality reduction. Instead of mentioning number of PCA components. I have mentioned the variance need to be preserved as 0.80 (A rule of thumb is to keep at least 70-80% of the explained variance).","metadata":{}},{"cell_type":"code","source":"X = train.drop(['id','prognosis'],axis=1)\ny = train['prognosis']","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:40:19.880765Z","iopub.execute_input":"2023-04-19T13:40:19.881199Z","iopub.status.idle":"2023-04-19T13:40:19.888347Z","shell.execute_reply.started":"2023-04-19T13:40:19.881162Z","shell.execute_reply":"2023-04-19T13:40:19.886802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 0.80)\npca.fit(X)\ndata_pca = pca.transform(X)\ndata_pca.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:40:21.439113Z","iopub.execute_input":"2023-04-19T13:40:21.439566Z","iopub.status.idle":"2023-04-19T13:40:21.466173Z","shell.execute_reply.started":"2023-04-19T13:40:21.439526Z","shell.execute_reply":"2023-04-19T13:40:21.464718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This result shows cols are highly non correlated and 50 cols accounts for 0.95 variance out of 66 columns. ","metadata":{}},{"cell_type":"code","source":"X = pd.DataFrame(data_pca,columns=['pca'+ str(i) for i in range(1, 33, 1)])\nX","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:40:25.214507Z","iopub.execute_input":"2023-04-19T13:40:25.214957Z","iopub.status.idle":"2023-04-19T13:40:25.250989Z","shell.execute_reply.started":"2023-04-19T13:40:25.214917Z","shell.execute_reply":"2023-04-19T13:40:25.249745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(20, 15))\nmask = np.zeros_like(X.corr())\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(X.corr(),mask=mask, linewidths=.5, cmap='Reds', annot=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:40:35.551275Z","iopub.execute_input":"2023-04-19T13:40:35.551682Z","iopub.status.idle":"2023-04-19T13:40:36.650502Z","shell.execute_reply.started":"2023-04-19T13:40:35.551647Z","shell.execute_reply":"2023-04-19T13:40:36.649294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder()\ny = y.values.reshape((-1,1))\ny = encoder.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:40:53.511161Z","iopub.execute_input":"2023-04-19T13:40:53.512374Z","iopub.status.idle":"2023-04-19T13:40:53.518768Z","shell.execute_reply.started":"2023-04-19T13:40:53.512323Z","shell.execute_reply":"2023-04-19T13:40:53.517474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"markdown","source":"Since we are not sure which model to use. Lets try all the models we are familiar and chose the best out of it. Here I am not doing any otimization techniques, just a basic version of all models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:42:00.281398Z","iopub.execute_input":"2023-04-19T13:42:00.282222Z","iopub.status.idle":"2023-04-19T13:42:02.83429Z","shell.execute_reply.started":"2023-04-19T13:42:00.282171Z","shell.execute_reply":"2023-04-19T13:42:02.832893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 0\nsplits = 5\nk = StratifiedKFold(n_splits = splits, random_state = seed, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:47:52.310389Z","iopub.execute_input":"2023-04-19T13:47:52.310813Z","iopub.status.idle":"2023-04-19T13:47:52.316864Z","shell.execute_reply.started":"2023-04-19T13:47:52.310777Z","shell.execute_reply":"2023-04-19T13:47:52.315637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Model_selection(model,X, y, cv = k, label = ''):\n    \n    train_roc_auc_score, val_roc_auc_score = [], []\n    train_map3, val_map3 = [], []\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n                \n        model.fit(X.iloc[train_idx], y[train_idx])\n        \n        train_preds = model.predict_proba(X.iloc[train_idx])\n        val_preds = model.predict_proba(X.iloc[val_idx])\n        \n        train_score = roc_auc_score(y[train_idx], train_preds, multi_class='ovr')\n        val_score = roc_auc_score(y[val_idx], val_preds, multi_class='ovr')\n        \n        train_roc_auc_score.append(train_score)\n        val_roc_auc_score.append(val_score)\n        \n        #select three most probable prognosis based on train dataset prediction\n        train_index = np.argsort(-train_preds)[:,:3] #return index of three most probable prognosis\n        \n        #select three most probable prognosis based on validation dataset prediction\n        val_index = np.argsort(-val_preds)[:,:3]\n    \n        #calculate map@3\n        train_score = mapk(y[train_idx].reshape(-1, 1), train_index, 3)\n        val_score = mapk(y[val_idx].reshape(-1, 1), val_index, 3)\n        \n        train_map3.append(train_score)\n        val_map3.append(val_score)\n    \n    print(f'Val roc_auc_score   : {np.mean(val_roc_auc_score):.5f} | Train roc_auc_score   : {np.mean(train_roc_auc_score):.5f} | {label}')\n    print(f'Val MAP@3 Score: {np.mean(val_map3):.5f} | Train MAP@3 Score: {np.mean(train_map3):.5f} | {label}\\n')\n    \n    return val_roc_auc_score, val_map3","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:47:53.634741Z","iopub.execute_input":"2023-04-19T13:47:53.635277Z","iopub.status.idle":"2023-04-19T13:47:53.648255Z","shell.execute_reply.started":"2023-04-19T13:47:53.635208Z","shell.execute_reply":"2023-04-19T13:47:53.646804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sourced from the ml_metrics package at https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\nimport numpy as np\n\ndef apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the average prescision at k between two lists of\n    items.\n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n        \n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=10):\n    \"\"\"\n    Computes the mean average precision at k.\n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n                 predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:47:54.444191Z","iopub.execute_input":"2023-04-19T13:47:54.444627Z","iopub.status.idle":"2023-04-19T13:47:54.457908Z","shell.execute_reply.started":"2023-04-19T13:47:54.444587Z","shell.execute_reply":"2023-04-19T13:47:54.456511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logloss_list, map3_list = pd.DataFrame(), pd.DataFrame()\n\nmodels = [\n    ('log', LogisticRegression(random_state = seed, max_iter = 1000)),\n    ('svc', SVC(random_state = seed, probability = True)),\n    ('gauss', GaussianProcessClassifier(random_state = seed)),\n    ('rf', RandomForestClassifier(random_state = seed)),\n    ('xgb', XGBClassifier(random_state = seed, objective = 'multi:softprob', eval_metric = 'map@3')),\n    ('lgb', LGBMClassifier(random_state = seed, objective = 'softmax', metric = 'softmax')),\n    ('gb', GradientBoostingClassifier(random_state = seed)),\n    ('ada', AdaBoostClassifier(random_state = seed)),\n    ('knn', KNeighborsClassifier())\n]\n\nfor (label, model) in models:\n    (logloss_list[label], map3_list[label]) = Model_selection(model, label = label, X= X, y=y.ravel())","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:51:45.955033Z","iopub.execute_input":"2023-04-19T13:51:45.95599Z","iopub.status.idle":"2023-04-19T13:53:08.995189Z","shell.execute_reply.started":"2023-04-19T13:51:45.955943Z","shell.execute_reply":"2023-04-19T13:53:08.993408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choosing SVC model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:11:37.314999Z","iopub.execute_input":"2023-04-19T14:11:37.315424Z","iopub.status.idle":"2023-04-19T14:11:37.323016Z","shell.execute_reply.started":"2023-04-19T14:11:37.315388Z","shell.execute_reply":"2023-04-19T14:11:37.321489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning the model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\ngrid = GridSearchCV(SVC(random_state = seed, probability = True),param_grid,refit=True,verbose=False)\ngrid.fit(X,y.ravel())\n\nprint(grid.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:17:42.919491Z","iopub.execute_input":"2023-04-19T14:17:42.920037Z","iopub.status.idle":"2023-04-19T14:18:28.3871Z","shell.execute_reply.started":"2023-04-19T14:17:42.919984Z","shell.execute_reply":"2023-04-19T14:18:28.385756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(random_state = seed, probability = True,C=1, gamma = 0.1)\nsvc.fit(X_train, y_train)\nprint(\"Training Accuracy\", svc.score(X_train, y_train))\nprint(\"Testing Accuracy\" , svc.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:19:13.179408Z","iopub.execute_input":"2023-04-19T14:19:13.179799Z","iopub.status.idle":"2023-04-19T14:19:13.403327Z","shell.execute_reply.started":"2023-04-19T14:19:13.179765Z","shell.execute_reply":"2023-04-19T14:19:13.402032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top3(X_test, flag = False):\n    predictions = svc.predict_proba(X_test)\n    prediction_index = np.argsort(-predictions, axis=1)\n    top_3_pred = prediction_index[:,:3]\n    original_shape = top_3_pred.shape\n    if flag:\n        top_3_pred = encoder.inverse_transform(top_3_pred.reshape(-1, 1))\n    top_3_pred = top_3_pred.reshape(original_shape)\n    return top_3_pred","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:11:22.300372Z","iopub.execute_input":"2023-04-19T14:11:22.300761Z","iopub.status.idle":"2023-04-19T14:11:22.30751Z","shell.execute_reply.started":"2023-04-19T14:11:22.300726Z","shell.execute_reply":"2023-04-19T14:11:22.306306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_3 = top3(X_test)\nmapk(y_test.reshape(-1, 1), top_3, k=3)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:19:21.21024Z","iopub.execute_input":"2023-04-19T14:19:21.210664Z","iopub.status.idle":"2023-04-19T14:19:21.229721Z","shell.execute_reply.started":"2023-04-19T14:19:21.210626Z","shell.execute_reply":"2023-04-19T14:19:21.228455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/playground-series-s3e13/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:11:50.761387Z","iopub.execute_input":"2023-04-19T14:11:50.761816Z","iopub.status.idle":"2023-04-19T14:11:50.80221Z","shell.execute_reply.started":"2023-04-19T14:11:50.76178Z","shell.execute_reply":"2023-04-19T14:11:50.800971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_df.drop(['id'], axis=1)\ntest = pca.transform(test)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:11:54.729374Z","iopub.execute_input":"2023-04-19T14:11:54.730285Z","iopub.status.idle":"2023-04-19T14:11:54.743022Z","shell.execute_reply.started":"2023-04-19T14:11:54.730214Z","shell.execute_reply":"2023-04-19T14:11:54.741395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.DataFrame(test,columns=['pca'+ str(i) for i in range(1, 33, 1)])\ntop3_pred_test = top3(test, True)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:19:53.765471Z","iopub.execute_input":"2023-04-19T14:19:53.765897Z","iopub.status.idle":"2023-04-19T14:19:53.788833Z","shell.execute_reply.started":"2023-04-19T14:19:53.76586Z","shell.execute_reply":"2023-04-19T14:19:53.787761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['prognosis'] = np.apply_along_axis(lambda x: np.array(' '.join(x), dtype=\"object\"), 1, top3_pred_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:31:37.863745Z","iopub.execute_input":"2023-04-19T14:31:37.864378Z","iopub.status.idle":"2023-04-19T14:31:37.874408Z","shell.execute_reply.started":"2023-04-19T14:31:37.864315Z","shell.execute_reply":"2023-04-19T14:31:37.872773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('submission.csv', columns=['id', 'prognosis'], index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:19:58.414888Z","iopub.execute_input":"2023-04-19T14:19:58.415341Z","iopub.status.idle":"2023-04-19T14:19:58.424545Z","shell.execute_reply.started":"2023-04-19T14:19:58.4153Z","shell.execute_reply":"2023-04-19T14:19:58.422992Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"cells":[{"metadata":{"_cell_guid":"dc4dbfda-7d98-463d-ba9e-cca5f39647a1","_uuid":"f468c9b445e96e3f0657ab3e600b8112e6a493ba"},"cell_type":"markdown","source":"# GridSearchCV for hyper-parameters tuning XGBoost models using Pipelines "},{"metadata":{"_cell_guid":"c7b0e891-a1fb-446d-88ef-e37ede7770b9","_uuid":"4ccf23a827e5ac3096636f0269b0acf5f3b71010"},"cell_type":"markdown","source":"`GridSearchCV` is a cross validation technique for tuning a model. Similar to `cross_val_score`, it uses the cross validation process explained in the [Cross-Validation tutorial](https://www.kaggle.com/dansbecker/cross-validation). However, the main objective of `GridSearchCV` is to find the most optimal parameters. We pass on a parameters' dictionary to the function and the function compares the cross-validation score for each combination of parameters in the dictionary and returns the set of best parameters. The tutorial specifically covers `XGBBoost` since they are very sensitive to hyperparameters' tuning and here we demonstrate how to use early stopping rounds in `GridSearchCV`.\n\n\nWe won't focus on the data loading. For now, you can imagine you are at a point where you already have train_X, val_X, train_y, and val_y. "},{"metadata":{"_kg_hide-input":true,"_cell_guid":"5d4817aa-562c-4e4d-b399-e87948f67aed","_uuid":"cbc2e5c7480d82e1ae7236a3a84f7693978ec451","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# Read Data\ndata = pd.read_csv('../input/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = np.array(data[cols_to_use])\ny = data.Price\ntrain_X, val_X, train_y, val_y = train_test_split(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3beacbfd-5bbe-41d2-a0d5-af7ccdb59ba6","_uuid":"38aa7d15d805328167f6f51212fdb19d50ed0f3b"},"cell_type":"markdown","source":"### Building a pipeline"},{"metadata":{"_cell_guid":"705489af-9da7-4ddc-8b12-4c1866a63cd3","_uuid":"65189c0aa62c1caa07ab967d275b2b514c23e92d"},"cell_type":"markdown","source":"We use `Imputer` to fill in missing values, followed by a `XGBRegressor` to make predictions.  These can be bundled together in a pipeline as shown below."},{"metadata":{"_cell_guid":"a5100ea6-b993-4165-b856-0d600f54e292","_uuid":"cfe500eff04a4a62300a11aad88905a85010fe5f","collapsed":true,"trusted":false},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = Pipeline([('imputer', Imputer()), ('xgbrg', XGBRegressor())])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2cdaa0fa-9a16-4b74-bb7f-cff65a7692be","_uuid":"2f088595a00af9518afe8938416cc8b1871f9ca3"},"cell_type":"markdown","source":"The above code is similar to the [Pipeline tutorial](https://www.kaggle.com/dansbecker/pipelines) except that here we use `Pipeline` instead of `make_pipeline` because we want to have a name for every step in our pipeline so that we can call on a step and set parameters."},{"metadata":{"_cell_guid":"051ba34d-a568-464e-8a51-977593429bb7","_uuid":"4312a41f7a3e75835a34bb8270d2d151b80664d3"},"cell_type":"markdown","source":"### GridSearchCV for tuning the model\n\nHere we use `GridSearchCV` on our pipeline and train it on our training set. We are using 5-fold cross validation by passing the argument `cv=5` in the `GridSearchCV`. The `param_grid` is the dictionary for the values of parameters that we want to compare. Using `GridSearchCV` on a pipeline is very similar to use it on a regressor/classifier, except that we add the name of the regression (here `xgbrg__`) in front of the parameters' names in the `param_grid`. "},{"metadata":{"_cell_guid":"ee68bcc0-5aab-4407-90be-cf15796c3399","_uuid":"09132a1d1b5a367ed536406ea8e1c936080e0b7a","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"xgbrg__n_estimators\": [10, 50, 100, 500],\n    \"xgbrg__learning_rate\": [0.1, 0.5, 1],\n}\n\nfit_params = {\"xgbrg__eval_set\": [(val_X, val_y)], \n              \"xgbrg__early_stopping_rounds\": 10, \n              \"xgbrg__verbose\": False} \n\nsearchCV = GridSearchCV(my_pipeline, cv=5, param_grid=param_grid, fit_params=fit_params)\nsearchCV.fit(train_X, train_y)  ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08eb543b-a181-424a-b276-717d1f148474","_uuid":"ee93e4c137fc3f2cd778e54017ed01e48cb888f5"},"cell_type":"markdown","source":"As explained in [Learning to Use XGBoost tutorial](https://www.kaggle.com/dansbecker/learning-to-use-xgboost), the number of trees in XGBoost models, that is `n_estimators`, are tuned by using `early_stopping_rounds`. The early stopping is decided by checking the prediction of the trained models on a validation set, and hence it is required that we pass an `eval_set` alongside the `early_stopping_rounds` in the `fit_params`."},{"metadata":{"_cell_guid":"5d1225d6-bf34-4844-bdf9-407d98d12566","_uuid":"0a807c5ccd2b95593f19acda2b5064692fc3d72b","collapsed":true,"trusted":false},"cell_type":"code","source":"searchCV.best_params_ ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ee20a57-ba71-462f-bc00-55504e747200","_uuid":"d9f566437bcaa5aa05af3a8c563a768bbcee800c","collapsed":true,"trusted":false},"cell_type":"code","source":"searchCV.cv_results_['mean_train_score']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dfb27a35-2437-4664-ac9d-91881608f4ef","_uuid":"eede73d18994addce96fc46cd52002ce672f6e4e","collapsed":true,"trusted":false},"cell_type":"code","source":"searchCV.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92a0ced5-f800-4815-8b6f-c1e590ba9aef","_uuid":"e821a6aa80c316e86e363f8d8c18860954a101b5","collapsed":true,"trusted":false},"cell_type":"code","source":"searchCV.cv_results_['mean_train_score'].mean(), searchCV.cv_results_['mean_test_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0638a4bf-7335-431e-9669-ba0fed147026","_uuid":"e7249b3cdf4d5a17ab065a984bef2f513e93c636","collapsed":true},"cell_type":"markdown","source":"More to come!"},{"metadata":{"_cell_guid":"44e3d8dc-c66c-4365-92dd-b84db4292e27","_uuid":"2c1a0c05a2275b11a00af24b40caa4da2ca313a1","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nimport pandas as pd\nimport numpy as np\nimport time\nimport re\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\nfrom requests.exceptions import Timeout\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:31:00.167214Z","iopub.execute_input":"2023-03-10T06:31:00.168554Z","iopub.status.idle":"2023-03-10T06:31:00.285065Z","shell.execute_reply.started":"2023-03-10T06:31:00.168497Z","shell.execute_reply":"2023-03-10T06:31:00.282801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"html = requests.get(\"https://www.ccarprice.com/au/\") #This is the website this script uses to scrape needed information\n\nif html.status_code == 200:\n    print(\"Connection established successfully!\")\nelse:\n    print(f\"Connection failed with status code {html.status_code}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:31:00.287812Z","iopub.execute_input":"2023-03-10T06:31:00.288568Z","iopub.status.idle":"2023-03-10T06:31:00.903985Z","shell.execute_reply.started":"2023-03-10T06:31:00.288518Z","shell.execute_reply":"2023-03-10T06:31:00.902385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Turning response object fom \"html\" variable into beautifulsoup object to crawl through the site\nsoup = BeautifulSoup(html.content, 'html.parser')","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:31:00.905684Z","iopub.execute_input":"2023-03-10T06:31:00.905995Z","iopub.status.idle":"2023-03-10T06:31:00.959549Z","shell.execute_reply.started":"2023-03-10T06:31:00.905965Z","shell.execute_reply":"2023-03-10T06:31:00.958338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mydivs = soup.find(\"div\", {\"class\": \"vertical-menu\"}).label.find(\"div\", {\"class\": \"show1\"}).find_all(\"a\", {\"class\": \"brnd\"})\nbrand_links = [x.get(\"href\") for x in mydivs] #This list will store links of all car brands given in the website\n\nprint(f'The webiste \"https://www.ccarprice.com/au/\" has data of {len(brand_links)} car brands.')\nprint('\\nAll Brands Links:')\nfor n, each_brand_link in enumerate(brand_links):\n    print(f'{n+1}. {each_brand_link}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-10T06:31:00.961014Z","iopub.execute_input":"2023-03-10T06:31:00.961542Z","iopub.status.idle":"2023-03-10T06:31:00.972083Z","shell.execute_reply.started":"2023-03-10T06:31:00.961508Z","shell.execute_reply":"2023-03-10T06:31:00.970779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now I'll create a dictionary with key as the car's brand and its value as a list of all car links of the corresponding car brand.**","metadata":{}},{"cell_type":"code","source":"all_brands_car_links = {}","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:32:57.65354Z","iopub.execute_input":"2023-03-10T06:32:57.653974Z","iopub.status.idle":"2023-03-10T06:32:57.661365Z","shell.execute_reply.started":"2023-03-10T06:32:57.653941Z","shell.execute_reply":"2023-03-10T06:32:57.659232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n, each_brand_link in enumerate(brand_links):\n\n    # Using the regular expression to extract the text\n    brand_name = re.search(r'https:\\/\\/www\\.ccarprice\\.com\\/au\\/(.+)\\-car', str(each_brand_link)).group(1)\n\n    # Matched string might contain special character and first letter of each word needs to be capitalize\n    if '-' in brand_name:\n        brand_name = brand_name.replace('-', ' ').title()\n    else:\n        brand_name = brand_name.title()\n\n    html1 = requests.get(str(each_brand_link))\n    soup1 = BeautifulSoup(html1.content, 'html.parser')\n\n    all_cars_of_this_brand = soup1.body.find(\"div\", {\"id\": \"page\"}).find_all(\"div\", {\"id\": \"pbox\", \"class\": \"price-cover\"})[-1].find_all(\"div\", {\"id\": \"pbox\", \"class\": \"listing\"})\n\n    all_cars_links_of_this_brand = [] #This list will store the links of all car correponding to a current car brand\n\n    for i in all_cars_of_this_brand: #Extracting just the links of each needed cars\n            text = i.getText().strip()\n            if text.split(\"\\n\")[-1] != \"Coming soon\":\n                all_cars_links_of_this_brand.append(i.a.get(\"href\"))\n    \n    all_brands_car_links[brand_name] = all_cars_links_of_this_brand\n    \n#     break","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:32:57.726306Z","iopub.execute_input":"2023-03-10T06:32:57.726845Z","iopub.status.idle":"2023-03-10T06:33:06.592387Z","shell.execute_reply.started":"2023-03-10T06:32:57.726799Z","shell.execute_reply":"2023-03-10T06:33:06.590329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_cars_details = 0\n\nprint('The website \"https://www.ccarprice.com/au/\" has the following data:\\n')\n\nfor n, (each_brand, list_of_cars) in enumerate(all_brands_car_links.items()):\n    print(f'{n+1}. {each_brand} has {len(list_of_cars)}')\n    total_cars_details += len(list_of_cars)\n\nprint(f'\\nThe website has a total of {total_cars_details} car details.')","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:33:06.594837Z","iopub.execute_input":"2023-03-10T06:33:06.595268Z","iopub.status.idle":"2023-03-10T06:33:06.603146Z","shell.execute_reply.started":"2023-03-10T06:33:06.595218Z","shell.execute_reply":"2023-03-10T06:33:06.601735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_car_details = pd.DataFrame() # Creatng empty df","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-10T06:45:01.891482Z","iopub.execute_input":"2023-03-10T06:45:01.89294Z","iopub.status.idle":"2023-03-10T06:45:01.902142Z","shell.execute_reply.started":"2023-03-10T06:45:01.892875Z","shell.execute_reply":"2023-03-10T06:45:01.900249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_number = 0\nfor n, (each_brand, list_of_cars_links) in enumerate(all_brands_car_links.items()):\n    \n    for m, link_of_each_car in enumerate(list_of_cars_links):\n        car_details_grouped_by_brand = {}\n        \n        try:\n            html2 = requests.get(link_of_each_car, timeout=10) #setting a timeout for server response\n        except:\n            continue #continuing to next webpage if timeout exceeds\n        else: #If timeout doesn't exceed, extracting data\n            soup2 = BeautifulSoup(html2.content, 'html.parser')\n\n            price_details = soup2.select_one('html body div#page.main div div#pbox.detail-cover div#pbox.image-detail div#pbox.detail-price')\n\n            try:\n                price_in_aud = re.search(r'Price in AUD\\:\\n?\\s*(.+)\\n?\\<br\\/\\>\\s*Price in USD\\:\\s*\\$?(.+)\\n?', str(price_details)).group(1).replace(',', '')\n            except:\n                price_in_aud = np.nan\n                \n            try:\n                price_in_usd = re.search(r'Price in AUD\\:\\n?\\s*(.+)\\n?\\<br\\/\\>\\s*Price in USD\\:\\s*\\$?(.+)\\n?', str(price_details)).group(2).replace(',', '')\n            except:\n                price_in_usd = np.nan\n\n            if price_in_aud == 'N/A' and price_in_usd == 'N/A':\n                price_in_aud = np.nan\n                price_in_usd = np.nan\n            \n\n            year_UNFILTERED = soup2.select_one('html body div#page.main div div#spec div#pbox.detail-cover div.tr div.td2').text.strip()\n            try:\n                year = re.search('.*(\\d{4}).*', year_UNFILTERED).group(1)\n            except:\n                year = np.nan\n\n            ##\n            car_details_grouped_by_brand['brand'] = each_brand\n            try:\n                car_details_grouped_by_brand['year'] = year\n            except:\n                car_details_grouped_by_brand['year'] = np.nan\n            car_details_grouped_by_brand['price_in_aud'] = price_in_aud\n            car_details_grouped_by_brand['price_in_usd'] = price_in_usd\n            \n            data_not_needed = ['Available Colors', 'Warranty']\n\n            list_of_car_features = soup2.select_one('html body div#page.main div div#spec div#pbox.detail-cover').find_all(\"div\", {\"class\": \"tr\"})\n            for each in list_of_car_features:\n                if each.find_all(\"div\")[0].text.strip() not in data_not_needed:\n                    feature_name= each.find_all(\"div\")[0].text.strip().lower().replace(\" \", '_')\n                    feature_data= each.find_all(\"div\")[1].text.strip()\n                    \n                    ##\n                    car_details_grouped_by_brand[feature_name] = feature_data\n\n            if n == 0 and m == 0:\n                inner_df = pd.DataFrame(car_details_grouped_by_brand, index=[index_number])\n                index_number += 1\n                all_car_details = inner_df.copy()\n            else:\n                inner_df = pd.DataFrame(car_details_grouped_by_brand, index=[index_number])\n                index_number += 1\n                all_car_details = pd.concat([all_car_details, inner_df], join='outer').copy()\n            \n            time.sleep(0.1)\n            \n            clear_output(wait=True)\n            print(f\"Number of scraped car data: {index_number+1}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:45:02.369773Z","iopub.execute_input":"2023-03-10T06:45:02.370421Z","iopub.status.idle":"2023-03-10T07:04:33.650737Z","shell.execute_reply.started":"2023-03-10T06:45:02.37038Z","shell.execute_reply":"2023-03-10T07:04:33.649281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_car_details","metadata":{"execution":{"iopub.status.busy":"2023-03-10T07:04:33.652985Z","iopub.execute_input":"2023-03-10T07:04:33.653339Z","iopub.status.idle":"2023-03-10T07:04:33.690816Z","shell.execute_reply.started":"2023-03-10T07:04:33.653299Z","shell.execute_reply":"2023-03-10T07:04:33.689567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the initial dataframe\nall_car_details.to_csv('CarPricesRaw.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T07:11:11.618927Z","iopub.execute_input":"2023-03-10T07:11:11.619314Z","iopub.status.idle":"2023-03-10T07:11:11.711919Z","shell.execute_reply.started":"2023-03-10T07:11:11.619281Z","shell.execute_reply":"2023-03-10T07:11:11.709552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction","metadata":{}},{"cell_type":"markdown","source":"**The initial dataset I used in this notebook is from <a href='https://opendatanepal.com/dataset/preliminary-data-of-national-population-and-housing-census-2021'>'opendatanepal.com'</a>. It contains Preliminary Data of National Population and Housing Census 2021. Scroll below to see more about the initial dataset.**\n\n**Using web scrapping, I added some columns to the initial dataset for analyzing purpose.**","metadata":{}},{"cell_type":"markdown","source":"# 2. Initial dataset","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport requests\n\nimport urllib.request\n\nimport re\nfrom bs4 import BeautifulSoup\n\nfrom requests.exceptions import Timeout\nfrom IPython.display import clear_output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-21T13:56:26.348756Z","iopub.execute_input":"2023-03-21T13:56:26.349213Z","iopub.status.idle":"2023-03-21T13:56:26.562618Z","shell.execute_reply.started":"2023-03-21T13:56:26.349148Z","shell.execute_reply":"2023-03-21T13:56:26.561386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downloading the initial dataset\n\nimport urllib.request\n\nlink_to_initial_dataset = 'https://opendatanepal.com/dataset/6368a9aa-4649-46e6-925a-ebbff0c49fc1/resource/b11d363a-91c3-4332-ba14-ee50b3a12ec4/download/preliminary-data-of-national-population-and-housing-census-2021-english.csv'\n\nfilename = \"preliminary-data-of-national-population-and-housing-census-2021-english.csv\"\n\nurllib.request.urlretrieve(link_to_initial_dataset, filename)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:26.567254Z","iopub.execute_input":"2023-03-21T13:56:26.567801Z","iopub.status.idle":"2023-03-21T13:56:27.95677Z","shell.execute_reply.started":"2023-03-21T13:56:26.567757Z","shell.execute_reply":"2023-03-21T13:56:27.95579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"census_2021_df = pd.read_csv('/kaggle/working/preliminary-data-of-national-population-and-housing-census-2021-english.csv')\ncensus_2021_df = census_2021_df.sort_values(by=['District', 'Local Level Name'], ascending=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:17:15.239639Z","iopub.execute_input":"2023-03-21T14:17:15.240017Z","iopub.status.idle":"2023-03-21T14:17:15.25074Z","shell.execute_reply.started":"2023-03-21T14:17:15.239986Z","shell.execute_reply":"2023-03-21T14:17:15.249738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reset the index\ncensus_2021_df = census_2021_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:17:16.267292Z","iopub.execute_input":"2023-03-21T14:17:16.267962Z","iopub.status.idle":"2023-03-21T14:17:16.273311Z","shell.execute_reply.started":"2023-03-21T14:17:16.267909Z","shell.execute_reply":"2023-03-21T14:17:16.272448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"census_2021_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:17:17.016502Z","iopub.execute_input":"2023-03-21T14:17:17.017323Z","iopub.status.idle":"2023-03-21T14:17:17.032027Z","shell.execute_reply.started":"2023-03-21T14:17:17.017278Z","shell.execute_reply":"2023-03-21T14:17:17.030631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I'll add a new column in 3rd column position to dictate what the type of local government body each district: \"Nagarpalika or Gaupalika\"**","metadata":{}},{"cell_type":"code","source":"local_government_type = []\nfor u in census_2021_df['Local Level Name']:\n    if 'Rural' in u or 'rural' in u:\n        local_government_type.append('Nagarpalika (Municipality)')\n    else:\n        local_government_type.append('Gaupalika (Rural Municipality)')\n\n# inserting the new column into the 3rd column position\ncensus_2021_df.insert(2, 'local_government_type', local_government_type)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:17:25.171066Z","iopub.execute_input":"2023-03-21T14:17:25.17227Z","iopub.status.idle":"2023-03-21T14:17:25.179948Z","shell.execute_reply.started":"2023-03-21T14:17:25.172224Z","shell.execute_reply":"2023-03-21T14:17:25.178684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Renaming column names\ncensus_2021_df = census_2021_df.rename(columns={\n    'District':'district',\n    'Local Level Name': 'local_government_name',\n    'Total family number': 'total_family_number',\n    'Total household number': 'total_household_number',\n    'Total population': 'total_population_number',\n    'Total Male': 'total_male_number',\n    'Total Female': 'total_female_number'\n})","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:17:25.896516Z","iopub.execute_input":"2023-03-21T14:17:25.896968Z","iopub.status.idle":"2023-03-21T14:17:25.90382Z","shell.execute_reply.started":"2023-03-21T14:17:25.89693Z","shell.execute_reply":"2023-03-21T14:17:25.902704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"census_2021_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:17:26.994181Z","iopub.execute_input":"2023-03-21T14:17:26.994616Z","iopub.status.idle":"2023-03-21T14:17:27.012866Z","shell.execute_reply.started":"2023-03-21T14:17:26.994578Z","shell.execute_reply":"2023-03-21T14:17:27.011729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"census_2021_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:28.074894Z","iopub.execute_input":"2023-03-21T13:56:28.075475Z","iopub.status.idle":"2023-03-21T13:56:28.19295Z","shell.execute_reply.started":"2023-03-21T13:56:28.075428Z","shell.execute_reply":"2023-03-21T13:56:28.192136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The data looks good for analyzing as it is but it can benefit from including several other data like district_latitude, district_longitude.**","metadata":{}},{"cell_type":"markdown","source":"# 3. Scrapping District's Provinces & Coordinates","metadata":{}},{"cell_type":"markdown","source":"**Despite wikipedia being considered as not a reliable source, I'm pretty sure that the coordinates, Zones & Provinces data are accurate. So for web scrapping each district's cooordinates, I'll use wikipedia.**","metadata":{}},{"cell_type":"code","source":"# Creating a dataframe with a single colum 'district' to which I'll add latitudes and longitudes columns\n\ndistrict_names = list(census_2021_df['district'].unique())\ndistrict_names.sort()\n\ndistrict_coordinates = pd.DataFrame(district_names, columns=['district'])","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:28.196483Z","iopub.execute_input":"2023-03-21T13:56:28.197045Z","iopub.status.idle":"2023-03-21T13:56:28.202383Z","shell.execute_reply.started":"2023-03-21T13:56:28.196989Z","shell.execute_reply":"2023-03-21T13:56:28.201454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latitudes = []\nlongitudes = []\nprovinces = []\n\nscraped_districts_counter = 0\ncannot_scrape_districts_counter = 0\n\nprovince_successful = 0\nprovince_unsuccessful = 0\n\nfor n, district in enumerate(district_coordinates['district']):\n    if district == 'Bhojpur':\n        link = 'https://en.wikipedia.org/wiki/Bhojpur_District,_Nepal'\n    else:\n        link = f\"https://en.wikipedia.org/wiki/{district.title()}_District\" #This is the website this script uses to scrape needed information\n    \n    try:\n        html = requests.get(link) \n    except:\n        print(f'Cannot establish connection to \"{link}\"')\n        latitudes = []\n        longitudes = []\n        break\n        \n    #Turning response object fom \"html\" variable into beautifulsoup object to crawl through the site\n    soup = BeautifulSoup(html.content, 'html.parser')\n\n    try:\n        coordinates = soup.body.find(\"span\", {\"id\": \"coordinates\"}).span.span.find(\"span\", {\"class\": \"geo-dms\"})\n        \n        latitude = coordinates.find(\"span\", {\"class\": \"latitude\"})\n        latitude = latitude.text.replace('N', '').replace('n', '').replace('″', '')\n        d, intermediate = latitude.split('°')\n        m , s = intermediate.split('′')\n        if s == '':\n            s = 0\n        latitude = float(d) + (float(m)/60) + (float(s)/3600) #\n\n        longitude = coordinates.find(\"span\", {\"class\": \"longitude\"})\n        longitude = longitude.text.replace('E', '').replace('e', '').replace('″', '')\n        d, intermediate = longitude.split('°')\n        m , s = intermediate.split('′')\n        if s == '':\n            s = 0\n        longitude = float(d) + (float(m)/60) + (float(s)/3600)\n        \n        scraped_districts_counter += 1\n    except:\n        latitude = np.nan\n        longitude = np.nan\n        cannot_scrape_districts_counter += 1\n\n    # coordinates_text = soup.select_one('/html/body/div[1]/div/div[3]/main/div[2]/div[3]/div[1]/p[26]/span/span/span/span/a/span[3]/span[1]')\n    \n    latitudes.append(latitude)\n    longitudes.append(longitude)\n    \n    \n    try:\n        inner_province = soup.body.find(\"table\", {\"class\": ['infobox', 'ib-settlement', 'vcard']}).tbody.find(\"tr\", {\"class\": 'mergedrow'}).find(\"td\", {\"class\": 'infobox-data'}).a.text.strip().title()\n        province_successful += 1\n    except:\n        inner_province = np.nan\n        province_unsuccessful += 1\n    provinces.append(inner_province)\n    \n    clear_output(wait=True)\n    print(f\"Current link: {link}\")\n    print(f\"Number of scrapped district's data:\")\n    print(f\"\\tCoordinates Successful = {scraped_districts_counter}\\n\")\n    print(f\"\\tCoordinates Failed = {cannot_scrape_districts_counter}\")\n    print(f\"\\tProvinces Successful (Including Repitations) = {province_successful}\")\n    print(f\"\\tProvinces Failed = {province_unsuccessful}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:28.203848Z","iopub.execute_input":"2023-03-21T13:56:28.204146Z","iopub.status.idle":"2023-03-21T13:56:57.585479Z","shell.execute_reply.started":"2023-03-21T13:56:28.204117Z","shell.execute_reply":"2023-03-21T13:56:57.584333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**14 district's website in wikipedia did not have its corresponding coordinates. Let's combine the recently created dataframe with 1 column with latitude and longitudes for corresponding districts.**","metadata":{}},{"cell_type":"code","source":"# Adding the columns\ndistrict_coordinates['latitude'] = latitudes\ndistrict_coordinates['longitude'] = longitudes\ndistrict_coordinates['province'] = provinces","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.587185Z","iopub.execute_input":"2023-03-21T13:56:57.587897Z","iopub.status.idle":"2023-03-21T13:56:57.595975Z","shell.execute_reply.started":"2023-03-21T13:56:57.587849Z","shell.execute_reply":"2023-03-21T13:56:57.595043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_coordinates","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.597694Z","iopub.execute_input":"2023-03-21T13:56:57.598043Z","iopub.status.idle":"2023-03-21T13:56:57.620426Z","shell.execute_reply.started":"2023-03-21T13:56:57.598011Z","shell.execute_reply":"2023-03-21T13:56:57.618823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.1. Data wrangling - Provinces","metadata":{}},{"cell_type":"code","source":"district_coordinates['province'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.621827Z","iopub.execute_input":"2023-03-21T13:56:57.622792Z","iopub.status.idle":"2023-03-21T13:56:57.632982Z","shell.execute_reply.started":"2023-03-21T13:56:57.622758Z","shell.execute_reply":"2023-03-21T13:56:57.632056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There shouldn't be 'Coordination Committee'. So, find out the district of it and giving it it's proper province name.**","metadata":{}},{"cell_type":"code","source":"district_coordinates[district_coordinates['province'] == 'Coordination Committee']","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.634409Z","iopub.execute_input":"2023-03-21T13:56:57.634699Z","iopub.status.idle":"2023-03-21T13:56:57.64878Z","shell.execute_reply.started":"2023-03-21T13:56:57.634671Z","shell.execute_reply":"2023-03-21T13:56:57.647749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Jajarkot is in 'Karnali Province'. So, fixing it.**","metadata":{}},{"cell_type":"code","source":"district_coordinates.loc[26, 'province'] = 'Karnali Province'\ndistrict_coordinates.loc[26, 'province']","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.649806Z","iopub.execute_input":"2023-03-21T13:56:57.650098Z","iopub.status.idle":"2023-03-21T13:56:57.661018Z","shell.execute_reply.started":"2023-03-21T13:56:57.65007Z","shell.execute_reply":"2023-03-21T13:56:57.659893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_coordinates['province'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.662492Z","iopub.execute_input":"2023-03-21T13:56:57.663293Z","iopub.status.idle":"2023-03-21T13:56:57.674529Z","shell.execute_reply.started":"2023-03-21T13:56:57.663247Z","shell.execute_reply":"2023-03-21T13:56:57.673465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All \"Province No. 1\" and \"Koshi Pradesh\" are actually \"Koshi Province\". So, fixing it.**","metadata":{}},{"cell_type":"code","source":"koshi_filtered = district_coordinates[(district_coordinates['province'] == 'Province No. 1') | (district_coordinates['province'] == 'Koshi Pradesh')].index\n\nfor each in koshi_filtered:\n    district_coordinates.loc[each, 'province'] = 'Koshi Province'","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.675712Z","iopub.execute_input":"2023-03-21T13:56:57.676016Z","iopub.status.idle":"2023-03-21T13:56:57.685738Z","shell.execute_reply.started":"2023-03-21T13:56:57.675986Z","shell.execute_reply":"2023-03-21T13:56:57.684823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_coordinates['province'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.687132Z","iopub.execute_input":"2023-03-21T13:56:57.687832Z","iopub.status.idle":"2023-03-21T13:56:57.700679Z","shell.execute_reply.started":"2023-03-21T13:56:57.687789Z","shell.execute_reply":"2023-03-21T13:56:57.699715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All \"Province No. 2\" are actually \"Madhesh Province\". So, fixing it.**","metadata":{}},{"cell_type":"code","source":"madesh_filtered = district_coordinates[(district_coordinates['province'] == 'Province No. 2')].index\n\nfor each in madesh_filtered:\n    district_coordinates.loc[each, 'province'] = 'Madhesh Province'","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.70208Z","iopub.execute_input":"2023-03-21T13:56:57.702395Z","iopub.status.idle":"2023-03-21T13:56:57.711343Z","shell.execute_reply.started":"2023-03-21T13:56:57.702365Z","shell.execute_reply":"2023-03-21T13:56:57.710572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_coordinates['province'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.712337Z","iopub.execute_input":"2023-03-21T13:56:57.713345Z","iopub.status.idle":"2023-03-21T13:56:57.725075Z","shell.execute_reply.started":"2023-03-21T13:56:57.713305Z","shell.execute_reply":"2023-03-21T13:56:57.724053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{district_coordinates['province'].value_counts().sum()} districts have provinces declared and {district_coordinates['province'].isnull().sum()} don't!\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.72605Z","iopub.execute_input":"2023-03-21T13:56:57.72662Z","iopub.status.idle":"2023-03-21T13:56:57.736766Z","shell.execute_reply.started":"2023-03-21T13:56:57.726588Z","shell.execute_reply":"2023-03-21T13:56:57.735724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_fill_province = district_coordinates[district_coordinates['province'].isnull()]\nto_fill_province","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.737966Z","iopub.execute_input":"2023-03-21T13:56:57.738627Z","iopub.status.idle":"2023-03-21T13:56:57.753938Z","shell.execute_reply.started":"2023-03-21T13:56:57.738588Z","shell.execute_reply":"2023-03-21T13:56:57.753181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'll manually find the provinces and fill them\nto_fill_province_index = to_fill_province.index\ncorresponding_provinces_in_order = ['Koshi Province', \n                                    'Bagmati Province', \n                                    'Lumbini Province', \n                                    'Koshi Province', \n                                    'Koshi Province', \n                                    'Sudurpashchim Province', \n                                    'Koshi Province', \n                                    'Bagmati Province', \n                                    'Koshi Province', \n                                    'Gandaki Province', \n                                    'Gandaki Province', \n                                    'Lumbini Province', \n                                    'Koshi Province',\n                                    'Lumbini Province', \n                                    'Karnali Province', \n                                    'Karnali Province', \n                                    'Koshi Province', \n                                    'Gandaki Province', \n                                    'Koshi Province']\n\nfor province_index, actual_val in zip(to_fill_province_index, corresponding_provinces_in_order):\n    district_coordinates.loc[province_index, 'province'] = actual_val","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.755072Z","iopub.execute_input":"2023-03-21T13:56:57.755404Z","iopub.status.idle":"2023-03-21T13:56:57.767077Z","shell.execute_reply.started":"2023-03-21T13:56:57.755373Z","shell.execute_reply":"2023-03-21T13:56:57.765988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_coordinates['province'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.768256Z","iopub.execute_input":"2023-03-21T13:56:57.769478Z","iopub.status.idle":"2023-03-21T13:56:57.781973Z","shell.execute_reply.started":"2023-03-21T13:56:57.769421Z","shell.execute_reply":"2023-03-21T13:56:57.78088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2. Data wrangling - Coordinates","metadata":{}},{"cell_type":"code","source":"# Displaying rows with null values for latitude or longitude\nto_fill_coordinates = district_coordinates[ (district_coordinates['latitude'].isnull()) | (district_coordinates['longitude'].isnull())]\nto_fill_coordinates","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.78344Z","iopub.execute_input":"2023-03-21T13:56:57.783849Z","iopub.status.idle":"2023-03-21T13:56:57.802664Z","shell.execute_reply.started":"2023-03-21T13:56:57.783815Z","shell.execute_reply":"2023-03-21T13:56:57.801423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So, we need to find coordinates of these 14 districts manually. I'll find the latitudes and longitudes of corresponding districts from google map.**","metadata":{}},{"cell_type":"code","source":"to_fill_coordinates_index = to_fill_coordinates.index\n\nto_fill_corresponding_latitudes_in_order = [27.108381,27.678777, 27.586242, 27.971370, 26.839804, 27.662786, 27.819579, 27.525530, 28.682423, 28.745739, 28.433012, 26.580362, 26.865413]\nto_fill_corresponding_longitudes_in_order = [85.069835, 85.431211, 84.478300, 82.422960, 86.027870, 85.321435, 85.627741, 83.706729,82.795816, 82.429728, 82.153392, 86.719284, 86.674491]\n\nfor coordinate_index, lat, lon in zip(to_fill_coordinates_index, to_fill_corresponding_latitudes_in_order, to_fill_corresponding_longitudes_in_order):\n    district_coordinates.loc[coordinate_index, 'latitude'] = lat\n    district_coordinates.loc[coordinate_index, 'longitude'] = lon","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.803951Z","iopub.execute_input":"2023-03-21T13:56:57.804298Z","iopub.status.idle":"2023-03-21T13:56:57.818776Z","shell.execute_reply.started":"2023-03-21T13:56:57.804265Z","shell.execute_reply":"2023-03-21T13:56:57.817619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_coordinates.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.824376Z","iopub.execute_input":"2023-03-21T13:56:57.825472Z","iopub.status.idle":"2023-03-21T13:56:57.834774Z","shell.execute_reply.started":"2023-03-21T13:56:57.825422Z","shell.execute_reply":"2023-03-21T13:56:57.833581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_coordinates","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:57.836284Z","iopub.execute_input":"2023-03-21T13:56:57.837628Z","iopub.status.idle":"2023-03-21T13:56:57.857595Z","shell.execute_reply.started":"2023-03-21T13:56:57.837474Z","shell.execute_reply":"2023-03-21T13:56:57.856448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scrapping District's Zone & Area","metadata":{}},{"cell_type":"markdown","source":"**I'll be web scrapping data of district's zone and area from <a href='https://kpadhne.com/77-districts-of-nepal/'>\"kpadhne.com\"</a>.**","metadata":{}},{"cell_type":"code","source":"link = 'https://kpadhne.com/77-districts-of-nepal/'\n\ntry:\n    html = requests.get(link) \nexcept:\n    print(f'Cannot establish connection to \"{link}\"')\n\n#Turning response object fom \"html\" variable into beautifulsoup object to crawl through the site\nsoup1 = BeautifulSoup(html.content, 'html.parser')\n\ndistrict_with_headquarters = soup1.body.find(\"div\", {\"class\": \"entry-content\"}).find_all('table')[10].find_all('tr')\ndistrict_and_headquarters = {}\nfor n, district in enumerate(district_with_headquarters):\n    if n != 0:\n        d_name = district.find_all('td')[0].text.strip().title()\n        if d_name == 'Nawalpur':\n            d_name = 'Nawalparasi East'\n        elif d_name == 'Parasi':\n            d_name = 'Nawalparasi West'\n        elif d_name == 'Eastern Rukum':\n            d_name = 'Rukum East'\n        elif d_name == 'Western Rukum':\n            d_name = 'Rukum West'\n        elif d_name == 'Chitwan':\n            d_name = 'Chitawan'\n        elif d_name == 'Kapilvastu':\n            d_name = 'Kapilbastu'\n        elif d_name == 'Tanahun':\n            d_name = 'Tanahu'\n            \n        d_headquarter = district.find_all('td')[1].text.strip().title()\n        \n        district_and_headquarters[d_name] = d_headquarter\n        \ndistrict_and_headquarters = {k: district_and_headquarters[k] for k in sorted(district_and_headquarters.keys())}\n\n\n\n\n\ndistrict_with_area = soup1.body.find(\"div\", {\"class\": \"entry-content\"}).find_all('table')[11].find_all('tr')\ndistrict_and_area = {}\nfor n, district in enumerate(district_with_area):\n    if n != 0:\n        d_name = district.find_all('td')[0].text.strip().title().split(' ')\n        if len(d_name) == 2:\n            d_name = d_name[0]\n        elif len(d_name) == 3:\n            d_name = f'{d_name[0]} {d_name[1]}'\n            \n        if d_name == 'Nawalpur':\n            d_name = 'Nawalparasi East'\n        elif d_name == 'Parasi':\n            d_name = 'Nawalparasi West'\n        elif d_name == 'Eastern Rukum':\n            d_name = 'Rukum East'\n        elif d_name == 'Western Rukum':\n            d_name = 'Rukum West'\n        elif d_name == 'Chitwan':\n            d_name = 'Chitawan'\n        elif d_name == 'Kapilvastu':\n            d_name = 'Kapilbastu'\n        elif d_name == 'Tanahun':\n            d_name = 'Tanahu'\n            \n        d_area = district.find_all('td')[1].text.strip().title()\n        \n        district_and_area[d_name] = d_area\ndistrict_and_area = {k: district_and_area[k] for k in sorted(district_and_area.keys())}\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-21T13:56:57.860933Z","iopub.execute_input":"2023-03-21T13:56:57.861303Z","iopub.status.idle":"2023-03-21T13:56:58.487202Z","shell.execute_reply.started":"2023-03-21T13:56:57.86127Z","shell.execute_reply":"2023-03-21T13:56:58.486134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating temporary dataframe out of scrapped district names, headquarters and areas\ntemp_df = pd.DataFrame({\n                        'district': list(district_and_headquarters.keys()),\n                        'district_headquarters': list(district_and_headquarters.values()),\n                        'area_km_squared': list(district_and_area.values())\n                       })                                    \n# Performing left join on df1 and df2\ndistrict_df = pd.merge(district_coordinates, temp_df, on='district', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.488502Z","iopub.execute_input":"2023-03-21T13:56:58.488809Z","iopub.status.idle":"2023-03-21T13:56:58.504706Z","shell.execute_reply.started":"2023-03-21T13:56:58.488779Z","shell.execute_reply":"2023-03-21T13:56:58.503425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.506364Z","iopub.execute_input":"2023-03-21T13:56:58.506691Z","iopub.status.idle":"2023-03-21T13:56:58.526326Z","shell.execute_reply.started":"2023-03-21T13:56:58.50666Z","shell.execute_reply":"2023-03-21T13:56:58.525228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scrapping Region Information","metadata":{}},{"cell_type":"markdown","source":"**I'll be web scrapping data of district's zone and area from <a href='http://www.statoids.com/ynp.html'>\"statoids.com\"</a>.**","metadata":{}},{"cell_type":"code","source":"link = 'http://www.statoids.com/ynp.html'\n\ntry:\n    html = requests.get(link) \nexcept:\n    print(f'Cannot establish connection to \"{link}\"')\n\n#Turning response object fom \"html\" variable into beautifulsoup object to crawl through the site\nsoup2 = BeautifulSoup(html.content, 'html.parser')\n\n# district_with_headquarters = soup2.body.find(\"div\", {\"class\": \"entry-content\"}).find_all('table')[10].find_all('tr')\ndistrict_statoids = soup2.body.find(\"table\", {\"class\": \"st\"}).find_all('tr')\n\ndta = []\nfor n, district in enumerate(district_statoids):\n    if n == 0:\n        continue\n    elif n == 76:\n        break\n    each_data = district.find_all('td')\n    each_data = [i.text for i in each_data]\n    dta.append(each_data)\n\n# creating a dataframe from the list\nstatoids_df = pd.DataFrame(dta)\n# renaming the columns\nstatoids_df.columns = ['district', 'HASC', 'Reg', 'population_2011', 'population_2001', 'population_1991', 'population_1981', 'area_km_squared', 'capital', 'region', 'zone']\nstatoids_df_filtered = statoids_df[['district', 'area_km_squared', 'region', 'zone']]\nstatoids_df_filtered['district'] = statoids_df_filtered['district'].apply(lambda x: x.strip())\n\nstatoids_df_filtered['region'] = statoids_df_filtered['region'].apply(lambda x: \"Hill\" if x == \"H\" else (\"Mountain\" if x == \"M\" else (\"Terai\" if x == \"T\" else np.nan))\n)\nstatoids_df_filtered['area_km_squared'] = statoids_df_filtered['area_km_squared'].apply(lambda x: x.replace(',', ''))\n\n# sort the dataframe in alphabetical order\nstatoids_df_filtered = statoids_df_filtered.sort_values(by='district')\n\n\n# removing 'Nawalparasi' and'Rukum' district as these districts are divided into 2 districts each (east and west)\nto_remove_index = statoids_df_filtered[statoids_df_filtered['district'].isin(['Nawalparasi','Rukum'])].index\nstatoids_df_filtered = statoids_df_filtered.drop(to_remove_index)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.528815Z","iopub.execute_input":"2023-03-21T13:56:58.529893Z","iopub.status.idle":"2023-03-21T13:56:58.697518Z","shell.execute_reply.started":"2023-03-21T13:56:58.529836Z","shell.execute_reply":"2023-03-21T13:56:58.696316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# renaming some district names\nto_rename = statoids_df_filtered[statoids_df_filtered['district'].isin(['Chitwan','Dang Deokhuri', 'Kapilvastu'])].index\n\nfor n, each in enumerate(to_rename):\n    if n == 0:\n        statoids_df_filtered.loc[each, 'district'] = 'Chitawan'\n    elif n == 1:\n        statoids_df_filtered.loc[each, 'district'] = 'Dang'\n    elif n == 2:\n        statoids_df_filtered.loc[each, 'district'] = 'Kapilbastu'\n        \nstatoids_df_filtered","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.698832Z","iopub.execute_input":"2023-03-21T13:56:58.699196Z","iopub.status.idle":"2023-03-21T13:56:58.716489Z","shell.execute_reply.started":"2023-03-21T13:56:58.699139Z","shell.execute_reply":"2023-03-21T13:56:58.715133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now I need to add rows for following districts:**\n\n**1. 'Nawalparasi East'**\n\n**2. 'Western Rukum'**\n\n**3. 'Rukum East'**\n\n**4. 'Rukum West'**","metadata":{}},{"cell_type":"code","source":"rows = [\n    ['Nawalparasi East', '1126', 'Terai', 'Lumbini'],\n    ['Nawalparasi West', '1162', 'Terai', 'Lumbini'],\n    ['Rukum East', '1168', 'Hill', 'Rapti'],\n    ['Rukum West', '1152', 'Hill', 'Rapti']\n]\n# create a new DataFrame with the new rows\nnew_df = pd.DataFrame(rows, columns=['district', 'area_km_squared', 'region', 'zone'])\n\n# append the new DataFrame to the existing one\nstatoids_df_filtered = statoids_df_filtered.append(new_df, ignore_index=True)\n\nstatoids_df_final = statoids_df_filtered.sort_values(by=['district'], ascending=True)\n\nstatoids_df_final","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.717791Z","iopub.execute_input":"2023-03-21T13:56:58.718173Z","iopub.status.idle":"2023-03-21T13:56:58.745492Z","shell.execute_reply.started":"2023-03-21T13:56:58.718123Z","shell.execute_reply":"2023-03-21T13:56:58.744364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statoids_df_final.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.746837Z","iopub.execute_input":"2023-03-21T13:56:58.747198Z","iopub.status.idle":"2023-03-21T13:56:58.755051Z","shell.execute_reply.started":"2023-03-21T13:56:58.747141Z","shell.execute_reply":"2023-03-21T13:56:58.754251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now I'll left join \"district_df\" and \"statoids_df_final\"!**","metadata":{}},{"cell_type":"code","source":"# performing a left join on the 'key' column\ndistrict_data_df = pd.merge(district_df, statoids_df_final[['district', 'region', 'zone']], on='district', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.756378Z","iopub.execute_input":"2023-03-21T13:56:58.75669Z","iopub.status.idle":"2023-03-21T13:56:58.769405Z","shell.execute_reply.started":"2023-03-21T13:56:58.756658Z","shell.execute_reply":"2023-03-21T13:56:58.768445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reset the index\ndistrict_data_df = district_data_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.770669Z","iopub.execute_input":"2023-03-21T13:56:58.77098Z","iopub.status.idle":"2023-03-21T13:56:58.780674Z","shell.execute_reply.started":"2023-03-21T13:56:58.770952Z","shell.execute_reply":"2023-03-21T13:56:58.779547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert col1 to float\ndistrict_data_df['area_km_squared'] = district_data_df['area_km_squared'].astype(float)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:50:57.78738Z","iopub.execute_input":"2023-03-21T14:50:57.788604Z","iopub.status.idle":"2023-03-21T14:50:57.794928Z","shell.execute_reply.started":"2023-03-21T14:50:57.788557Z","shell.execute_reply":"2023-03-21T14:50:57.793312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"district_data_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.782177Z","iopub.execute_input":"2023-03-21T13:56:58.782823Z","iopub.status.idle":"2023-03-21T13:56:58.805768Z","shell.execute_reply.started":"2023-03-21T13:56:58.782786Z","shell.execute_reply":"2023-03-21T13:56:58.804952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"census_2021_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.807303Z","iopub.execute_input":"2023-03-21T13:56:58.807601Z","iopub.status.idle":"2023-03-21T13:56:58.823511Z","shell.execute_reply.started":"2023-03-21T13:56:58.807572Z","shell.execute_reply":"2023-03-21T13:56:58.822494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Database Schema & Design","metadata":{}},{"cell_type":"markdown","source":"**The database schema design was created using <a href='https://drawsql.app'>drawsql.app</a>. A SQLite database was created following schema design given below:**","metadata":{}},{"cell_type":"code","source":"from IPython import display\ndisplay.Image(\"/kaggle/input/nepal-database-schema/Nepal_DB_Schema.PNG\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:56:58.824864Z","iopub.execute_input":"2023-03-21T13:56:58.825224Z","iopub.status.idle":"2023-03-21T13:56:58.850547Z","shell.execute_reply.started":"2023-03-21T13:56:58.825184Z","shell.execute_reply":"2023-03-21T13:56:58.849532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Currently there is 2 dataframes (tables):**\n\n**1. 2021 Census Population Data**\n\n**2. District Data**","metadata":{}},{"cell_type":"code","source":"districtsData = {\n    'district_id': [n+1 for n, _ in enumerate(district_data_df['district'])],\n    'district': district_data_df['district']\n}\ndistricts = pd.DataFrame(districtsData)\ndistricts","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:04:06.883089Z","iopub.execute_input":"2023-03-21T14:04:06.884517Z","iopub.status.idle":"2023-03-21T14:04:06.902583Z","shell.execute_reply.started":"2023-03-21T14:04:06.884464Z","shell.execute_reply":"2023-03-21T14:04:06.901244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the district column to add district_id column instead\ndistrict_data_df = district_data_df.drop(columns=['district'])\n\n# Insert the new district_id at index 0\ndistrict_data_df.insert(0, 'district_id', districts['district_id'])\n\ndistrict_data_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:07:13.87366Z","iopub.execute_input":"2023-03-21T14:07:13.874838Z","iopub.status.idle":"2023-03-21T14:07:13.896365Z","shell.execute_reply.started":"2023-03-21T14:07:13.874794Z","shell.execute_reply":"2023-03-21T14:07:13.895231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_dict = {}\nfor n, d in enumerate(list(districts['district'])):\n    temp_dict[d] = n+1\n    \ncensus_2021_df[census_2021_df['district']=='Achham'].shape[0]\ncensus_2021_df['district'] = census_2021_df['district'].apply(lambda x: temp_dict[x])\n\n# Rename column\ncensus_2021_df.rename(columns={'district': 'district_id'}, inplace=True)\n\ncensus_2021_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:20:02.167Z","iopub.execute_input":"2023-03-21T14:20:02.167405Z","iopub.status.idle":"2023-03-21T14:20:02.187778Z","shell.execute_reply.started":"2023-03-21T14:20:02.167372Z","shell.execute_reply":"2023-03-21T14:20:02.18655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I have transformed the dataframes according to schema. Now I'll create a sqlite database and store the three tables in it.**","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# Creating a connection to an SQLite database\nconn = sqlite3.connect('nepal.db')\n\n# Saving the DataFrames to the database\n\n# First saving districts dataframe as districts table\ndistricts.to_sql(\n    'districts', \n    conn, \n    if_exists='replace', \n    index=False, \n    dtype={\n        'district_id': 'tinyint NOT NULL',\n        'district': 'varchar(20)'\n    }\n)\n\n\n# First saving districts dataframe as districts table\ndistrict_data_df.to_sql(\n    'district_info', \n    conn, \n    if_exists='replace', \n    index=False, \n    dtype={\n        'district_id': 'tinyint NOT NULL',\n        'latitude': 'decimal',\n        'longitude': 'decimal',\n        'province': 'varchar(25) NOT NULL',\n        'district_headquarters': 'varchar(20)',\n        'area_km_squared': 'decimal',\n        'region': 'varchar(10) NOT NULL',\n        'zone': 'varchar(15) NOT NULL',\n    }\n)\n\n\n# First saving districts dataframe as districts table\ncensus_2021_df.to_sql(\n    'census_population_2021', \n    conn, \n    if_exists='replace', \n    index=False, \n    dtype={\n        'district_id': 'tinyint NOT NULL',\n        'local_government_name': 'varchar(50)',\n        'local_government_type': 'varchar(40)',\n        'total_family_number': 'int',\n        'total_household_number': 'int',\n        'total_population_number': 'int',\n        'total_male_number': 'int',\n        'total_female_number': 'int',\n    }\n)\n\n# Close the connection\nconn.close()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:38:19.127873Z","iopub.execute_input":"2023-03-21T14:38:19.128313Z","iopub.status.idle":"2023-03-21T14:38:19.221027Z","shell.execute_reply.started":"2023-03-21T14:38:19.128276Z","shell.execute_reply":"2023-03-21T14:38:19.220011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I have successfully create the database according to the schema! I think it would be best if I also save the dataframes as csv.**","metadata":{}},{"cell_type":"code","source":"# saving all 3 DataFrame to CSV file\n\ndistricts.to_csv('districts.csv', index=False)\ndistrict_data_df.to_csv('district_info.csv', index=False)\ncensus_2021_df.to_csv('census_population_2021.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:51:03.929603Z","iopub.execute_input":"2023-03-21T14:51:03.930009Z","iopub.status.idle":"2023-03-21T14:51:03.947885Z","shell.execute_reply.started":"2023-03-21T14:51:03.929975Z","shell.execute_reply":"2023-03-21T14:51:03.946537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
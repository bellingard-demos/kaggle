{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://www.comet.com/images/logo_comet_light.png\" width=\"300px\"/>\n\n# Interactive Confusion Matrix for Image Classification\n\n## Introduction\n\nObject Detection is one of the most popular applications of Machine Learning for Computer Vision. A detection model predicts both the class and location of each distinct object in an image. Object Detection models have a wide range of applications including manufacturing, surveillance, health care, and more. \n\n<a href=\"https://www.comet.com/anmorgan24/interactive-confusion-matrix/view/new/panels/?utm_source=Medium&utm_medium=referral&utm_content=TV_object_detection_blog\"><img src=\"https://s11.gifyu.com/images/Untitled-design-35f8c53e6f7cae4c6.png\" alt=\"Untitled-design-35f8c53e6f7cae4c6.png\" border=\"0\" with=\"850\"/></a>\n\nTorchVisionis a Python package that extends the PyTorch framework for computer vision use cases. In TorchVision’s detection module, developers can find [pre-trained object detection models](https://pytorch.org/vision/stable/models.html) that are ready to be finetuned on their own datasets. Pre-trained object detection models can also be found in the [PyTorch Hub](https://pytorch.org/hub/), [HuggingFace Hub](https://huggingface.co/docs/hub/index), and [other model zoos](https://docs.openvino.ai/latest/omz_demos.html), so computer vision engineers have a wide selection of models to choose from! But how can you systematically find the best model for a particular use-case? In this tutorial, we'll explore how to use an [experiment tracking](https://heartbeat.comet.ml/how-experiment-management-makes-it-easier-to-build-better-models-faster-ef99b6fee164) tool like [Comet](https://www.comet.com/signup/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog) to visually compare and evaluate object detection models.\n\nCheck out **[the public project here](https://colab.research.google.com/drive/1Fyrk6Br3EtahbFttIQh1cMgT1ztHKrzs#scrollTo=WXMuIndNl3-p)!**\nA full article is also coming soon!","metadata":{}},{"cell_type":"markdown","source":"## The Code","metadata":{}},{"cell_type":"code","source":"!pip install comet_ml --root-user-action=ignore --quiet\nimport comet_ml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To instantiate your Comet Experiment, you'll need to grab your API key from your [account settings](https://www.comet.com/account-settings/profile/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog). If you don't already have an account, [create one here for free](https://www.comet.com/signup/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog).","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nYOUR_COMET_API_KEY = user_secrets.get_secret(\"YOUR-COMET-API-KEY\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comet_ml.init(api_key= YOUR_COMET_API_KEY,\n              #project_name= \"interactive-confusion-matrix-kaggle\",\n              #workspace= \"anmorgan24\")\nexperiment = comet_ml.Experiment()\nexperiment.set_name(\"mask_rcnn\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's organize the folders and import a [modified version of the TorchVision module](https://github.com/anmorgan24/vision) that will generate the desired outputs of our experiment:","metadata":{}},{"cell_type":"code","source":"%%shell\nmv train/train/* train/; rm -rf train/train\nmv valid/valid/* valid/; rm -rf valid/valid\n\npip install tdqm cython torchmetrics --quiet\n\n# Download TorchVision repo with edits\ngit clone https://github.com/anmorgan24/vision.git\n\n# copy relevant files to working directory\ncd vision\ncp references/detection/utils.py ../\ncp references/detection/transforms.py ../\ncp references/detection/coco_eval.py ../\ncp references/detection/engine.py ../\ncp references/detection/coco_utils.py ../","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the necessary packages:","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom joblib import dump\nfrom PIL import Image, ImageDraw\nfrom skimage import measure\nimport json\nfrom random import randrange\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport pycocotools\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nimport torchvision\n\n#models\nfrom torchvision import models\nfrom torchvision.models.detection import faster_rcnn, RetinaNet, FCOS\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator, DefaultBoxGenerator\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the labels files as dictionaries:","metadata":{}},{"cell_type":"code","source":"with open('train_annotations') as json_file:\n    train_labels = json.load(json_file)\nwith open('valid_annotations') as json_file:\n    valid_labels = json.load(json_file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's display a random example image from the validaton set, with the ground truth bounding box to make sure the image files have successfully loaded. ","metadata":{}},{"cell_type":"code","source":"# show example image\n# generate random number within range of valid_labels length, convert to string and pad with zeros\nnumber = str(randrange(len(valid_labels)+1)).zfill(3)\n\nbbox_coords = valid_labels[int(number)]['bbox']\nim = Image.open('./valid/image_id_'+number+'.jpg')\nfig, ax = plt.subplots()\nax.imshow(im)\nrect = patches.Rectangle((bbox_coords[0], bbox_coords[1]), bbox_coords[2], bbox_coords[3], linewidth=2, edgecolor='deeppink', facecolor='none')\nax.add_patch(rect)\n\nplt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import some of the functions and modules we'll be using from TorchVision directly to our working directory:","metadata":{}},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we define our hyperparameters as a dictionary and log them to [Comet](https://www.comet.com/signup/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog). Note that the number of classes equals the number of objects to identify plus the background class. Because we have two label classes (\"penguin\" and \"turtle'), we specify three total classes.\n\nComet will keep track of the sets of hyperparameters used with each model, for each experiment, so that we can easily debug and reproduce our results. To learn more about tuning hyperparameters in Comet, check out [this great article](https://heartbeat.comet.ml/hyperparameter-tuning-in-comet-e7aa637f124c) from Dr. Angelica Lo Duca (she literally wrote the book on Comet!).","metadata":{}},{"cell_type":"code","source":"hyper_params = {\"lr\": 0.005,\n                \"momentum\" : 0.9,\n                \"weight_decay\" : 0.0005,\n                \"step_size\" : 3,\n                \"gamma\" : 0.1,\n                \"num_epochs\" : 3,\n                # num_classes = num of objects to identify + background class\n                \"num_classes\" : 3,\n                \"model_name\": \"mask_rcnn\",\n                \"feature_extract\": False}\n\nexperiment.log_parameters(hyper_params) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define a label dictionary to encode our [categorical labels](https://heartbeat.comet.ml/feature-engineering-for-categorical-data-897e98caea35). Remember that by default, the first class (class \"0\") is the background.","metadata":{}},{"cell_type":"code","source":"label_dict = {1: \"penguin\",\n              2: \"turtle\"}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_f1_score(precision, recall):\n    \"\"\"\n    Calculate f1 score from precision and recall values\n    \"\"\"\n    if precision + recall > 0:\n        f1_score = 2* ((precision * recall)/(precision + recall))\n\n    else:\n        f1_score = 0\n\n    return f1_score\n\ndef get_transform(train):\n\n    \"\"\"Returns composed Torch transforms\"\"\"\n\n    transforms = []\n    transforms.append(T.ToTensor())\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)\n\ndef my_log_func(logger):\n    \"\"\"\n    Parses out the data collected in COCO_evaluator's metric_logger and logs it to Comet.\n      Parameters:\n          logger (metric_logger): The COCO_evaluator metric_logger object to be parsed out.\n    \"\"\"\n    met_logger = str(logger).split('  ')\n    for met in met_logger:\n        temp_dict ={}\n        temp_list = met.split(': ')\n        temp_dict[temp_list[0]]=float(temp_list[1].split(' ')[0])\n        [experiment.log_metric(metr, temp_dict[metr]) for metr in temp_dict.keys()]\n\ndef make_coco_boxes(tensor_bboxes):\n\n    \"\"\"Convert torch tensor Pascal VOC bboxes to COCO format for Comet annotations\"\"\"\n\n    list_boxes=torch.Tensor.tolist(tensor_bboxes)\n    coco_boxes = [[list_boxes[0], list_boxes[1], (list_boxes[2]-list_boxes[0]), (list_boxes[3]-list_boxes[1])]]\n    return coco_boxes\n\ndef make_ped_points(binary_mask):\n\n    \"\"\"Converts binary mask labels to polygon point list\"\"\"\n\n    contours = measure.find_contours(binary_mask, 0.5)\n    ped_points=[]\n    for contour in contours:\n        contour = np.flip(contour, axis=1)\n        segmentation = contour.ravel().tolist()\n        ped_points.append(segmentation)\n    return ped_points\n\ndef make_annotations(prediction):\n\n    \"\"\" Parses out the COCO evaluator outputs into appropriately formated lists for Comet's annotations. \"\"\"\n\n    if len(prediction[0][1]['boxes']) == 0:\n        return None\n\n    annotations = [{\n      \"name\": \"image id: {}\".format(prediction[0][0]),\n      \"data\": []\n    }]\n\n    for i in range(len(prediction[0][1]['boxes'])):\n        annotations[0][\"data\"].append({\n          \"label\" : label_dict[torch.Tensor.tolist(prediction[0][2]['labels'])[i]], \n          \"score\": round((torch.Tensor.tolist(prediction[0][2]['scores'])[i]*100),2),\n          \"boxes\" : make_coco_boxes(prediction[0][2]['boxes'][i]),\n          #\"boxes\": [torch.Tensor.tolist(prediction[0][2]['boxes'])[i]], \n          # uncomment the following to log segmentation masks to Comet\n          # use only with models that return mask segmentation predictions, else will throw error\n          #\"points\": make_ped_points(prediction[0][2]['masks'][i].numpy().squeeze())\n          \"points\": None\n        })\n    return annotations\n\ndef add_PIL_bboxes(img, bbox):\n    img = img\n    img1 = ImageDraw.Draw(img)  \n    img1.rectangle(bbox, fill=None, outline =\"deeppink\", width=2)\n\nToPILImage = torchvision.transforms.ToPILImage()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll provide two options for models to use in our experiment: MaskRCNN and FastRCNN. Note that MaskRCNN is based on FastRCNN but with an extra component to detect and predict segmentation masks. We will not be using segmentation masks in this experiment, but feel free to use this tutorial on a dataset of your own that does!","metadata":{}},{"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n  \n    \"\"\" Returns the specified model with the specified parameters \"\"\"\n\n    if model_name =='mask_rcnn':\n\n        \"\"\"FastRCNN + MaskRCNN with ResNet50 backbone\"\"\"\n\n        # load an object detection model pre-trained on COCO\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='MaskRCNN_ResNet50_FPN_Weights.DEFAULT')\n         \n        # get the number of input features for the classifier\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        # replace the pre-trained head with a new one\n        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n        # now get the number of input features for the mask classifier\n        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n        hidden_layer = 256\n        # and replace the mask predictor with a new one\n        model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                         hidden_layer,\n                                                         num_classes)\n        return model\n\n    if model_name =='fast_rcnn':\n\n        \"\"\" Fast RCNN with ResNet50 backbone \"\"\"\n\n        # load an instance segmentation model pre-trained on COCO\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='MaskRCNN_ResNet50_FPN_Weights.DEFAULT')\n         \n        # get the number of input features for the classifier\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        # replace the pre-trained head with a new one\n        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n        return model   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to define a [custom PyTorch Dataset](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) class to pre-process and index our images and metadata to retrieve later. Note that while we don't use all of the keys created in the `target` dictionary below, our model and evaluator expect them to be there, so we define them but leave them empty.","metadata":{}},{"cell_type":"code","source":"class PenguinsVsTurtles(torch.utils.data.Dataset):\n  \n    def __init__(self, root, annotations_list, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root))))\n        self.labels = [dct['category_id'] for dct in annotations_list]\n        self.annotations_list = annotations_list\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, self.imgs[idx])\n        label_bbox = self.annotations_list[idx]['bbox']\n        img = Image.open(img_path).convert(\"RGB\")\n\n        masks = np.zeros((3,640,640))\n\n        # get bounding box coordinates for each mask\n        # note that we return bounding boxes in pascal voc format, as that's\n        # what our model accepts. The COCO evaluator will convert to COCO format later.\n        num_objs = 1\n        boxes = []\n        boxes.append([label_bbox[0], label_bbox[1], (label_bbox[0]+label_bbox[2]), (label_bbox[1]+label_bbox[3])])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.as_tensor((self.annotations_list[idx]['category_id'],), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now define our [dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), which generate our inputs, apply transformations to our images, and [collate](https://plainenglish.io/blog/understanding-collate-fn-in-pytorch-f9d1742647d3) our batches.","metadata":{}},{"cell_type":"code","source":"# use our dataset and defined transformations\ntrain_dataset = PenguinsVsTurtles('./train', annotations_list=train_labels, transforms=get_transform(train=True))\nvalid_dataset = PenguinsVsTurtles('./valid', annotations_list=valid_labels, transforms=get_transform(train=False))\n\n# define training and validation data loaders\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=2, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset, batch_size=1, shuffle=False, num_workers=2,\n    collate_fn=utils.collate_fn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We instantiate our model and log the model graph to [Comet](https://www.comet.com/signup/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog). We unfreeze our model layers for training, and pass our hyperparameter values as defined above in our hyperparameter dictionary.","metadata":{}},{"cell_type":"code","source":"# get the model using our helper function\nmodel = initialize_model(hyper_params[\"model_name\"], hyper_params[\"num_classes\"], hyper_params[\"feature_extract\"])\n#log model graph\nexperiment.set_model_graph(model)\n\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, \n                            lr=hyper_params[\"lr\"],\n                            momentum=hyper_params[\"momentum\"], \n                            weight_decay=hyper_params[\"weight_decay\"])\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size= hyper_params[\"step_size\"],\n                                               gamma=hyper_params[\"gamma\"])\n\nnum_epochs = hyper_params[\"num_epochs\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = []\nimage_map_results = []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train and validate\nfor epoch in range(hyper_params['num_epochs']):\n\n    # train for one epoch, printing every 5 iterations\n    metric_logger = train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=50)\n    my_log_func(logger=metric_logger)\n\n    # update the learning rate\n    lr_scheduler.step()\n\n    # Evaluate with validation dataset\n    coco_evaluator, predictions= evaluate(model, valid_data_loader, device)\n\n    # log images and annotations to Comet\n    for prediction in predictions:\n        image = ToPILImage(prediction[0][3])\n        name = 'image id: {}'.format(str(prediction[0][0]))\n        annotations = make_annotations(prediction)\n        experiment.log_image(image, name=name, annotations= annotations)\n      \n      #image-level mAP\n        if epoch == hyper_params['num_epochs']-1:\n            metric = MeanAveragePrecision(iou_type=\"bbox\")\n            metric.update([prediction[0][2]], [prediction[0][1]])\n            result = metric.compute()\n            image_map_results.append([x.item() for x in result.values()])\n            image_id.append(name)\n\n    # log epoch mAP with IoU threshold >= .50 and maxDets = 100\n    epoch_map = coco_evaluator.coco_eval['bbox'].stats[1]\n    experiment.log_metric(\"epoch mAP\", epoch_map)\n    # log epoch mAR with 0.5 <= IoU thresh <= 0.95 and maxDets = 100\n    epoch_mar = coco_evaluator.coco_eval['bbox'].stats[8]\n    experiment.log_metric(\"epoch mAR\", epoch_mar)\n\n    #calculate and log epoch f1\n    epoch_f1 = calculate_f1_score(epoch_map, epoch_mar)\n    experiment.log_metric(\"epoch f1\", epoch_f1)\n\n    #labels - 1 to remove background class\n    ground_truth_labels = [predictions[idx][0][1]['labels'].item()-1 for idx in range(len(predictions))]\n    predicted_labels = [predictions[idx][0][2]['labels'][0].item()-1 for idx in range(len(predictions))]\n    image_list = [ToPILImage(predictions[idx][0][3]) for idx in range(len(predictions))]\n    for i, pil_img in enumerate(image_list):\n        add_PIL_bboxes(pil_img, predictions[i][0][2]['boxes'].tolist()[0])\n\n    experiment.log_confusion_matrix(\n        ground_truth_labels,\n        predicted_labels,\n        images=image_list,\n        row_label=\"Actual category\",\n        column_label = \"Predicted category\",\n        title=\"Confusion Matrix: Penguins vs Turtles\",\n        file_name=\"confusion-matrix-epoch-{}.json\".format(epoch),\n        labels = list(label_dict.values())\n        )\n\n# create pandas DataFrame and log to Comet Data Panel\ncolumns = [k for k in result.keys()]\nresults_dict = dict(zip(image_id, image_map_results))\nexperiment.log_table('image_level_map.csv', pd.DataFrame.from_dict(results_dict, orient='index', columns=columns))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment.end()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's head on over to the Comet UI to explore our results and see how we might improve our model based on insights gained from the confusion matrices we logged.\n\n## Using the Confusion Matrix\n\nSelect the experiment you’d like to view, then find the ‘Confusion Matrix’ tab on the lefthand sidebar. We can add multiple matrices to the same view, or switch between confusion matrices by selecting them from the drop-down menu at the top. By hovering over the different cells of the confusion matrix, you’ll see a quick break down of the samples from that cell. By clicking on a cell, we can also see specific instances where the model misclassified an image. By default, a maximum of 25 example images is uploaded per cell, but this can be reconfigured with the API.\n\n[![accessing_conf_matrices.gif](https://s12.gifyu.com/images/accessing_conf_matrices.gif)](https://www.comet.com/anmorgan24/interactive-confusion-matrix/view/1oobhPiRdETUwTnWr9AGhOK0b/panels/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog)\n\nBecause we trained our model for three epochs and logged one matrix per epoch, here we can watch how our models improve over time. Are there particular images our model tends to struggle with? How can we use this information to augment our training data and improve our model’s performance? \n\n### View Specific Instances\n\nIn the example below, the model seems to get confused by images of white turtles, so maybe we can add some more examples in a future run. In any event, we can see that our model clearly makes fewer mistakes over time, eventually classifying all of the images correctly.\n\n[![Screenshot-2023-05-01-at-7.29.50-PM.png](https://s12.gifyu.com/images/Screenshot-2023-05-01-at-7.29.50-PM.png)](https://www.comet.com/anmorgan24/interactive-confusion-matrix/view/1oobhPiRdETUwTnWr9AGhOK0b/panels/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog)\n\nWe can also click on individual images to examine them more closely. This can be especially helpful in object detection use cases, where visualizing the bounding box location can help us understand where the model is going wrong.\n\n[![image-misclassification-instances.gif](https://s12.gifyu.com/images/image-misclassification-instances.gif)](https://www.comet.com/anmorgan24/interactive-confusion-matrix/view/1oobhPiRdETUwTnWr9AGhOK0b/panels/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog)\n\nWhen examining specific instances of misclassifications, we can see that the model sometimes categorizes large boulders as turtles, and tends to get confused by one particularly unique breed of penguin. We could choose to augment our training data with images containing similar examples to improve performance.\n\n### Aggregating Values\n\nWe can also choose three different methods of aggregating the cells in our confusion matrices: by count, percent by row, and percent by column. \n\n[![augmenting-conf-mat-values2.gif](https://s11.gifyu.com/images/augmenting-conf-mat-values2.gif)](https://www.comet.com/anmorgan24/interactive-confusion-matrix/view/1oobhPiRdETUwTnWr9AGhOK0b/panels/?utm_source=Kaggle_ANM&utm_medium=referral&utm_content=confusion_matrix_with_images_blog)\n\nThanks for making it all the way to the end, and we hope you found this tutorial useful!\n","metadata":{}}]}
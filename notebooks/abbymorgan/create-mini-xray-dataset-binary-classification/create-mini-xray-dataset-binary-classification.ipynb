{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Create Mini Xray Dataset With Healthy and Diseased Xrays ","metadata":{}},{"cell_type":"markdown","source":"This notebook is modified from **K Scott Mader's** notebook [here](https://www.kaggle.com/code/kmader/create-a-mini-xray-dataset-equalized/notebook) to create a mini chest x-ray dataset that is split 50:50 between normal and diseased images.\n\nIn my notebook I will use this dataset to test a pretrained model on a binary classification task (diseased vs. healthy xray), and then visualize which specific labels the model has the most trouble with. \n\nAlso, because disease classification is such an important task to get right, it's likely that any AI/ML medical classification task will include a human-in-the-loop. In this way, this process more closely resembles how this sort of ML would be used in the real world.\n\nNote that the original notebook on which this one was based had two versions: [Standard](https://www.kaggle.com/code/kmader/create-a-mini-xray-dataset-standard) and [Equalized](https://www.kaggle.com/code/kmader/create-a-mini-xray-dataset-equalized/notebook). In this notebook we will be using the equalized version in order to save ourselves the extra step of performing CLAHE during the tensor transformations.\n\n### Goal\n\nThe goal of this notebook, as originally stated by Mader, is \"to make a much easier to use mini-dataset out of the Chest X-Ray collection. The idea is to have something akin to MNIST or Fashion MNIST for medical images.\" In order to do this, we will preprocess, normalize, and scale down the images, and then save them into an HDF5 file with the corresponding tabular data.\n\n### Import Libraries","metadata":{}},{"cell_type":"code","source":"from itertools import chain\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport h5py\nfrom cv2 import imread, createCLAHE # read and equalize images\nfrom skimage.transform import resize\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:12.402006Z","iopub.execute_input":"2023-01-07T15:44:12.402393Z","iopub.status.idle":"2023-01-07T15:44:12.789521Z","shell.execute_reply.started":"2023-01-07T15:44:12.402337Z","shell.execute_reply":"2023-01-07T15:44:12.78875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Helper Functions","metadata":{}},{"cell_type":"code","source":"def write_df_as_hdf(out_path,\n                    out_df,\n                    compression='gzip'):\n    with h5py.File(out_path, 'w') as h:\n        for k, arr_dict in tqdm(out_df.to_dict().items()):\n            try:\n                s_data = np.stack(arr_dict.values(), 0)\n                try:\n                    h.create_dataset(k, data=s_data, compression=\n                    compression)\n                except TypeError as e:\n                    try:\n                        h.create_dataset(k, data=s_data.astype(np.string_),\n                                         compression=compression)\n                    except TypeError as e2:\n                        print('%s could not be added to hdf5, %s' % (\n                            k, repr(e), repr(e2)))\n            except ValueError as e:\n                print('%s could not be created, %s' % (k, repr(e)))\n                all_shape = [np.shape(x) for x in arr_dict.values()]\n                warn('Input shapes: {}'.format(all_shape))","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:15.040711Z","iopub.execute_input":"2023-01-07T15:44:15.041061Z","iopub.status.idle":"2023-01-07T15:44:15.048822Z","shell.execute_reply.started":"2023-01-07T15:44:15.041033Z","shell.execute_reply":"2023-01-07T15:44:15.047797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imread_and_normalize(im_path):\n    clahe_tool = createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    img_data = np.mean(imread(im_path), 2).astype(np.uint8)\n    img_data = clahe_tool.apply(img_data)\n    n_img = (255*resize(img_data, OUT_DIM, mode = 'constant')).clip(0,255).astype(np.uint8)\n    return np.expand_dims(n_img, -1)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:17.976812Z","iopub.execute_input":"2023-01-07T15:44:17.977143Z","iopub.status.idle":"2023-01-07T15:44:17.983124Z","shell.execute_reply.started":"2023-01-07T15:44:17.977115Z","shell.execute_reply":"2023-01-07T15:44:17.982155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define configs\n# SAMPLE_SIZE = sample size per category (diseased vs. healthy)\n# Total samples saved to output file will be 2 * SAMPLE_SIZE\n\nSAMPLE_SIZE = 100\nFILE_NAME = \"TEST\"\nOUT_DIM = (128, 128)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:20.764782Z","iopub.execute_input":"2023-01-07T15:44:20.765259Z","iopub.status.idle":"2023-01-07T15:44:20.768301Z","shell.execute_reply.started":"2023-01-07T15:44:20.765231Z","shell.execute_reply":"2023-01-07T15:44:20.767739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_xray_df = pd.read_csv('../input/data/Data_Entry_2017.csv') \nall_image_paths = {os.path.basename(x): x for x in \n                   glob(os.path.join('..', 'input', 'data','images*', '*', '*.png'))}\nprint('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n\nall_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\nall_xray_df.sample(3)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:22.864576Z","iopub.execute_input":"2023-01-07T15:44:22.864882Z","iopub.status.idle":"2023-01-07T15:44:23.404849Z","shell.execute_reply.started":"2023-01-07T15:44:22.864855Z","shell.execute_reply":"2023-01-07T15:44:23.404035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess Labels\nHere we one-hot encode the disease labels to make them easier to group and sort on later. We will also create a subset of the diseased Xray data that preserves the original disease distribution.","metadata":{}},{"cell_type":"code","source":"# Visualize distribution of most popular diseases in full population\n# Only show 15 most common diseases\n# First (most common bar) represents normal xrays, will drop\nlabel_counts = all_xray_df['Finding Labels'].value_counts()[1:16] # \nfig, ax1 = plt.subplots(1,1,figsize = (12, 8))\nax1.bar(np.arange(len(label_counts))+0.5, label_counts)\nax1.set_xticks(np.arange(len(label_counts))+0.5)\n_ = ax1.set_xticklabels(label_counts.index, rotation = 90)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:25.605034Z","iopub.execute_input":"2023-01-07T15:44:25.605999Z","iopub.status.idle":"2023-01-07T15:44:25.832404Z","shell.execute_reply.started":"2023-01-07T15:44:25.605944Z","shell.execute_reply":"2023-01-07T15:44:25.831481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split 'Finding Labels'\nall_xray_df['Finding Labels'] = all_xray_df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\nall_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\nprint('All Labels', all_labels)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:29.964803Z","iopub.execute_input":"2023-01-07T15:44:29.965114Z","iopub.status.idle":"2023-01-07T15:44:30.195031Z","shell.execute_reply.started":"2023-01-07T15:44:29.965088Z","shell.execute_reply":"2023-01-07T15:44:30.19418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dummy encode disease classifications\nfor c_label in all_labels:\n    if len(c_label)>1: # leave out empty labels\n        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:30.296173Z","iopub.execute_input":"2023-01-07T15:44:30.296498Z","iopub.status.idle":"2023-01-07T15:44:30.791202Z","shell.execute_reply.started":"2023-01-07T15:44:30.296471Z","shell.execute_reply":"2023-01-07T15:44:30.790438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Normal Subset\nCreate random sample of normal Xray data (of size `SAMPLE_SIZE`)","metadata":{}},{"cell_type":"code","source":"# Create subset of SAMPLE_SIZE # of normal samples and set aside\nnormal_xray_df = all_xray_df[all_xray_df['Finding Labels']=='']\nnormal_xray_df = normal_xray_df.sample(SAMPLE_SIZE)\nnormal_xray_df['Finding Labels'] = normal_xray_df['Finding Labels'].map(lambda x: x.replace('', 'Normal'))","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:33.032789Z","iopub.execute_input":"2023-01-07T15:44:33.033105Z","iopub.status.idle":"2023-01-07T15:44:33.063861Z","shell.execute_reply.started":"2023-01-07T15:44:33.033076Z","shell.execute_reply":"2023-01-07T15:44:33.06298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Diseased Subset\nCreate sample of diseased Xray data that preserves original distribution (of size `SAMPLE_SIZE`)","metadata":{}},{"cell_type":"code","source":"# Calculate disease sample weights\ndis_xray_df = all_xray_df[all_xray_df['Finding Labels']!='']\n# Make a subset of the the diseased samples that preserves original distribution\n# Weight is 0.1 + number of findings\nsample_weights = dis_xray_df['Finding Labels'].map(lambda x: len(x.split('|')) if len(x)>0 else 0).values + 1e-1\nsample_weights /= sample_weights.sum()\ndis_xray_df = dis_xray_df.sample(SAMPLE_SIZE, weights=sample_weights)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:38.87369Z","iopub.execute_input":"2023-01-07T15:44:38.873997Z","iopub.status.idle":"2023-01-07T15:44:38.927976Z","shell.execute_reply.started":"2023-01-07T15:44:38.873972Z","shell.execute_reply":"2023-01-07T15:44:38.926881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize distribution of most popular diseases in sample population\nlabel_counts = dis_xray_df['Finding Labels'].value_counts()[:15]\nfig, ax1 = plt.subplots(1,1,figsize = (12, 8))\nax1.bar(np.arange(len(label_counts))+0.5, label_counts)\nax1.set_xticks(np.arange(len(label_counts))+0.5)\n_ = ax1.set_xticklabels(label_counts.index, rotation = 90)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:41.220625Z","iopub.execute_input":"2023-01-07T15:44:41.221209Z","iopub.status.idle":"2023-01-07T15:44:41.435484Z","shell.execute_reply.started":"2023-01-07T15:44:41.221176Z","shell.execute_reply":"2023-01-07T15:44:41.434638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combine and Save Tabular Data\nCombine `SAMPLE_SIZE` of normal and `SAMPLE_SIZE` of diseased tabular data and save to HDF5.","metadata":{}},{"cell_type":"code","source":"# Combine samples\n# Shuffle samples with .sample(frac=1) and drop indices\nfinal_xray_df = pd.concat([dis_xray_df, normal_xray_df], axis = 0).sample(frac=1).reset_index(drop=True)\nfinal_xray_df.sample(3)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:45.172821Z","iopub.execute_input":"2023-01-07T15:44:45.173151Z","iopub.status.idle":"2023-01-07T15:44:45.198683Z","shell.execute_reply.started":"2023-01-07T15:44:45.17312Z","shell.execute_reply":"2023-01-07T15:44:45.197767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write tabular data to HDF5 file\nwrite_df_as_hdf(f'{FILE_NAME}.h5', final_xray_df)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:48.004391Z","iopub.execute_input":"2023-01-07T15:44:48.004768Z","iopub.status.idle":"2023-01-07T15:44:48.218447Z","shell.execute_reply.started":"2023-01-07T15:44:48.004739Z","shell.execute_reply":"2023-01-07T15:44:48.217569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show breakdown of tabular data \nwith h5py.File(f'{FILE_NAME}.h5', 'r') as h5_data:\n    for c_key in h5_data.keys():\n        print(c_key, h5_data[c_key].shape, h5_data[c_key].dtype)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:44:50.280543Z","iopub.execute_input":"2023-01-07T15:44:50.281034Z","iopub.status.idle":"2023-01-07T15:44:50.294824Z","shell.execute_reply.started":"2023-01-07T15:44:50.281007Z","shell.execute_reply":"2023-01-07T15:44:50.294062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Image Subset and Save\nCollect relevant images (referred to in tabular dataset) and add to HDF5.","metadata":{}},{"cell_type":"code","source":"# Show example Xray image\ntest_img = imread_and_normalize(all_xray_df['path'].values[0])\nplt.matshow(test_img[:,:,0])","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:45:07.480904Z","iopub.execute_input":"2023-01-07T15:45:07.481207Z","iopub.status.idle":"2023-01-07T15:45:07.758009Z","shell.execute_reply.started":"2023-01-07T15:45:07.481182Z","shell.execute_reply":"2023-01-07T15:45:07.757313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preallocate output\nout_image_arr = np.zeros((all_xray_df.shape[0],)+OUT_DIM+(1,), dtype=np.uint8)\nif False:\n    # a difficult to compress array for size approximations\n    out_image_arr = np.random.uniform(0, 255,\n                                  size = (final_xray_df.shape[0],)+OUT_DIM+(1,)).astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:45:10.667733Z","iopub.execute_input":"2023-01-07T15:45:10.668057Z","iopub.status.idle":"2023-01-07T15:45:10.673119Z","shell.execute_reply.started":"2023-01-07T15:45:10.668028Z","shell.execute_reply":"2023-01-07T15:45:10.672156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_xray_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:45:12.723462Z","iopub.execute_input":"2023-01-07T15:45:12.723824Z","iopub.status.idle":"2023-01-07T15:45:12.730676Z","shell.execute_reply.started":"2023-01-07T15:45:12.723786Z","shell.execute_reply":"2023-01-07T15:45:12.729827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preallocate output\nout_image_arr = np.zeros((final_xray_df.shape[0],)+OUT_DIM+(1,), dtype=np.uint8)\nif False:\n    # a difficult to compress array for size approximations\n    out_image_arr = np.random.uniform(0, 255,\n                                  size = (final_xray_df.shape[0],)+OUT_DIM+(1,)).astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:45:15.215722Z","iopub.execute_input":"2023-01-07T15:45:15.21655Z","iopub.status.idle":"2023-01-07T15:45:15.221008Z","shell.execute_reply.started":"2023-01-07T15:45:15.21652Z","shell.execute_reply":"2023-01-07T15:45:15.22033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, c_path in enumerate(tqdm(final_xray_df['path'].values)):\n    out_image_arr[i] = imread_and_normalize(c_path)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:45:17.227478Z","iopub.execute_input":"2023-01-07T15:45:17.227789Z","iopub.status.idle":"2023-01-07T15:45:36.509795Z","shell.execute_reply.started":"2023-01-07T15:45:17.227765Z","shell.execute_reply":"2023-01-07T15:45:36.508885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Append the array\nwith h5py.File(f'{FILE_NAME}.h5', 'a') as h5_data:\n    h5_data.create_dataset('images', data = out_image_arr, compression = None) # compression takes too long\n    for c_key in h5_data.keys():\n        print(c_key, h5_data[c_key].shape, h5_data[c_key].dtype)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:45:41.099808Z","iopub.execute_input":"2023-01-07T15:45:41.100379Z","iopub.status.idle":"2023-01-07T15:45:41.117335Z","shell.execute_reply.started":"2023-01-07T15:45:41.100345Z","shell.execute_reply":"2023-01-07T15:45:41.116444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Output File-size %2.2fMB' % (os.path.getsize(f'{FILE_NAME}.h5')/1e6))","metadata":{"execution":{"iopub.status.busy":"2023-01-07T15:45:45.123344Z","iopub.execute_input":"2023-01-07T15:45:45.123697Z","iopub.status.idle":"2023-01-07T15:45:45.129519Z","shell.execute_reply.started":"2023-01-07T15:45:45.123667Z","shell.execute_reply":"2023-01-07T15:45:45.128632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next Steps\nTo read **this** HDF5 file into your own environment, set up your Kaggle credentials in your working directory and run:\n\n`!kaggle kernels output abbymorgan/create-mini-xray-dataset-binary-classification -p path/to/dest`\n\nIf you copy this notebook and make **your own subset** of the original dataset, to load your HDF5 file:\n\n- Make sure to **commit** the final version of your notebook ([see here for more info on commiting to Kaggle](https://www.kaggle.com/general/224266)).\n- Navigate to the 'Data' tab of your saved notebook and copy the bash command at the bottom of the page.\n- Paste the bash command in your working directory containing a `kaggle.json` with your Kaggle credentials. \n\nFor more information on how to read HDF5 files, see the following resources:\n\n- `[colab link here]`\n- [h5py Documentation](https://docs.h5py.org/en/stable/quick.html)\n- ['How to Read HDF5 Files in Python](https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/), Python For the Lab","metadata":{}}]}
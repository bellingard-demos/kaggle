{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img align= \"left\" src=\"https://cdn.comet.ml/img/notebook_logo.png\">\n","metadata":{}},{"cell_type":"markdown","source":"# ‚òÑ Comet Data Panels\n\nComet‚Äôs long-standing mission is to help Data Scientists and Machine Learning Engineers find the best model for their use-case by arming them with powerful tools and visualizations. In an effort to continue to deliver on this mission, we are excited to launch our new built-in panel: **The Data Panel**. The Data Panel allows users to visualize and interact with their tabular data and aggregate tables across Experiments. The Panel works out of the box with pandas dataframes and .csv files.\n\nTo showcase our new Data Panel, we‚Äôve put together a public project using [Kaggle‚Äôs Credit Card Fraud Detection dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). View the [Comet public project here](https://www.comet.com/anmorgan24/datapanels/view/ndkH4IkEmrTdYr0hvO97AxjeY/panels?utm_source=Medium&utm_medium=referral&utm_content=data_panels_blog). For more detail, check out the [full blog post here](https://medium.com/better-programming/credit-card-fraud-detection-with-autoencoders-4e01b2038a91).","metadata":{}},{"cell_type":"markdown","source":"## üî© Install requirements & set up\n_____","metadata":{}},{"cell_type":"code","source":"!pip install comet_ml torch torchvision tqdm --root-user-action=ignore --quiet","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:29:30.443888Z","iopub.execute_input":"2023-02-19T14:29:30.444976Z","iopub.status.idle":"2023-02-19T14:29:47.866049Z","shell.execute_reply.started":"2023-02-19T14:29:30.444867Z","shell.execute_reply":"2023-02-19T14:29:47.864146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To instantiate your Comet Experiment, you'll need to grab your API key from your [account settings](https://www.comet.com/account-settings/profile?utm_source=Medium&utm_medium=referral&utm_content=data_panels_blog). If you don't already have an account, [create one here for free](https://www.comet.com/signup?utm_source=Medium&utm_medium=referral&utm_content=data_panels_blog).","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nYOUR_COMET_API_KEY = user_secrets.get_secret(\"YOUR-COMET-API-KEY\")","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:29:47.868934Z","iopub.execute_input":"2023-02-19T14:29:47.869487Z","iopub.status.idle":"2023-02-19T14:29:48.163081Z","shell.execute_reply.started":"2023-02-19T14:29:47.869433Z","shell.execute_reply":"2023-02-19T14:29:48.161728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import comet_ml\nfrom comet_ml import Experiment \n\nexperiment = Experiment(\n    api_key = YOUR_COMET_API_KEY,\n    project_name = \"datapanels-kaggle\",\n    workspace = \"anmorgan24\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:29:48.164842Z","iopub.execute_input":"2023-02-19T14:29:48.165953Z","iopub.status.idle":"2023-02-19T14:29:50.963906Z","shell.execute_reply.started":"2023-02-19T14:29:48.165891Z","shell.execute_reply":"2023-02-19T14:29:50.962439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data_utils\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as img\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:29:50.969421Z","iopub.execute_input":"2023-02-19T14:29:50.970547Z","iopub.status.idle":"2023-02-19T14:29:54.608094Z","shell.execute_reply.started":"2023-02-19T14:29:50.970473Z","shell.execute_reply":"2023-02-19T14:29:54.606745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üõ† Data preparation\n_____\n\n\n[Fraud detection](https://heartbeat.comet.ml/fraud-detection-imbalanced-classification-and-managing-your-machine-learning-experiments-using-224ecf00bf7e) comes with its own set of unique challenges. Fraudulent transactions represent anomalous data that make up a very small percentage of total transactions. These significant [class imbalances](https://heartbeat.comet.ml/imbalanced-classification-demystified-66a401d6e805) make many traditional evaluation metrics, like [accuracy](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/), irrelevant. In the Kaggle dataset, fraud makes up just 0.172% of all transactions, meaning a model that predicted ‚Äúno fraud‚Äù for every single observation would still achieve an accuracy of over 98%! \n\nIn the pie chart below, fraudulent transactions are represented by the tiny orange sliver!","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/creditcardfraud/creditcard.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:29:54.619322Z","iopub.execute_input":"2023-02-19T14:29:54.620517Z","iopub.status.idle":"2023-02-19T14:29:59.935735Z","shell.execute_reply.started":"2023-02-19T14:29:54.620478Z","shell.execute_reply":"2023-02-19T14:29:59.934232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orange, purple = '#ffa022', '#4012ff'\nfig, ax = plt.subplots()\nax.pie(data.groupby(['Class']).Class.count(), labels= ['Normal', 'Fraud'], colors=[purple, orange])\nplt.title('Normal vs fraudulent transactions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:29:59.937287Z","iopub.execute_input":"2023-02-19T14:29:59.937695Z","iopub.status.idle":"2023-02-19T14:30:00.196359Z","shell.execute_reply.started":"2023-02-19T14:29:59.937659Z","shell.execute_reply":"2023-02-19T14:30:00.194796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Additionally, in order to protect all personally identifiable information ([PII](https://www.techtarget.com/searchsecurity/definition/personally-identifiable-information-PII)) in this dataset, all features (besides `Time` and `Amount`) have undergone a [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) transformation. This means that we cannot use any domain knowledge for the purposes of feature engineering or selection. \n\nLet's start out by visualizing our data. It would be impossible to plot 30 dimensions, so first we'll apply [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), a dataset decomposition technique, and plot the 2 components with maximum information.","metadata":{}},{"cell_type":"code","source":"#data = pd.read_csv('creditcard.csv')\n\n# Take a random sample of 1000 normal transactions and all fraud transactions\nnon_fraud = data[data['Class'] == 0].sample(1000)\nfraud = data[data['Class'] == 1]\n\n# Shuffle fraud and non_fraud samples and separate into features and labels\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['Class'], axis = 1).values\ny = df[\"Class\"].values","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:00.197775Z","iopub.execute_input":"2023-02-19T14:30:00.198107Z","iopub.status.idle":"2023-02-19T14:30:00.282872Z","shell.execute_reply.started":"2023-02-19T14:30:00.198076Z","shell.execute_reply":"2023-02-19T14:30:00.281475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tsne_plot(X, y, name, title='TSNE plot of Fraud and Non Fraud'):\n    tsne = TSNE(n_components=2, random_state=24, init='random', learning_rate=200)\n    X_t = tsne.fit_transform(X)\n\n    plt.figure(figsize=(12,8))\n    plt.scatter(X_t[np.where(y==0), 0], X_t[np.where(y==0), 1], color='darkorange', alpha = 0.7, label = \"Non Fraud\")\n    plt.scatter(X_t[np.where(y==1), 0], X_t[np.where(y==1), 1], color='mediumblue', alpha = 0.7, label = \"Fraud\")\n\n    plt.legend(loc='best')\n    plt.title(title, fontsize=16)\n    experiment.log_figure(figure=plt)\n    plt.savefig(name)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:00.284309Z","iopub.execute_input":"2023-02-19T14:30:00.284747Z","iopub.status.idle":"2023-02-19T14:30:00.294778Z","shell.execute_reply.started":"2023-02-19T14:30:00.284711Z","shell.execute_reply":"2023-02-19T14:30:00.293484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_plot(X, y, name=\"original.png\")","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:00.299345Z","iopub.execute_input":"2023-02-19T14:30:00.299842Z","iopub.status.idle":"2023-02-19T14:30:11.728794Z","shell.execute_reply.started":"2023-02-19T14:30:00.299804Z","shell.execute_reply":"2023-02-19T14:30:11.727368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe in this graph that there is little difference between fraudulent and non-fraudlent transactions. [Most machine learning models would struggle to classify this data as-is](https://machinelearningmastery.com/imbalanced-classification-is-hard/).\n\nFirst, we'll perform some very basic transformations to the data before feeding it to our model. The original **`Time`** feature represents the number of seconds elapsed between each transaction and the first transaction in the data. We'll convert this relative time measure to hour-of-the-day. \n\nWe also scale the **`Time`** and **`Amount`** features, as all other features (**`V1`**, **`V2`**, ... **`V28`**) were previously scaled during the PCA transformation. Finally, because we are training an autoencoder to ‚Äúlearn‚Äù the embeddings of a normal transaction, we‚Äôll subset just normal samples as our training dataset, and use a 50:50 split of normal and fraudulent samples for our validation set. We'll also set aside 250 samples for a test dataset of completely unseen data.","metadata":{}},{"cell_type":"code","source":"# Convert relative 'Time' measure to hour of day\ndata[\"Time\"] = data[\"Time\"].apply(lambda x : x / 3600 % 24)\n\n# Scale 'Time' and 'Amount'\ndata['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\ndata['Time'] = StandardScaler().fit_transform(data['Time'].values.reshape(-1, 1))\n\n# Separate fraudulent transactions from normal transactions\nnorm_data, fraud_data = data[data['Class']==0], data[data['Class']==1]\n\n# Training data will be 2000 normal samples\n# Validation data will be 1000 samples, approximately half fraud and half normal samples; shuffled and with indices reset\n# We leave out 200 normal samples and 50 fraud samples for a final test dataset of 250 unseen samples; we'll choose a random subsample of these later\ntrain_data = norm_data.iloc[:2000, :]\nval_data = pd.concat([norm_data.iloc[2000:2558,:], fraud_data.iloc[:442, :]], axis =0).sample(frac=1).reset_index(drop=True)\ntest_samples = pd.concat([norm_data.iloc[2558:2608,:], fraud_data.iloc[442:,:]], axis=0).sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:11.730337Z","iopub.execute_input":"2023-02-19T14:30:11.730744Z","iopub.status.idle":"2023-02-19T14:30:11.911881Z","shell.execute_reply.started":"2023-02-19T14:30:11.730709Z","shell.execute_reply":"2023-02-19T14:30:11.910321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ü¶æ Model training and inference\n_____\n\nWhile there are various ways to approach this problem, one of the simplest and most effective is to use an autoencoder. Autoencoders learn an implicit representation of normality from the abundant ‚Äúnormal‚Äù samples, allowing us to reserve our sparse fraudulent data samples for testing. During inference, new samples are compared against the embeddings of normal samples to determine whether or not they are fraudulent.  While Deep Learning techniques aren‚Äôt applied to tabular data as often as [unstructured data](https://en.wikipedia.org/wiki/Unstructured_data), the stark class imbalances in this dataset make it a perfect candidate for use with an autoencoder. \n\nFirst we'll define our model hyperparameters and create DataLoader objects for our training and validation splits.","metadata":{}},{"cell_type":"code","source":"# These hyperparameters will be logged\nhyper_params = {\n    \"learning_rate\": 1e-1/10,\n    \"epochs\": 150,\n    \"batch_size\": 32,\n    \"weight_decay\": 1e-8,\n    'threshold': 0.75\n}\n\nexperiment.log_parameters(hyper_params)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:11.9141Z","iopub.execute_input":"2023-02-19T14:30:11.914863Z","iopub.status.idle":"2023-02-19T14:30:11.923208Z","shell.execute_reply.started":"2023-02-19T14:30:11.914806Z","shell.execute_reply":"2023-02-19T14:30:11.921797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create pytorch data loader for training set\ntrain_target = torch.tensor(train_data['Class'].values.astype(np.float32))\ntrain = torch.tensor(train_data.drop('Class', axis = 1).values.astype(np.float32))\ntrain_tensor = data_utils.TensorDataset(train, train_target) \ntrain_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = hyper_params['batch_size'], shuffle = True)\n\n# Create pytorch data loader for validation set\nval_target = torch.tensor(val_data['Class'].values.astype(np.float32))\nval = torch.tensor(val_data.drop('Class', axis = 1).values.astype(np.float32))\nval_tensor = data_utils.TensorDataset(val, val_target) \nval_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = 1, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:11.925319Z","iopub.execute_input":"2023-02-19T14:30:11.925818Z","iopub.status.idle":"2023-02-19T14:30:11.991027Z","shell.execute_reply.started":"2023-02-19T14:30:11.925772Z","shell.execute_reply":"2023-02-19T14:30:11.989892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(30,14),\n            nn.Tanh(),\n            nn.Linear(14,7),\n            nn.Tanh(),\n            )\n        self.decoder = nn.Sequential(\n            nn.Linear(7, 14),\n            nn.Tanh(),\n            nn.Linear(14,30),\n            nn.Tanh()\n            )\n    def forward(self,x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:11.992435Z","iopub.execute_input":"2023-02-19T14:30:11.993175Z","iopub.status.idle":"2023-02-19T14:30:12.008489Z","shell.execute_reply.started":"2023-02-19T14:30:11.993135Z","shell.execute_reply":"2023-02-19T14:30:12.007084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Autoencoder()\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), \n                             lr=hyper_params[\"learning_rate\"], \n                             weight_decay=hyper_params['weight_decay'])","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:12.010205Z","iopub.execute_input":"2023-02-19T14:30:12.010691Z","iopub.status.idle":"2023-02-19T14:30:12.035715Z","shell.execute_reply.started":"2023-02-19T14:30:12.010643Z","shell.execute_reply":"2023-02-19T14:30:12.034797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(hyper_params['epochs']):\n    losses = []\n    for (data, _) in train_loader:\n        # ===================forward=====================\n        output = model(data)\n        loss = criterion(output, data)     \n        # ===================backward====================\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.detach().cpu().numpy())\n    # ===================log========================\n    experiment.log_metric('epoch_loss', np.mean(losses), epoch = epoch+1)\n\n    losses = []\n    labels = []\n    preds = []\n    embeds = []\n    \n    for data, label in val_loader:\n        # ===================forward=====================\n        reconstructed = model(data)\n        loss = torch.sigmoid(criterion(reconstructed, data)).item()\n        losses.append(round((loss),6))\n        labels.append(label.item())\n        preds.append(reconstructed[0][-1].item())\n        embeds.append(reconstructed[0][:-1].detach().cpu().numpy())\n\n    # ===================log========================  \n    experiment.log_metric('val_loss', np.mean(losses), epoch = epoch +1)\n\n    if epoch == (hyper_params[\"epochs\"] - 1):\n        print(\"logging table\")\n        df = pd.DataFrame()\n        df['Reconstruction_Loss'] = losses\n        df['Labels_gt'] = labels\n        df['Labels_preds']= [int(x >= hyper_params['threshold']) for x in losses]\n        experiment.log_table('val_predictions.csv', df)\nprint('complete')","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:30:12.037037Z","iopub.execute_input":"2023-02-19T14:30:12.037521Z","iopub.status.idle":"2023-02-19T14:31:23.977495Z","shell.execute_reply.started":"2023-02-19T14:30:12.037473Z","shell.execute_reply":"2023-02-19T14:31:23.975791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ‚§¥ Precision-recall curve, threshold values, classification report\n____\nDefine plot functions:","metadata":{}},{"cell_type":"code","source":"def plot_precision_recall_curves(df):\n    # Calculate precision and recall \n    precision, recall, thresholds = precision_recall_curve(labels, preds)\n\n    # Plot recall precision tradeoff\n    sns.set(style=\"white\")\n    plt.figure(figsize=(8,6))\n    plt.step(recall, precision, color= purple, alpha=0.4, where='post')\n    plt.fill_between(recall,precision,step='post',alpha=0.2,color=purple)\n    plt.title('Recall vs Precision', fontsize=16)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    experiment.log_figure(figure=plt)\n    #plt.savefig('precision_recall_curve_x.png')\n    print(plt.show())\n\n    # Plot precision and recall for different thresholds\n    plt.figure(figsize=(8,6))\n    plt.plot(thresholds, precision[1:], label=\"Precision\",color= orange, linewidth=2.5)\n    plt.plot(thresholds, recall[1:], label=\"Recall\",color= purple, alpha=0.9,linewidth=2.5)\n    plt.title('Precision and recall for different threshold values', fontsize=16)\n    plt.xlabel('Threshold')\n    plt.ylabel('Precision/Recall')\n    plt.legend()\n    experiment.log_figure(figure=plt)\n    #plt.savefig('threshold_values_x.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:23.979689Z","iopub.execute_input":"2023-02-19T14:31:23.981407Z","iopub.status.idle":"2023-02-19T14:31:23.993425Z","shell.execute_reply.started":"2023-02-19T14:31:23.981362Z","shell.execute_reply":"2023-02-19T14:31:23.991632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reconstruction_error_plot(df, ylims = (None, None), title=\"Reconstruction error for different classes\"):  \n    groups = df.groupby('Labels_gt')\n    sns.set(style=\"white\")\n    sns.set_palette(sns.color_palette([orange, purple]))\n    fig, ax = plt.subplots(figsize=(8,6)) \n\n    for name, group in groups:\n        ax.plot(group.index, \n            group.Reconstruction_Loss, \n            marker='o', \n            ms=5, \n            linestyle='',\n            label= \"Fraud\" if name == 1 else \"Nonfraud\")\n    \n    ax.hlines(hyper_params['threshold'], \n            ax.get_xlim()[0], \n            ax.get_xlim()[1], \n            colors=\"r\", \n            zorder=100, \n            label='Threshold',\n            linestyle='--')\n    ax.legend()\n    plt.title(title, fontsize=16)\n    plt.ylabel(\"Reconstruction error\")\n    plt.xlabel(\"Data point index\")\n    plt.ylim(top=ylims[0], bottom=ylims[1])\n    plt.legend(loc='best')\n    experiment.log_figure(figure=plt)\n    #plt.savefig('reconstruction_error_plot_x.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:23.994993Z","iopub.execute_input":"2023-02-19T14:31:23.995318Z","iopub.status.idle":"2023-02-19T14:31:24.019445Z","shell.execute_reply.started":"2023-02-19T14:31:23.995281Z","shell.execute_reply":"2023-02-19T14:31:24.018198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot metrics:","metadata":{}},{"cell_type":"code","source":"plot_precision_recall_curves(df)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:24.021062Z","iopub.execute_input":"2023-02-19T14:31:24.021425Z","iopub.status.idle":"2023-02-19T14:31:25.399463Z","shell.execute_reply.started":"2023-02-19T14:31:24.021392Z","shell.execute_reply":"2023-02-19T14:31:25.39847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstruction_error_plot(df)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:25.400787Z","iopub.execute_input":"2023-02-19T14:31:25.401809Z","iopub.status.idle":"2023-02-19T14:31:26.195511Z","shell.execute_reply.started":"2023-02-19T14:31:25.40177Z","shell.execute_reply":"2023-02-19T14:31:26.194141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(embeds)\ny=df['Labels_gt'].values\ntsne_plot(X, y, name='latent_rep.png')","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:26.196823Z","iopub.execute_input":"2023-02-19T14:31:26.197154Z","iopub.status.idle":"2023-02-19T14:31:34.858575Z","shell.execute_reply.started":"2023-02-19T14:31:26.197124Z","shell.execute_reply":"2023-02-19T14:31:34.857632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the plot of our original dataset t-SNE plot versus that of our autoencoder embeddings:","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"white\")\nplt.figure(figsize=(18,12))\n\nplt.subplot(1, 2, 1)\nplt.imshow(img.imread('original.png'))\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img.imread('latent_rep.png'))\nplt.axis('off')\n\nplt.tight_layout","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:34.859738Z","iopub.execute_input":"2023-02-19T14:31:34.860569Z","iopub.status.idle":"2023-02-19T14:31:35.730945Z","shell.execute_reply.started":"2023-02-19T14:31:34.860529Z","shell.execute_reply":"2023-02-19T14:31:35.729868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, the embeddings learned by our Autoencoder (on the right) are much easier to separate than the raw data (on the left). ","metadata":{}},{"cell_type":"code","source":"print(\"Classification report \\n {0}\".format(classification_report(df.Labels_gt, df.Labels_preds)))","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:35.732229Z","iopub.execute_input":"2023-02-19T14:31:35.732681Z","iopub.status.idle":"2023-02-19T14:31:35.748616Z","shell.execute_reply.started":"2023-02-19T14:31:35.732647Z","shell.execute_reply":"2023-02-19T14:31:35.747165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now our model is ready to be tested on new, unseen data. We‚Äôll log the summary statistics of our test dataset to monitor for any data distribution shifts or disturbances. Later, we'll also be able to reference this metadata to help explain any unusual or unexpected outputs. The code snippet below logs the summary statistics of our test dataset as a csv file called ‚Äúinput_statistics‚Äù:","metadata":{}},{"cell_type":"code","source":"# We take 10 random samples from the unseen test set we defined earlier\ntest_data = test_samples.sample(10).reset_index(drop=True)\n\n# Log summary statistics of test data (only 5 columns, round to 4 decimal places)\nexperiment.log_table('input_statistics.csv', test_data.describe().iloc[:,[0, 1, 2, 29, 30]].round(4))","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:35.750436Z","iopub.execute_input":"2023-02-19T14:31:35.751611Z","iopub.status.idle":"2023-02-19T14:31:35.841675Z","shell.execute_reply.started":"2023-02-19T14:31:35.751539Z","shell.execute_reply":"2023-02-19T14:31:35.840709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we've completed training and inference, we can head over to the Comet UI to check out our Data Panels! Because we're using an interactive environment, first we'll call **`experiment.end()`** to ensure all logged tables are successfully uploaded.","metadata":{}},{"cell_type":"code","source":"experiment.end()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T14:31:35.843257Z","iopub.execute_input":"2023-02-19T14:31:35.844534Z","iopub.status.idle":"2023-02-19T14:31:37.354178Z","shell.execute_reply.started":"2023-02-19T14:31:35.844479Z","shell.execute_reply":"2023-02-19T14:31:37.353198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ‚öô Using Data Panels\n\n____","metadata":{}},{"cell_type":"markdown","source":"To add the Data Panels to your default view in the Comet UI, you can access them directly in the built-in panel options:\n\n[![add_Comet_data_panels.gif](https://s9.gifyu.com/images/add_data_panels.gif)](https://www.comet.com/site/)","metadata":{}},{"cell_type":"markdown","source":"Not detecting fraudulent cases when they are actually occurring could potentially cost banks and other financial institutions a lot of money. Let's take a look at instances where the ground truth label for a sample was fraud, but our model predicted no fraud ([false negatives](https://en.wikipedia.org/wiki/False_positives_and_false_negatives)). \n\n[![CometDataPanels_misclassifying.gif](https://s9.gifyu.com/images/DP_misclassifying.gif)](https://www.comet.com/site/)","metadata":{}},{"cell_type":"markdown","source":"One of the most useful ways to use the Data Panel is to track how prediction columns are changing for the same samples across experiments. Concatenate the tables via columns and see side-by-side how your prediction columns are comparing to previous runs.\n\n[![DataPanels_concatenate_columns.gif](https://s9.gifyu.com/images/DP_concatenate_columns.gif)](https://www.comet.com/site/)","metadata":{}},{"cell_type":"markdown","source":"If you have any feedback, or for help and support with Data Panels, please join the [Comet Community Slack channel](https://cometml.slack.com/join/shared_invite/zt-1fa356mer-2AMqwrzobWAJNx1oo1KSpQ#/shared-invite/email?utm_source=Medium&utm_medium=referral&utm_content=data_panels_blog).\nFor more context around this experiment, check out [the full blog post here](https://medium.com/better-programming/credit-card-fraud-detection-with-autoencoders-4e01b2038a91).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
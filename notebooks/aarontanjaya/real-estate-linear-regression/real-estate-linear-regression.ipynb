{"cells":[{"metadata":{},"cell_type":"markdown","source":"First, we load the data and view it to get a sense of the data. Then we see whether the data is already in a numerical form or not and whether there's any missing value by using pandas' .dtypes property and isnull(). We also store the transaction number in another variable to be used for plotting later."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport pandas as pd\ndata=pd.read_csv(\"/kaggle/input/real-estate-price-prediction/Real estate.csv\")\n\nprint(data.head())\nprint(data.dtypes)\nprint(data.isnull().sum())\nno=data['No']\ndata.drop('No',axis=1,inplace=True)\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there isnt any missing value or undesirable data format among the data so we could proceed to EDA by plotting the columns (features) to get a better understanding of the data. I divided the datapoints' price into 3 category+ low, high and med with med being the datapoints which price lies on +-1 standard deviation from median. The standard that i chose here is pretty arbitrary as other notebooks have shown the data is non-gaussian. I only use this categorization to make visualization easier."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns #seaborn and matplotlib for visualization\nimport matplotlib.pyplot as plt\nmedian=data['Y house price of unit area'].median()\nstd=data['Y house price of unit area'].std()\nhigh=median+std\nlow=median-std\nprint(\"\\nStandard deviation:\",std,\"Median:\",median,\"Low:\",low,\"High:\",high,\"\\n\")\ndata['price']='nan'\ndata.loc[(data['Y house price of unit area']>=high),'price']='High'\ndata.loc[((data['Y house price of unit area']<high)&(data['Y house price of unit area']>low)),'price']='Med'\ndata.loc[(data['Y house price of unit area']<=low),'price']='Low'\nprint(data.head())\nsns.displot(data,x='Y house price of unit area',kind='kde')\nplt.title('price')\nfor item in data.drop(['Y house price of unit area','price'],axis=1).columns.tolist():\n    sns.displot(data,x=item,kind='kde',hue='price')\n    plt.title(item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets plot the real estates based on its latitude and longitude so we could get a sense of the layout and location."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data['X6 longitude'][data['price']=='High'],data['X5 latitude'][data['price']=='High'],c='g',label=\"High\",alpha=0.5)\nplt.scatter(data['X6 longitude'][data['price']=='Med'],data['X5 latitude'][data['price']=='Med'],c='b',label=\"Med\",marker='^',alpha=0.5)\nplt.scatter(data['X6 longitude'][data['price']=='Low'],data['X5 latitude'][data['price']=='Low'],c='r',label=\"Low\",marker='p',alpha=0.5)\nplt.title(\"Location\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the correlation between price and location (latitude and longitude) in this data is pretty linear in the sense that the higher-priced real estates lies in the location with higher longitude and latitude + postivie correlation from the heatmap correlation plot so we could probably still get a pretty accurate result from using the raw latitude and longitude data without needing to add a cluster class, or can we?, let's try that out."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nimport numpy as np\ndb=DBSCAN(eps=0.008)\nclust=db.fit(data[['X6 longitude','X5 latitude']])\nprint(np.unique(clust.labels_))\ndata['cluster']=db.fit_predict(data[['X6 longitude','X5 latitude']])\nsns.scatterplot(data=data,x='X6 longitude',y='X5 latitude',hue='cluster')\nplt.show()\n\ndata['cluster'].replace(-1,3,inplace=True)\nprint(data['cluster'].unique())\nsns.scatterplot(data=data,x='X6 longitude',y='X5 latitude',hue='cluster')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below I normalized the data using Z score normalization (z = (x - u) / s) as Linear Regression is sensitive to its features' scale. I also divided the data into 3 sets: the orginal one, the clustered one (Dropping both longitude and latitude) and a dimensionally reduced one (2 dimensionality reduction using PCA)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\ntarget=data['Y house price of unit area']\ndataxori=data.drop(['cluster','price','Y house price of unit area'],axis=1)\ndataxclust=data.drop(['X6 longitude','X5 latitude','price','Y house price of unit area'],axis=1)\n\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()\ndataxoriss=ss.fit_transform(dataxori)\ndataxclustss=ss.fit_transform(dataxclust)\ndataxPCA=PCA(n_components=4).fit_transform(dataxoriss)#reduce 2 dimesions to 4\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we split the data into train and test sets with test size of 0.3 and random state 5 to compare it."},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrainori,xtestori,ytrainori,ytestori=train_test_split(dataxoriss,target,test_size=0.3,random_state=5)#30% test data\nxtrainclust,xtestclust,ytrainclust,ytestclust=train_test_split(dataxclustss,target,test_size=0.3,random_state=5)\nxtrainpca,xtestpca,ytrainpca,ytestpca=train_test_split(dataxPCA,target,test_size=0.3,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All done, time to build the model. Here I tried 3 models (Linear Regression, Ridge Regression and Polynomial Regression) and 3 scoring functions (R^2, MSE and MAE). All applied on the 3 different datasets that I have prepared before."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\ndef report(ytrue,ypred,title):\n    print(title)\n    print(\"R2\")\n    print(r2_score(ytrue,ypred))\n    print(\"MSE\")\n    print(mean_squared_error(ytrue,ypred))\n    print(\"MAE\")\n    print(mean_absolute_error(ytrue,ypred),'\\n\\n')\n    return ' '\npipe=make_pipeline(PolynomialFeatures(2),LinearRegression())\nlin=LinearRegression()\nlin.fit(xtrainori,ytrainori)\nprint(\"Linear Regression\")\nprint(report(ytrainori,lin.predict(xtrainori),'Original Train'))\nprint(report(ytestori,lin.predict(xtestori),'Original Test'))\nlin.fit(xtrainclust,ytrainclust)\nprint(report(ytrainclust,lin.predict(xtrainclust),'Clustered Train'))\nprint(report(ytestclust,lin.predict(xtestclust),'Clustered Test'))\nlin.fit(xtrainpca,ytrainpca)\nprint(report(ytrainpca,lin.predict(xtrainpca),'PCA Train'))\nprint(report(ytestpca,lin.predict(xtestpca),'PCA Test'))\n\nlin1=Ridge()\nlin1.fit(xtrainori,ytrainori)\nprint(\"Ridge Regression\")\nprint(report(ytrainori,lin1.predict(xtrainori),'Original Train'))\nprint(report(ytestori,lin1.predict(xtestori),'Original Test'))\nlin1.fit(xtrainclust,ytrainclust)\nprint(report(ytrainclust,lin1.predict(xtrainclust),'Clustered Train'))\nprint(report(ytestclust,lin1.predict(xtestclust),'Clustered Test'))\nlin1.fit(xtrainpca,ytrainpca)\nprint(report(ytrainpca,lin1.predict(xtrainpca),'PCA Train'))\nprint(report(ytestpca,lin1.predict(xtestpca),'PCA Test'))\n\nlin2=pipe\nlin2.fit(xtrainori,ytrainori)\nprint(\"Polynomial Regression\")\nprint(report(ytrainori,lin2.predict(xtrainori),'Original Train'))\nprint(report(ytestori,lin2.predict(xtestori),'Original Test'))\nlin2.fit(xtrainclust,ytrainclust)\nprint(report(ytrainclust,lin2.predict(xtrainclust),'Clustered Train'))\nprint(report(ytestclust,lin2.predict(xtestclust),'Clustered Test'))\nlin2.fit(xtrainpca,ytrainpca)\nprint(report(ytrainpca,lin2.predict(xtrainpca),'PCA Train'))\nprint(report(ytestpca,lin2.predict(xtestpca),'PCA Test'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we try to train the model using the test data and then plot the result together with the actual value per datapoint so we could get a picture of how well the models fit the actual unseen values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nlin.fit(xtrainpca,ytrainpca)\nfig=plt.figure(figsize=(20,10))\nax=fig.add_subplot()\nax.plot(np.arange(len(xtestpca)),ytestpca,label='True')\nax.plot(np.arange(len(xtestpca)),lin.predict(xtestpca),c='g',linestyle='--',label='ori')\nplt.title(\"PCA Linear\")\nplt.show()\n\nlin2.fit(xtrainpca,ytrainpca)\nfig1=plt.figure(figsize=(20,10))\nax1=fig1.add_subplot()\nax1.plot(np.arange(len(xtestpca)),ytestpca,label='True')\nax1.plot(np.arange(len(xtestpca)),lin2.predict(xtestpca),c='g',linestyle='--',label='ori')\nplt.title(\"PCA Ridge\")\nplt.show()\n\nlin2.fit(xtrainpca,ytrainpca)\nfig2=plt.figure(figsize=(20,10))\nax2=fig2.add_subplot()\nax2.plot(np.arange(len(xtestpca)),ytestpca,label='True')\nax2.plot(np.arange(len(xtestpca)),lin2.predict(xtestpca),c='g',linestyle='--',label='ori')\nplt.title(\"PCA Poly\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, plotting the the model with full data and its error, from top to bottom, repeating: Linear, Ridge, Polynomial to get a sense of how well the model could fit itself to the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(20,10))\nax=fig.add_subplot()\nlin.fit(dataxoriss,target)\nax.plot(no,target,label='True')\npredori=lin.predict(dataxoriss)\nax.plot(no,predori,c='g',linestyle='--',label='ori')\nlin.fit(dataxclustss,target)\npredclust=lin.predict(dataxclustss)\nax.plot(no,predclust,c='r',linestyle='--',label='clust')\nlin.fit(dataxPCA,target)\npredpca=lin.predict(dataxPCA)\nax.plot(no,predpca,c='y',linestyle='--',label='PCA')\nplt.legend()\nplt.title(\"Hasil\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin1.fit(dataxoriss,target)\nfig=plt.figure(figsize=(20,10))\nax=fig.add_subplot()\nax.plot(no,target,label='True')\npredori1=lin1.predict(dataxoriss)\nax.plot(no,predori1,c='g',linestyle='--',label='ori')\nlin1.fit(dataxclustss,target)\npredclust1=lin1.predict(dataxclustss)\nax.plot(no,predclust1,c='r',linestyle='--',label='clust')\nlin1.fit(dataxPCA,target)\npredpca1=lin1.predict(dataxPCA)\nax.plot(no,predpca1,c='y',linestyle='--',label='PCA')\nplt.legend()\nplt.title(\"Hasil\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin2.fit(dataxoriss,target)\nfig=plt.figure(figsize=(20,10))\nax=fig.add_subplot()\nax.plot(no,target,label='True')\npredori2=lin2.predict(dataxoriss)\nax.plot(no,predori2,c='g',linestyle='--',label='ori')\nlin2.fit(dataxclustss,target)\npredclust2=lin2.predict(dataxclustss)\nax.plot(no,predclust2,c='r',linestyle='--',label='clust')\nlin2.fit(dataxPCA,target)\npredpca2=lin2.predict(dataxPCA)\nax.plot(no,predpca2,c='y',linestyle='--',label='PCA')\nplt.legend()\nplt.title(\"Hasil\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the difference between actual and predicted values, Linear:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=plt.figure(figsize=(20,10))\nax1=fig1.subplots(3)\nax1[0].plot(no,target-predori)\nax1[0].set_title(\"Error ori\")\nax1[1].plot(no,target-predclust)\nax1[1].set_title(\"Error clust\")\nax1[2].plot(no,target-predpca)\nax1[2].set_title(\"Error pca\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ridge:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=plt.figure(figsize=(20,10))\nax1=fig1.subplots(3)\nax1[0].plot(no,target-predori1)\nax1[0].set_title(\"Error ori\")\nax1[1].plot(no,target-predclust1)\nax1[1].set_title(\"Error clust\")\nax1[2].plot(no,target-predpca1)\nax1[2].set_title(\"Error pca\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Polynomial:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=plt.figure(figsize=(20,10))\nax1=fig1.subplots(3)\nax1[0].plot(no,target-predori2)\nax1[0].set_title(\"Error ori\")\nax1[1].plot(no,target-predclust2)\nax1[1].set_title(\"Error clust\")\nax1[2].plot(no,target-predpca2)\nax1[2].set_title(\"Error pca\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing \nNatural Language Processing (NLP) is the study of making computers understand how humans naturally speak, write and communicate.\n\nI will be using NLTK (Natural Language Toolkit) for doing natural language processing in English Language. The NLTK is a a collection of python libraries designed specially for identifying and tag parts of speech found in text of natural language like English.","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:11:02.454324Z","iopub.execute_input":"2021-12-20T13:11:02.454601Z","iopub.status.idle":"2021-12-20T13:11:03.155061Z","shell.execute_reply.started":"2021-12-20T13:11:02.454574Z","shell.execute_reply":"2021-12-20T13:11:03.15434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/gmail-spam-detection-dataset/spam1.csv', encoding = 'windows-1252')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:14:40.130228Z","iopub.execute_input":"2021-12-20T13:14:40.130985Z","iopub.status.idle":"2021-12-20T13:14:40.167915Z","shell.execute_reply.started":"2021-12-20T13:14:40.130935Z","shell.execute_reply":"2021-12-20T13:14:40.166845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:14:42.345331Z","iopub.execute_input":"2021-12-20T13:14:42.345615Z","iopub.status.idle":"2021-12-20T13:14:42.359836Z","shell.execute_reply.started":"2021-12-20T13:14:42.345586Z","shell.execute_reply":"2021-12-20T13:14:42.358992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding one more column with the name spam.\n# Here if a mail is spam it will print 1 else 0.\ndf['spam'] = df['type'].map({'spam': 1, 'ham': 0}).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:14:52.165283Z","iopub.execute_input":"2021-12-20T13:14:52.165563Z","iopub.status.idle":"2021-12-20T13:14:52.17298Z","shell.execute_reply.started":"2021-12-20T13:14:52.165535Z","shell.execute_reply":"2021-12-20T13:14:52.171756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:14:52.544542Z","iopub.execute_input":"2021-12-20T13:14:52.544827Z","iopub.status.idle":"2021-12-20T13:14:52.554601Z","shell.execute_reply.started":"2021-12-20T13:14:52.54479Z","shell.execute_reply":"2021-12-20T13:14:52.553786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:14:56.00235Z","iopub.execute_input":"2021-12-20T13:14:56.002653Z","iopub.status.idle":"2021-12-20T13:14:56.009057Z","shell.execute_reply.started":"2021-12-20T13:14:56.00262Z","shell.execute_reply":"2021-12-20T13:14:56.008201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['spam'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:14:57.083141Z","iopub.execute_input":"2021-12-20T13:14:57.083482Z","iopub.status.idle":"2021-12-20T13:14:57.091709Z","shell.execute_reply.started":"2021-12-20T13:14:57.083445Z","shell.execute_reply":"2021-12-20T13:14:57.090646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:14:58.699601Z","iopub.execute_input":"2021-12-20T13:14:58.699893Z","iopub.status.idle":"2021-12-20T13:14:58.714802Z","shell.execute_reply.started":"2021-12-20T13:14:58.699864Z","shell.execute_reply":"2021-12-20T13:14:58.713858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:15:03.021082Z","iopub.execute_input":"2021-12-20T13:15:03.021915Z","iopub.status.idle":"2021-12-20T13:15:03.030741Z","shell.execute_reply.started":"2021-12-20T13:15:03.021868Z","shell.execute_reply":"2021-12-20T13:15:03.030127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our dataset contains 5572 rows, in which 4825 are ham and 747 mails are spam mails.\nMoreover, our dataset does not contain any null or 0 values.","metadata":{}},{"cell_type":"markdown","source":"# Tokenization \nTokenization stands for splitting up of data into tokens, that is comma seperated values.","metadata":{}},{"cell_type":"code","source":"df['text'][1]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:16:01.256479Z","iopub.execute_input":"2021-12-20T13:16:01.256816Z","iopub.status.idle":"2021-12-20T13:16:01.263603Z","shell.execute_reply.started":"2021-12-20T13:16:01.256779Z","shell.execute_reply":"2021-12-20T13:16:01.262636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizer(text):\n    return text.split()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:16:05.095785Z","iopub.execute_input":"2021-12-20T13:16:05.096219Z","iopub.status.idle":"2021-12-20T13:16:05.100902Z","shell.execute_reply.started":"2021-12-20T13:16:05.096179Z","shell.execute_reply":"2021-12-20T13:16:05.100157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:16:09.533589Z","iopub.execute_input":"2021-12-20T13:16:09.534087Z","iopub.status.idle":"2021-12-20T13:16:09.553638Z","shell.execute_reply.started":"2021-12-20T13:16:09.534052Z","shell.execute_reply":"2021-12-20T13:16:09.552764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'][1]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:16:11.319494Z","iopub.execute_input":"2021-12-20T13:16:11.319776Z","iopub.status.idle":"2021-12-20T13:16:11.326192Z","shell.execute_reply.started":"2021-12-20T13:16:11.319748Z","shell.execute_reply":"2021-12-20T13:16:11.325577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stemming\nStemming is the process of removing of suffix to convert the word into core values. For example, converting waits, waiting, waited to the core word wait.\n\nThere are different stemmers in the package such as snowball, porter, lancaster, etc. I will be using Snowball.","metadata":{}},{"cell_type":"code","source":"porter = SnowballStemmer(\"english\", ignore_stopwords=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:18:37.197918Z","iopub.execute_input":"2021-12-20T13:18:37.198386Z","iopub.status.idle":"2021-12-20T13:18:37.202493Z","shell.execute_reply.started":"2021-12-20T13:18:37.198354Z","shell.execute_reply":"2021-12-20T13:18:37.201574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stem_it(text):\n    return [porter.stem(word) for word in text]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:18:39.577955Z","iopub.execute_input":"2021-12-20T13:18:39.578574Z","iopub.status.idle":"2021-12-20T13:18:39.582512Z","shell.execute_reply.started":"2021-12-20T13:18:39.578538Z","shell.execute_reply":"2021-12-20T13:18:39.581787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(stem_it)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:18:53.414705Z","iopub.execute_input":"2021-12-20T13:18:53.415429Z","iopub.status.idle":"2021-12-20T13:18:54.655135Z","shell.execute_reply.started":"2021-12-20T13:18:53.415392Z","shell.execute_reply":"2021-12-20T13:18:54.654293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'][1]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:18:55.968882Z","iopub.execute_input":"2021-12-20T13:18:55.969159Z","iopub.status.idle":"2021-12-20T13:18:55.975895Z","shell.execute_reply.started":"2021-12-20T13:18:55.96913Z","shell.execute_reply":"2021-12-20T13:18:55.974701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmitization\nIt is the process of finding lemma of a word depending on their meaning. It aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as lemma. For example, converting is, am, was, are to the lemma word be. \n\nDifference between Stemming and Lemmitization is that stemming can often create non-existent words, whereas lemmas are actual words. ","metadata":{}},{"cell_type":"code","source":"df['text'][153]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:21:54.401231Z","iopub.execute_input":"2021-12-20T13:21:54.401618Z","iopub.status.idle":"2021-12-20T13:21:54.409557Z","shell.execute_reply.started":"2021-12-20T13:21:54.401585Z","shell.execute_reply":"2021-12-20T13:21:54.408561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmitizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:19:24.837812Z","iopub.execute_input":"2021-12-20T13:19:24.838078Z","iopub.status.idle":"2021-12-20T13:19:24.841946Z","shell.execute_reply.started":"2021-12-20T13:19:24.838049Z","shell.execute_reply":"2021-12-20T13:19:24.8411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmit_it(text):\n    return [lemmitizer.lemmatize(word, pos = 'a') for word in text]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:19:55.761385Z","iopub.execute_input":"2021-12-20T13:19:55.761668Z","iopub.status.idle":"2021-12-20T13:19:55.766096Z","shell.execute_reply.started":"2021-12-20T13:19:55.761641Z","shell.execute_reply":"2021-12-20T13:19:55.765333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lemmit_it)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:19:57.719031Z","iopub.execute_input":"2021-12-20T13:19:57.719363Z","iopub.status.idle":"2021-12-20T13:20:00.356751Z","shell.execute_reply.started":"2021-12-20T13:19:57.719326Z","shell.execute_reply":"2021-12-20T13:20:00.355879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'][153]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:10.30165Z","iopub.execute_input":"2021-12-20T13:22:10.301943Z","iopub.status.idle":"2021-12-20T13:22:10.309398Z","shell.execute_reply.started":"2021-12-20T13:22:10.301913Z","shell.execute_reply":"2021-12-20T13:22:10.308323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StopWord Remmoval\nIt is used to remove common words such as is, an, the, etc. The search engine is programmed to ignore such words.","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:15.204356Z","iopub.execute_input":"2021-12-20T13:22:15.204768Z","iopub.status.idle":"2021-12-20T13:22:15.209447Z","shell.execute_reply.started":"2021-12-20T13:22:15.204739Z","shell.execute_reply":"2021-12-20T13:22:15.20855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stop_it(text):\n    review = [word for word in text if not word in stop_words]\n    return review","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:19.21466Z","iopub.execute_input":"2021-12-20T13:22:19.214952Z","iopub.status.idle":"2021-12-20T13:22:19.2195Z","shell.execute_reply.started":"2021-12-20T13:22:19.21492Z","shell.execute_reply":"2021-12-20T13:22:19.218555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(stop_it)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:22.315277Z","iopub.execute_input":"2021-12-20T13:22:22.315713Z","iopub.status.idle":"2021-12-20T13:22:22.488956Z","shell.execute_reply.started":"2021-12-20T13:22:22.31568Z","shell.execute_reply":"2021-12-20T13:22:22.487977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:28.215631Z","iopub.execute_input":"2021-12-20T13:22:28.215912Z","iopub.status.idle":"2021-12-20T13:22:28.229424Z","shell.execute_reply.started":"2021-12-20T13:22:28.215883Z","shell.execute_reply":"2021-12-20T13:22:28.228339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(' '.join)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:30.060073Z","iopub.execute_input":"2021-12-20T13:22:30.060729Z","iopub.status.idle":"2021-12-20T13:22:30.07213Z","shell.execute_reply.started":"2021-12-20T13:22:30.06069Z","shell.execute_reply":"2021-12-20T13:22:30.071444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:33.782372Z","iopub.execute_input":"2021-12-20T13:22:33.782808Z","iopub.status.idle":"2021-12-20T13:22:33.792171Z","shell.execute_reply.started":"2021-12-20T13:22:33.782777Z","shell.execute_reply":"2021-12-20T13:22:33.791558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization\nIt is the method to convert textual data into numeric format. Since computers are unable to understand textual data, hence we need to convert text into numerical format.\n\nI will be using TfidfVectorizer for the same, that is Term Frequency-Inverse Document Frequency.","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer()\ny = df.spam.values\nx = tfidf.fit_transform(df['text'])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:36.816013Z","iopub.execute_input":"2021-12-20T13:22:36.81686Z","iopub.status.idle":"2021-12-20T13:22:36.944645Z","shell.execute_reply.started":"2021-12-20T13:22:36.816821Z","shell.execute_reply":"2021-12-20T13:22:36.94345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 0, test_size = 0.2, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:39.99233Z","iopub.execute_input":"2021-12-20T13:22:39.992602Z","iopub.status.idle":"2021-12-20T13:22:39.998879Z","shell.execute_reply.started":"2021-12-20T13:22:39.992575Z","shell.execute_reply":"2021-12-20T13:22:39.998199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:42.076746Z","iopub.execute_input":"2021-12-20T13:22:42.077667Z","iopub.status.idle":"2021-12-20T13:22:42.088209Z","shell.execute_reply.started":"2021-12-20T13:22:42.077616Z","shell.execute_reply":"2021-12-20T13:22:42.087401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(x_train, y_train)\ny_pred  = lr.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:44.600319Z","iopub.execute_input":"2021-12-20T13:22:44.600622Z","iopub.status.idle":"2021-12-20T13:22:44.679939Z","shell.execute_reply.started":"2021-12-20T13:22:44.600589Z","shell.execute_reply":"2021-12-20T13:22:44.678826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_log = accuracy_score(y_pred, y_test)*100\nprint(\"Accuracy\", acc_log)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:50.365899Z","iopub.execute_input":"2021-12-20T13:22:50.366205Z","iopub.status.idle":"2021-12-20T13:22:50.373891Z","shell.execute_reply.started":"2021-12-20T13:22:50.366171Z","shell.execute_reply":"2021-12-20T13:22:50.372969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LinearSVC Accuracy","metadata":{}},{"cell_type":"code","source":"svc = LinearSVC(random_state=0)\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:54.185147Z","iopub.execute_input":"2021-12-20T13:22:54.185927Z","iopub.status.idle":"2021-12-20T13:22:54.201331Z","shell.execute_reply.started":"2021-12-20T13:22:54.185891Z","shell.execute_reply":"2021-12-20T13:22:54.200445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_svc = accuracy_score(y_pred, y_test)*100\nprint(\"Accuracy\", acc_svc)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:22:57.511918Z","iopub.execute_input":"2021-12-20T13:22:57.512462Z","iopub.status.idle":"2021-12-20T13:22:57.519109Z","shell.execute_reply.started":"2021-12-20T13:22:57.512425Z","shell.execute_reply":"2021-12-20T13:22:57.51807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictive Model\nSince, the accuracy of LinearSVC is slightly better than Logistic Regression, I will be using LinearSVC to make the predictive model.","metadata":{}},{"cell_type":"code","source":"#input_mail = input(\"Enter the mail text: \")\ninput_mail = 'Your free ringtone is waiting to be collected. Simply text the password \\MIX\\\" to 85069 to verify. Get Usher and Britney. FML'\ninput_mail = [input_mail]\ntransformed_data = tfidf.transform(input_mail)\n\nprediction = svc.predict(transformed_data)\n\nif (prediction == 1):\n    print(\"\\nSpam mail\")\nelse:\n    print(\"\\nHam mail\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T13:25:52.377046Z","iopub.execute_input":"2021-12-20T13:25:52.377735Z","iopub.status.idle":"2021-12-20T13:25:52.386709Z","shell.execute_reply.started":"2021-12-20T13:25:52.377692Z","shell.execute_reply":"2021-12-20T13:25:52.385644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Word Embedding**\n- A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n- It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary\n- These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values. Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n- The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. The position of a word in the learned vector space is referred to as its embedding. Two popular examples of methods of learning word embeddings from text include:\n    - **Word2Vec.**\n    - **GloVe.**\n","metadata":{}},{"cell_type":"markdown","source":"**In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.**","metadata":{}},{"cell_type":"markdown","source":"# **Keras Embedding Layer**\n- Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer.\n- The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:17.264767Z","iopub.execute_input":"2023-10-16T18:44:17.265179Z","iopub.status.idle":"2023-10-16T18:44:21.354768Z","shell.execute_reply.started":"2023-10-16T18:44:17.26514Z","shell.execute_reply":"2023-10-16T18:44:21.353391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews = ['nice food', \n           'amazing restaurant', \n           'too good', \n           'just loved it!',\n           'will go again', \n           'horrible food', \n           'never go there',\n           'poor service',\n           'poor quality',\n           'needs improvement']\n\nsentiment = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:21.363761Z","iopub.execute_input":"2023-10-16T18:44:21.364131Z","iopub.status.idle":"2023-10-16T18:44:21.370064Z","shell.execute_reply.started":"2023-10-16T18:44:21.364067Z","shell.execute_reply":"2023-10-16T18:44:21.368445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Keras provides the one_hot() function that creates a hash of each word as an efficient integer encoding.** \n- **We will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of collisions from the hash function.**","metadata":{}},{"cell_type":"code","source":"# Apply on a sample\none_hot('amazing restaurant', 50)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:21.372269Z","iopub.execute_input":"2023-10-16T18:44:21.37275Z","iopub.status.idle":"2023-10-16T18:44:21.385452Z","shell.execute_reply.started":"2023-10-16T18:44:21.372705Z","shell.execute_reply":"2023-10-16T18:44:21.383857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 50\nencoded_docs = [one_hot(r, vocab_size) for r in reviews]\nencoded_docs","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:21.389767Z","iopub.execute_input":"2023-10-16T18:44:21.390227Z","iopub.status.idle":"2023-10-16T18:44:21.398453Z","shell.execute_reply.started":"2023-10-16T18:44:21.390184Z","shell.execute_reply":"2023-10-16T18:44:21.397551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length.**\n- **We will pad all input sequences to have the length of 3 words**","metadata":{}},{"cell_type":"code","source":"# pad documents to a max length of 3 words\nmax_length = 3\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\npadded_docs","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:21.400014Z","iopub.execute_input":"2023-10-16T18:44:21.400458Z","iopub.status.idle":"2023-10-16T18:44:21.411859Z","shell.execute_reply.started":"2023-10-16T18:44:21.40042Z","shell.execute_reply":"2023-10-16T18:44:21.410821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **We are now ready to define our Embedding layer as part of our neural network model.**\n- **The Embedding has a vocabulary of 50 and an input length of 3. We will choose a small embedding space of 8 dimensions(features).**\n- **The model is a simple binary classification model. Importantly, the output from the Embedding layer will be 3 vectors of 8 dimensions each, one for each word. We flatten this to a one 24-element vector to pass on to the Dense output layer**\n","metadata":{}},{"cell_type":"code","source":"embed_vector_size = 8\n\n# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_vector_size, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:21.413722Z","iopub.execute_input":"2023-10-16T18:44:21.414886Z","iopub.status.idle":"2023-10-16T18:44:21.552383Z","shell.execute_reply.started":"2023-10-16T18:44:21.414797Z","shell.execute_reply":"2023-10-16T18:44:21.551186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize the model\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:21.553893Z","iopub.execute_input":"2023-10-16T18:44:21.554284Z","iopub.status.idle":"2023-10-16T18:44:21.57433Z","shell.execute_reply.started":"2023-10-16T18:44:21.554256Z","shell.execute_reply":"2023-10-16T18:44:21.572943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the model\nmodel.fit(padded_docs, sentiment, epochs=50, verbose=1)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-16T18:44:21.576313Z","iopub.execute_input":"2023-10-16T18:44:21.576732Z","iopub.status.idle":"2023-10-16T18:44:22.782593Z","shell.execute_reply.started":"2023-10-16T18:44:21.576694Z","shell.execute_reply":"2023-10-16T18:44:22.781494Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, sentiment, verbose=0)\nprint('Accuracy: %0.2f' % (accuracy*100))","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:22.784925Z","iopub.execute_input":"2023-10-16T18:44:22.785849Z","iopub.status.idle":"2023-10-16T18:44:22.997989Z","shell.execute_reply.started":"2023-10-16T18:44:22.785815Z","shell.execute_reply":"2023-10-16T18:44:22.996758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# **Word2vec**\n- **We will use a NLP module [gensim].**","metadata":{}},{"cell_type":"code","source":"import gensim\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:22.999773Z","iopub.execute_input":"2023-10-16T18:44:23.000256Z","iopub.status.idle":"2023-10-16T18:44:37.675648Z","shell.execute_reply.started":"2023-10-16T18:44:23.000202Z","shell.execute_reply":"2023-10-16T18:44:37.674227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas.**","metadata":{}},{"cell_type":"code","source":"df = pd.read_json(\"/kaggle/input/amazon-reviews/Cell_Phones_and_Accessories_5.json\", lines=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:37.677193Z","iopub.execute_input":"2023-10-16T18:44:37.678467Z","iopub.status.idle":"2023-10-16T18:44:41.848717Z","shell.execute_reply.started":"2023-10-16T18:44:37.678429Z","shell.execute_reply":"2023-10-16T18:44:41.847291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:41.850032Z","iopub.execute_input":"2023-10-16T18:44:41.850367Z","iopub.status.idle":"2023-10-16T18:44:41.856421Z","shell.execute_reply.started":"2023-10-16T18:44:41.850342Z","shell.execute_reply":"2023-10-16T18:44:41.855391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Simple Preprocessing & Tokenization**\n- The first thing to do for any data science task is to clean the data.\n- For NLP, we apply various processing like converting all the words to lower case, trimming spaces, removing punctuations. This is something we will do over here too.\n\n- Additionally, we can also remove stop words like 'and', 'or', 'is', 'the', 'a', 'an' and convert words to their root forms like 'running' to 'run'.","metadata":{}},{"cell_type":"code","source":"# We will use 'simple_preprocess' utils function from gensim\nreview_text = df['reviewText'].apply(gensim.utils.simple_preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:44:41.860761Z","iopub.execute_input":"2023-10-16T18:44:41.861923Z","iopub.status.idle":"2023-10-16T18:45:02.937721Z","shell.execute_reply.started":"2023-10-16T18:44:41.861879Z","shell.execute_reply":"2023-10-16T18:45:02.936411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review_text","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:45:02.939005Z","iopub.execute_input":"2023-10-16T18:45:02.939339Z","iopub.status.idle":"2023-10-16T18:45:02.950406Z","shell.execute_reply.started":"2023-10-16T18:45:02.939315Z","shell.execute_reply":"2023-10-16T18:45:02.949184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessed text\nreview_text.loc[0]","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-10-16T18:45:02.951944Z","iopub.execute_input":"2023-10-16T18:45:02.952263Z","iopub.status.idle":"2023-10-16T18:45:02.965251Z","shell.execute_reply.started":"2023-10-16T18:45:02.952237Z","shell.execute_reply":"2023-10-16T18:45:02.963799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original Text\ndf.reviewText.loc[0]","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:45:02.966801Z","iopub.execute_input":"2023-10-16T18:45:02.96773Z","iopub.status.idle":"2023-10-16T18:45:02.978898Z","shell.execute_reply.started":"2023-10-16T18:45:02.967689Z","shell.execute_reply":"2023-10-16T18:45:02.977501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training the Word2Vec Model**\n- **Train the model for reviews. Use a window of size 10 i.e. 10 words before the present word and 10 words ahead. A sentence with at least 2 words should only be considered, configure this using *min_count* parameter.**","metadata":{}},{"cell_type":"markdown","source":"#### **Create Word2Vec model**","metadata":{}},{"cell_type":"code","source":"model = gensim.models.Word2Vec(window=10, min_count=2, workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:45:02.980109Z","iopub.execute_input":"2023-10-16T18:45:02.980519Z","iopub.status.idle":"2023-10-16T18:45:02.992671Z","shell.execute_reply.started":"2023-10-16T18:45:02.980488Z","shell.execute_reply":"2023-10-16T18:45:02.991602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Build Vocabulary**","metadata":{}},{"cell_type":"code","source":"model.build_vocab(review_text, progress_per=1000)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:45:02.994314Z","iopub.execute_input":"2023-10-16T18:45:02.995478Z","iopub.status.idle":"2023-10-16T18:45:07.127832Z","shell.execute_reply.started":"2023-10-16T18:45:02.995438Z","shell.execute_reply":"2023-10-16T18:45:07.126419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Train Model** ","metadata":{}},{"cell_type":"code","source":"model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:45:07.129374Z","iopub.execute_input":"2023-10-16T18:45:07.129787Z","iopub.status.idle":"2023-10-16T18:46:29.260441Z","shell.execute_reply.started":"2023-10-16T18:45:07.129749Z","shell.execute_reply":"2023-10-16T18:46:29.259172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Find similar words**","metadata":{}},{"cell_type":"code","source":"model.wv.most_similar(\"bad\")","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:46:29.262164Z","iopub.execute_input":"2023-10-16T18:46:29.262627Z","iopub.status.idle":"2023-10-16T18:46:29.290273Z","shell.execute_reply.started":"2023-10-16T18:46:29.262586Z","shell.execute_reply":"2023-10-16T18:46:29.288869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get similarity between two words\nmodel.wv.similarity(w1=\"great\", w2=\"good\")","metadata":{"execution":{"iopub.status.busy":"2023-10-16T18:46:29.292082Z","iopub.execute_input":"2023-10-16T18:46:29.292713Z","iopub.status.idle":"2023-10-16T18:46:29.303323Z","shell.execute_reply.started":"2023-10-16T18:46:29.292674Z","shell.execute_reply":"2023-10-16T18:46:29.301833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"### Further Reading\n\nYou can read about gensim more at https://radimrehurek.com/gensim/models/word2vec.html","metadata":{}}]}
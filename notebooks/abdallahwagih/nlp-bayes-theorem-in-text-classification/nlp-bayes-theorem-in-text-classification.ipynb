{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bayes' theorem\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Probability</p>\n\n<center>\n<img src='https://i.postimg.cc/GtpZV52k/dice.webp' width=400>\n</center>\n<br>\n\nA **probability** is a number between 0 and 1 (inclusive) that represents **how likely** an event is going to happen. The closer to 1 the more likely you believe that event will occur. For example, the probability of rolling a 6 on a fair die is 1/6 because it is 1 of 6 equally likely events. \n\nIf you roll two fair dice, the probability of getting two 6's in a row is 1/6 * 1/6 = 1/36. We can multiply the probability because these events are **independent**. That is, the number that appears on the first die has **no affect whatsoever** on the number that appears on the second die.\n\nIn general, for two **independent events** $A$ and $B$, we have the following\n\n<br>\n$$\n\\large\n\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\tag{1}\n$$\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Conditional probability </p>\n\nTo see where equation (1) comes from, we need to talk about **conditional probability**. This is the probability of an event in the case of having **prior information** on that event. It is defined as \n\n<br>\n$$\n\\large\n\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\tag{2}\n$$\n<br>\n\nFor example, if we take a card from a shuffled deck at **random** and we are told it is a **spade** (event B), then how likely is it to be the **ace of spades** (event A)? \n\nAnswer: We know that $\\mathbb{P}(A) = 1/52$ and $\\mathbb{P}(B) = 1/4$, therefore $\\mathbb{P}(A | B) = 1/52 \\div 1/4 = 1/13$. \n\nRe-arranging equation (2) we get \n\n<br>\n$$\n\\large\n\\mathbb{P}(A \\cap B) = \\mathbb{P}(A | B) \\mathbb{P}(B) \\tag{3}\n$$\n<br>\n<br>\n\nAnd when $A$ and $B$ are independent, we have $\\mathbb{P}(A | B) = \\mathbb{P}(A)$ (because $B$ has no affect on $A$) so we get back the independence equation we saw earlier, i.e. (1). \n\nNote that when two events are not independent, we call them **dependent events**. Be aware that sometimes equation (3) is called the **multiplication rule**. \n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Deriving Bayes' theorem </p>\n\nDeriving Bayes' theorem is surprisingly easy. We start with equation (3), $\\mathbb{P}(A \\cap B) = \\mathbb{P}(A | B) \\mathbb{P}(B)$. If we swap event $A$ with event $B$ and vice versa, we get $\\mathbb{P}(B \\cap A) = \\mathbb{P}(B | A) \\mathbb{P}(A)$. And since probability is **commutative**, i.e. $\\mathbb{P}(A \\cap B) = \\mathbb{P}(B \\cap A)$, we can set the two equations equal to each other. That is,\n\n<br>\n$$\n\\large\n\\mathbb{P}(A | B) \\mathbb{P}(B) = \\mathbb{P}(B | A) \\mathbb{P}(A) \\tag{4}\n$$\n<br>\n\nDividing by $\\mathbb{P}(B)$ (assuming it is non-zero), we arrive at **Bayes' theorem**\n\n<br>\n$$\n\\large\n\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(B | A) \\mathbb{P}(A)}{\\mathbb{P}(B)} \\tag{5}\n$$\n<br>\n\nThis equation is the **fundamental theorem** of **Bayesian statistics** and in fact each term in this equation gets its own name because it is so important. \n\n* $\\mathbb{P}(B | A)$ is called the **likelihood**,\n* $\\mathbb{P}(A)$ is called the **prior**,\n* $\\mathbb{P}(B)$ is called the **evidence**,\n* $\\mathbb{P}(A | B)$ is called the **posterior**. \n\nIt is immensively useful because simply put, it allows us to **update our beliefs when new data becomes available**. \n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Example </p>\n\n<center>\n<img src='https://i.postimg.cc/9QZBH1yq/covid.jpg' width=600>\n</center>\n<br>\n    \nLet's say you took a **covid test** and it came back **positive**. You want to know what is the probability that you actually have covid. \n\nTo work this out, we'll let $A$ be the event you that have **covid**, and $B$ be the event that a covid test is **positive**. We are trying to find $\\mathbb{P}(A | B)$, i.e. given you have a positive test, how likely is it that you have covid as well. \n\nFrom experiments done by scientists, we are told that the probability of a **random** covid test being positive is 15%, i.e. $\\mathbb{P}(B)=0.15$.\n\nFurthermore, the **recall** or **sensitivity** of the tests is around 80%. That means, if you actually have covid then the probability of returning a positive test is 80%, so $\\mathbb{P}(B | A) = 0.8$. \n\nFinally, the probability of a **random** person having covid at the moment is estimated to be 1/50 or 2%. This means $\\mathbb{P}(B)=0.02$. \n\nUsing Bayes' theorem, we calculate that $\\mathbb{P}(A | B) = 0.8 \\times 0.02 \\div 0.15 = 0.107$. We have shown that if you get a positive covid test, the probability of actually having covid is only **about 10%**. This statistic is called the **precision** of the test - out of all the positives, how many of them were true positives. \n\nThis surprising results comes from the fact that the distributions of covid cases and covid tests are **highly imbalanced**. Even though the tests have a **high sensitivity**, because most people don't actually have covid the tests will pick up a lot of false positives, i.e. the test has a **low precision**. This isn't a huge problem though as false negatives are more important to avoid than false positives due to the contagious nature of the virus. ","metadata":{"papermill":{"duration":0.007972,"end_time":"2023-02-20T20:35:11.656698","exception":false,"start_time":"2023-02-20T20:35:11.648726","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Text classification\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Conditioning on a word</p>\n\n<center>\n<img src='https://i.postimg.cc/tJxmKZPz/sentiment.jpg' width=500>\n</center>\n<br>\n\nSuppose we want to **classify** a piece of text as being either **positive or negative sentiment**. If the word **\"amazing\"** appears, what is the probability that that document is **positive**? Let's use Bayes' theorem. \n\n<br>\n$$\n\\large\n\\mathbb{P}(\\text{positive} | \\text{\"amazing\"}) = \\frac{\\mathbb{P}(\\text{\"amazing\"} | \\text{positive}) \\mathbb{P}(\\text{positive})}{\\mathbb{P}(\\text{\"amazing\"})} \\tag{6}\n$$\n<br>\n\nWe can work out the **3 terms** on right side of the equation as follows:\n\n* $\\mathbb{P}(\\text{positive})$ - count how many documents are positive and divide by the total number of documents,\n* $\\mathbb{P}(\\text{\"amazing\"})$ - count how many times the word \"amazing\" appears in the corpus and divide by the total number of words,\n* $\\mathbb{P}(\\text{\"amazing\"} | \\text{positive})$ - out of all the positive documents, count how many times the word \"amazing\" appears and divide by the number of words in the positive documents.\n\nSo we converted the question from inferring the sentiment into a matter of working out the **proportions** a word appears in different documents. And this is something computers can do **very quickly**. But what do we do if we want to condition on a whole document and not just a single word? Let's see below.\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Conditioning on a document</p>\n\nThis time we want to find the most likely class when we **condition on an entire document** of text. We can write this task as the following optimization problem \n\n<br>\n$$\n\\large\n\\text{argmax}_{c \\in C} \\mathbb{P}(c|d) = \\text{argmax}_{c \\in C} \\frac{\\mathbb{P}(d|c)\\mathbb{P}(c)}{\\mathbb{P}(d)} \\tag{7}\n$$\n<br>\n\nwhere $c$ and $d$ denote a class and document respectively. Since we are maximizing over all the classes, we can **remove the normalizing constant** that only depends on the document. This is really useful because it means we don't have to create a model for how likely each document appears in a corpus. This reduces our problem to\n\n<br>\n$$\n\\large\n\\hat{c} = \\text{argmax}_{c \\in C} \\mathbb{P}(c|d) = \\text{argmax}_{c \\in C} \\mathbb{P}(d|c)\\mathbb{P}(c) \\tag{8}\n$$\n<br>\n\nWorking out the **probability of each class** $\\mathbb{P}(c)$ is simple - we just count many documents of that class there are and divide by the total number of documents. The question now is, how do we work out the **likelihood** $\\mathbb{P}(d|c)$. Remember that the document is just a sequence of tokens, so we can also write this term as $\\mathbb{P}([w_1,w_2,...,w_n]|c)$.\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Naive Bayes</p>\n\nTo work out $\\mathbb{P}([w_1,w_2,...,w_n]|c)$, we make **two assumptions** on the data. First, just like in a bag-of-word representation, we assume **word order doesn't matter**. Second, we assume that each token appears **independently** of all other tokens. \n\nThis second assumption is where the term **\"naive\"** in Naive Bayes comes from. It is called naive because the assumption obviously **isn't true** in all languages. For example, if someone says the word \"Merry\" then proceeding word is much more likely to be \"Christmas\" than \"cabbage\". That is, \"Merry\" and \"Christmas\" are **not independent** words - they are in fact **correlated**. That being said, the naive assumption models the data sufficiently well.\n\nUnder these assumptions, we can express the likelihood as a **product** of conditional probabilities. In particular, $\\mathbb{P}([w_1,w_2,...,w_n]|c) = \\mathbb{P}(w_1|c) \\mathbb{P}(w_2|c) ... \\mathbb{P}(w_{n-1}|c) \\mathbb{P}(w_n|c)$. And these are quantities we **can calculate** in the same way as we were doing before - for $i=1,...,n$ count the number of times word $w_i$ appears in documents with class $c$ and divide by the total number of words in the corresponding documents. We can therefore update our model to be\n\n<br>\n$$\n\\large\n\\hat{c} = \\text{argmax}_{c \\in C} \\mathbb{P}(c) \\mathbb{P}(w_1|c) \\mathbb{P}(w_2|c) ... \\mathbb{P}(w_{n-1}|c) \\mathbb{P}(w_n|c) \\tag{9}\n$$\n<br>\n\nFor very large corpuses, these probabilities will likely be **very small**. And multiplying lots of small number together can produce numbers too small for our computes to keep track of. To overcome this, it is standard to simply **apply the logarithm** to the above expression. This doesn't change the model, but it does **improve numerical stability**. Using properties of the logarithm, the **final Naive Bayes model** can be written as\n\n<br>\n$$\n\\large\n\\hat{c} = \\text{argmax}_{c \\in C} \\log \\mathbb{P}(c) + \\sum_{i=1}^{n} \\log \\mathbb{P}(w_i|c) \\tag{10}\n$$\n<br>\n\nNaive Bayes is a **very fast** algorithm because it only requires **one forward pass** of the dataset. We will later see deep learning approaches that perform multiple passes - these will be more accurate but slower.\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Practical details</p>\n\nIn practice, there are a few **computational tricks** that are used to **improve the performance** of a Naive Bayes classifier. First, many words just don't appear in a classes set of documents so we could end up with resulting probabilities equal to 0. This is a problem because we need to take the logarithm of these probabilities and the **log of 0 is undefined**. There are many ways to deal with this (see [A Comparison of Smoothing Techniques for Bilingual Lexicon Extraction\nfrom Comparable Corpora](https://aclanthology.org/W13-2504.pdf)). The most popular one is called **additive smoothing** where the probabilities $\\frac{\\text{count}(w_i) \\, \\text{in c}}{\\text{count}(w) \\, \\text{in c}}$ are modified using a **smoothing parameter** $\\alpha \\in (0,1]$ to become\n\n$$\n\\large\np_i = \\frac{\\text{count}(w_i) \\, \\text{in c} + \\alpha}{\\text{count}(w) \\, \\text{in c} + \\alpha |V|}\n$$\n\nwhere $|V|$ is the size of the vocabulary. When $\\alpha=1$, this is called **Laplace smoothing** and when $\\alpha<1$, this is called **Lidstone smoothing**. This hyperparameter can be tuned to the specific application. The result is that the probabilities are **always positive** which ensure **stability** in our solution. \n\n<center>\n<img src='https://i.postimg.cc/cJdFpTNL/softmax.png' width=250>\n</center>\n<br>\n\nAnother useful trick to be aware about is that to return the **probabilities** associated with our predictions, i.e. how confident we are with assigning a particular class, we can apply the **softmax** function to the output of our computations across all the classes. This makes sense since the model outputs are log transformed and the softmax function applies the exponential function to all outputs and then normalizes. However, keep in mind that this is only a **rough estimate** of the probabilities because the smoothing will have introduce some error into the calculations to ensure stability. ","metadata":{"papermill":{"duration":0.00768,"end_time":"2023-02-20T20:35:11.672592","exception":false,"start_time":"2023-02-20T20:35:11.664912","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Application\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Newsgroup classification</p>","metadata":{"papermill":{"duration":0.007699,"end_time":"2023-02-20T20:35:11.688294","exception":false,"start_time":"2023-02-20T20:35:11.680595","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Import libraries**","metadata":{"papermill":{"duration":0.007281,"end_time":"2023-02-20T20:35:11.705488","exception":false,"start_time":"2023-02-20T20:35:11.698207","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Core\nimport spacy\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.6)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# sklearn\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import f1_score, classification_report, ConfusionMatrixDisplay\nfrom sklearn.naive_bayes import MultinomialNB","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:11.723989Z","iopub.status.busy":"2023-02-20T20:35:11.723256Z","iopub.status.idle":"2023-02-20T20:35:22.175768Z","shell.execute_reply":"2023-02-20T20:35:22.174383Z"},"papermill":{"duration":10.465493,"end_time":"2023-02-20T20:35:22.178529","exception":false,"start_time":"2023-02-20T20:35:11.713036","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to use the **20 news groups** sklearn dataset again and the tast this time is given a news article, we need to classify which news group it belongs to. ","metadata":{"papermill":{"duration":0.00718,"end_time":"2023-02-20T20:35:22.193535","exception":false,"start_time":"2023-02-20T20:35:22.186355","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Fetch data\ntrain_corpus = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\ntest_corpus = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n\n# Print dataset sizes\nprint('Train size:', len(train_corpus.data))\nprint('Test size:', len(test_corpus.data))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:22.210245Z","iopub.status.busy":"2023-02-20T20:35:22.20953Z","iopub.status.idle":"2023-02-20T20:35:32.452207Z","shell.execute_reply":"2023-02-20T20:35:32.451023Z"},"papermill":{"duration":10.254204,"end_time":"2023-02-20T20:35:32.455159","exception":false,"start_time":"2023-02-20T20:35:22.200955","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And let's **preview the data** to get a better idea of what we're dealing with.","metadata":{"papermill":{"duration":0.0087,"end_time":"2023-02-20T20:35:32.47393","exception":false,"start_time":"2023-02-20T20:35:32.46523","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Labels\nprint('Labels:', train_corpus.target_names)\n\n# Label encoding\nprint('\\nLabel encoding:', train_corpus.target)\n\n# Example article\nprint('\\nExample article:', train_corpus.data[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:32.493815Z","iopub.status.busy":"2023-02-20T20:35:32.493342Z","iopub.status.idle":"2023-02-20T20:35:32.501518Z","shell.execute_reply":"2023-02-20T20:35:32.500411Z"},"papermill":{"duration":0.019977,"end_time":"2023-02-20T20:35:32.503545","exception":false,"start_time":"2023-02-20T20:35:32.483568","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's plot the **target distribution**. It appears to be **reasonably balanced**, which is good.","metadata":{"papermill":{"duration":0.007539,"end_time":"2023-02-20T20:35:32.520333","exception":false,"start_time":"2023-02-20T20:35:32.512794","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.countplot(train_corpus.target)\nplt.title('Target distribution')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:32.539109Z","iopub.status.busy":"2023-02-20T20:35:32.538483Z","iopub.status.idle":"2023-02-20T20:35:32.863359Z","shell.execute_reply":"2023-02-20T20:35:32.862652Z"},"papermill":{"duration":0.336476,"end_time":"2023-02-20T20:35:32.865129","exception":false,"start_time":"2023-02-20T20:35:32.528653","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll start with a **baseline tokenizer** that simply removes punctuation, spaces and numbers but we'll improve on this later and compare the results. To speed up this process, we'll use a **blank pipeline** as this doesn't try to parse, lemmatize etc the text.","metadata":{"papermill":{"duration":0.008148,"end_time":"2023-02-20T20:35:32.881633","exception":false,"start_time":"2023-02-20T20:35:32.873485","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load english language model\nnlp = spacy.blank('en')\n\n# Define a custom tokenizer using spacy\ndef custom_tokenizer(doc):\n    return [t.text for t in nlp(doc) if not t.is_punct and not t.is_space and t.is_alpha]","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:32.899915Z","iopub.status.busy":"2023-02-20T20:35:32.899124Z","iopub.status.idle":"2023-02-20T20:35:33.120121Z","shell.execute_reply":"2023-02-20T20:35:33.119078Z"},"papermill":{"duration":0.232429,"end_time":"2023-02-20T20:35:33.122273","exception":false,"start_time":"2023-02-20T20:35:32.889844","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To model the data, we need to **convert our text to numbers**. The most natural way is to use a **Bag-of-Words representation**, which corresponds to the derivations we've done above. But later, we'll also compare this to the **TF-IDF representation**, which is known to work well with Naive Bayes.","metadata":{"papermill":{"duration":0.007991,"end_time":"2023-02-20T20:35:33.138866","exception":false,"start_time":"2023-02-20T20:35:33.130875","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\n# Define vectorizer\nvectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n\n# Fit and transform train data\nX = vectorizer.fit_transform(train_corpus.data)\ny = train_corpus.target\n\n# Transform test data\nX_test = vectorizer.transform(test_corpus.data)\ny_test = test_corpus.target","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:33.157196Z","iopub.status.busy":"2023-02-20T20:35:33.15682Z","iopub.status.idle":"2023-02-20T20:35:57.676325Z","shell.execute_reply":"2023-02-20T20:35:57.675004Z"},"papermill":{"duration":24.531208,"end_time":"2023-02-20T20:35:57.678413","exception":false,"start_time":"2023-02-20T20:35:33.147205","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're now ready to **train** a Naive Bayes model. In sklearn, it is called `MultinomialNB` because the data has a **multinomial distribution** (since we assume words appear **independently**). Notice how **fast** the model gets trained compared to the vectorization.","metadata":{"papermill":{"duration":0.008067,"end_time":"2023-02-20T20:35:57.695021","exception":false,"start_time":"2023-02-20T20:35:57.686954","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\n# Define model\nclf = MultinomialNB()\n\n# Train model\nclf.fit(X, y)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:57.713057Z","iopub.status.busy":"2023-02-20T20:35:57.71268Z","iopub.status.idle":"2023-02-20T20:35:57.788141Z","shell.execute_reply":"2023-02-20T20:35:57.7869Z"},"papermill":{"duration":0.087756,"end_time":"2023-02-20T20:35:57.791103","exception":false,"start_time":"2023-02-20T20:35:57.703347","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we **evaluate the model** on the test set. We'll use this score as a **baseline** to compare it to improved models.","metadata":{"papermill":{"duration":0.008367,"end_time":"2023-02-20T20:35:57.808971","exception":false,"start_time":"2023-02-20T20:35:57.800604","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Predict on test set\ntest_preds = clf.predict(X_test)\n\n# Measure f1-score\nprint('Test set F1-score:', f1_score(y_test, test_preds, average='macro'))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:57.828731Z","iopub.status.busy":"2023-02-20T20:35:57.827885Z","iopub.status.idle":"2023-02-20T20:35:57.852342Z","shell.execute_reply":"2023-02-20T20:35:57.851538Z"},"papermill":{"duration":0.036551,"end_time":"2023-02-20T20:35:57.854782","exception":false,"start_time":"2023-02-20T20:35:57.818231","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the **confusion matrix** to see where the model made **misclassifications**.","metadata":{"papermill":{"duration":0.008305,"end_time":"2023-02-20T20:35:57.872171","exception":false,"start_time":"2023-02-20T20:35:57.863866","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Figure size\nfig, ax = plt.subplots(figsize=(30, 30))\nax.grid(False)\n\n# Create the confusion matrix\ndisp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, normalize='true', display_labels=train_corpus.target_names, xticks_rotation='vertical', ax=ax)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:57.891789Z","iopub.status.busy":"2023-02-20T20:35:57.891199Z","iopub.status.idle":"2023-02-20T20:35:59.892071Z","shell.execute_reply":"2023-02-20T20:35:59.8912Z"},"papermill":{"duration":2.016059,"end_time":"2023-02-20T20:35:59.897053","exception":false,"start_time":"2023-02-20T20:35:57.880994","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Observations:*\n* The more **specific** a topic is, the better the model is at classifying it.\n* Topics with lots of vocabulary **overlap**, like christianity and atheism produced the most errors.","metadata":{"papermill":{"duration":0.013594,"end_time":"2023-02-20T20:35:59.924457","exception":false,"start_time":"2023-02-20T20:35:59.910863","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Another useway way to see how well the model did is via the sklearn **classification report**, which shows a breakdown of **precision** and **recall** scores on a **class-by-class basis**.","metadata":{"papermill":{"duration":0.012779,"end_time":"2023-02-20T20:35:59.950435","exception":false,"start_time":"2023-02-20T20:35:59.937656","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(classification_report(y_test, test_preds, target_names=train_corpus.target_names))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:35:59.978647Z","iopub.status.busy":"2023-02-20T20:35:59.977982Z","iopub.status.idle":"2023-02-20T20:35:59.999369Z","shell.execute_reply":"2023-02-20T20:35:59.99844Z"},"papermill":{"duration":0.038077,"end_time":"2023-02-20T20:36:00.001662","exception":false,"start_time":"2023-02-20T20:35:59.963585","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Improvements to baseline</p>\n\nLet's try to **improve** upon the above baseline using the things we've learnt so far. For starters, we'll use add **lemmatization** and **stop-word removal** to our tokenizer. This will reduce the number of synonyms and low information words.","metadata":{"papermill":{"duration":0.013229,"end_time":"2023-02-20T20:36:00.02858","exception":false,"start_time":"2023-02-20T20:36:00.015351","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load full english language model\nnlp = spacy.load('en_core_web_sm')\n\n# Disable named-entity recognition and parsing to save time\nunwanted_pipes = ['ner', 'parser']\n\n# Custom tokenizer using spacy\ndef custom_tokenizer(doc):\n    with nlp.disable_pipes(*unwanted_pipes):\n        return [t.lemma_ for t in nlp(doc) if not t.is_punct and not t.is_space and not t.is_stop and t.is_alpha]","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:36:00.058112Z","iopub.status.busy":"2023-02-20T20:36:00.057692Z","iopub.status.idle":"2023-02-20T20:36:00.701924Z","shell.execute_reply":"2023-02-20T20:36:00.700421Z"},"papermill":{"duration":0.66239,"end_time":"2023-02-20T20:36:00.704988","exception":false,"start_time":"2023-02-20T20:36:00.042598","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use the **TF-IDF representation** as our vectorizer this time. This takes quite a bit **longer** than before because of the more complex tokenizer and vectorizer.","metadata":{"papermill":{"duration":0.012917,"end_time":"2023-02-20T20:36:00.731322","exception":false,"start_time":"2023-02-20T20:36:00.718405","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\n# Define vectorizer\nvectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n\n# Fit and transform train data\nX = vectorizer.fit_transform(train_corpus.data)\ny = train_corpus.target\n\n# Transform test data\nX_test = vectorizer.transform(test_corpus.data)\ny_test = test_corpus.target","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:36:00.760093Z","iopub.status.busy":"2023-02-20T20:36:00.759719Z","iopub.status.idle":"2023-02-20T20:40:29.762984Z","shell.execute_reply":"2023-02-20T20:40:29.761439Z"},"papermill":{"duration":269.032181,"end_time":"2023-02-20T20:40:29.777328","exception":false,"start_time":"2023-02-20T20:36:00.745147","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remember how **additive smoothing** is being applied in a **Naive Bayes** model for numerical stability. This is set using the parameter `alpha` and is default value is 1. We're now going to **tune** this **hyperparameter** using **grid search**. Even grid search and cross validation, the model is **trained very quickly**.","metadata":{"papermill":{"duration":0.012974,"end_time":"2023-02-20T20:40:29.804875","exception":false,"start_time":"2023-02-20T20:40:29.791901","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\n# Parameters to tune\ngrid = {'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n\n# Define model\nclf = MultinomialNB()\n\n# Define grid search\nmodel = GridSearchCV(clf, param_grid=grid, scoring='f1_macro', n_jobs=-1, cv=5, verbose=5)\n\n# Train model using grid search\nmodel.fit(X, y)\n\n# Print best value of alpha\nprint('Best parameters:', model.best_params_)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:40:29.835132Z","iopub.status.busy":"2023-02-20T20:40:29.834528Z","iopub.status.idle":"2023-02-20T20:40:31.442786Z","shell.execute_reply":"2023-02-20T20:40:31.441241Z"},"papermill":{"duration":1.625367,"end_time":"2023-02-20T20:40:31.444892","exception":false,"start_time":"2023-02-20T20:40:29.819525","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And **evaluate** the model.","metadata":{"papermill":{"duration":0.013347,"end_time":"2023-02-20T20:40:31.472038","exception":false,"start_time":"2023-02-20T20:40:31.458691","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Predict on test set\ntest_preds = model.predict(X_test)\n\n# Measure f1-score\nprint('Test set F1-score:', f1_score(y_test, test_preds, average='macro'))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:40:31.501534Z","iopub.status.busy":"2023-02-20T20:40:31.500196Z","iopub.status.idle":"2023-02-20T20:40:31.520043Z","shell.execute_reply":"2023-02-20T20:40:31.518816Z"},"papermill":{"duration":0.03826,"end_time":"2023-02-20T20:40:31.523043","exception":false,"start_time":"2023-02-20T20:40:31.484783","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great - that is a **10% improvement**!","metadata":{"papermill":{"duration":0.012984,"end_time":"2023-02-20T20:40:31.550334","exception":false,"start_time":"2023-02-20T20:40:31.53735","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Plot the **confusion matrix**.","metadata":{"papermill":{"duration":0.013038,"end_time":"2023-02-20T20:40:31.576584","exception":false,"start_time":"2023-02-20T20:40:31.563546","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Figure size\nfig, ax = plt.subplots(figsize=(30, 30))\nax.grid(False)\n\n# Create the confusion matrix\ndisp = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, normalize='true', display_labels=train_corpus.target_names, xticks_rotation='vertical', ax=ax)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:40:31.608586Z","iopub.status.busy":"2023-02-20T20:40:31.60704Z","iopub.status.idle":"2023-02-20T20:40:33.653276Z","shell.execute_reply":"2023-02-20T20:40:33.651992Z"},"papermill":{"duration":2.066743,"end_time":"2023-02-20T20:40:33.657282","exception":false,"start_time":"2023-02-20T20:40:31.590539","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, print the **classification report**. ","metadata":{"papermill":{"duration":0.018922,"end_time":"2023-02-20T20:40:33.696594","exception":false,"start_time":"2023-02-20T20:40:33.677672","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(classification_report(y_test, test_preds, target_names=train_corpus.target_names))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:40:33.735302Z","iopub.status.busy":"2023-02-20T20:40:33.734902Z","iopub.status.idle":"2023-02-20T20:40:33.757331Z","shell.execute_reply":"2023-02-20T20:40:33.756308Z"},"papermill":{"duration":0.04473,"end_time":"2023-02-20T20:40:33.759349","exception":false,"start_time":"2023-02-20T20:40:33.714619","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Breakdown of improvements:** (found via additional experiements)\n* Adding **lemmatization and stop-word removal** to the tokenizer improved F1-score from **57.5% to 63.2%**.\n* Using **TF-IDF** instead of Bag-of-Word improved F1-score from **63.2% to 63.8%**.\n* Tuning the **additive smoothing** hyperparameter improved F1-score from **63.8% to 67.6%**.","metadata":{"papermill":{"duration":0.018403,"end_time":"2023-02-20T20:40:33.795196","exception":false,"start_time":"2023-02-20T20:40:33.776793","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Classifying new articles</p>\n\nThe last thing we'll do is take a **new article of text** and **classify it** using our pipeline, which consists of tokenization, vectorization and naive bayes modelling. \n\nWe'll also return the **confidence** in our prediction, i.e. the **estimated probability** that the model believes the text belongs to that class.","metadata":{"papermill":{"duration":0.017264,"end_time":"2023-02-20T20:40:33.830267","exception":false,"start_time":"2023-02-20T20:40:33.813003","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Space related article\narticle = [\"Nasa's Swot satellite will survey millions of rivers and lakes\"]\n\n# Vectorize text\nX_article = vectorizer.transform(article)\n\n# Predict probability of each class\nproba_article = model.predict_proba(X_article)\n\n# Calculate predicted class and confidence\nid_max = np.argmax(proba_article)\npred_article = train_corpus.target_names[id_max]\nconf = np.max(proba_article)\n\n# Print predictions\nprint('Prediction:', pred_article)\nprint('Confidence:', conf)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:40:33.867451Z","iopub.status.busy":"2023-02-20T20:40:33.866549Z","iopub.status.idle":"2023-02-20T20:40:33.881442Z","shell.execute_reply":"2023-02-20T20:40:33.880107Z"},"papermill":{"duration":0.035965,"end_time":"2023-02-20T20:40:33.883922","exception":false,"start_time":"2023-02-20T20:40:33.847957","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}
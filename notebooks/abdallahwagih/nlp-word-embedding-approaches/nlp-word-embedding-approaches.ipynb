{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word Embeddings\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">One-hot encoding</p>\n\nThe simplest way to vectorize a set of words, is to use **one-hot encoding**. This maps each word into a vector with length equal to the size of the vocabulary. The vector is completely filled with 0's except for a single entry, which has a 1 correspoding to the index of the word in the vocabulary. \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/65VjcWzg/onehot.png\" width=600>\n</center>\n<br>\n\nThis is a pretty **terrible way** to vectorize words, not only because it is very **memory inefficient** but also because there is no relationship between words. \n\nIn particular, if the voculary contains 10,000 words, then each vector has length equal to 10,000. Furthermore, any two distinct vectors will always have a **dot product equal to 0**, corresponding to no similarity.\n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Capturing meaning</p>\n\nA better way then would be to represent words as **shorter and denser vectors** that capture some meaning between words. And this is what an embedding aim to do. \n\nAn **embedding** is simply a **representation of an object** (e.g. a word, movie, graph, etc) as a **vector of real numbers**. It *embeds* an object into a **high-dimensional vector space**. \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/9FZq6z7X/unitcube.png\" width=300>\n</center>\n<br>\n\nFor example, let's say you have a collection of videos games. We can represent each game by measuring a number of its **attributes** like `[<fantasy>,<strategy>,<multiplayer>,<action>,<adventure>]`. So a game like 'Minecraft' could be represented by `[0.1,0.6,0.4,0.5,0.9]` (although the numbers don't have to be between 0 and 1) and if we wanted to find similar game, we could return the game with the **highest similar score**. \n\nNotice how there are many **different ways to embed the same object**. Moreover, the features we hand-selected may not be the best ones to represent these objects. But the main idea is to find a representation where **similar objects** (i.e. are semantically similar) are **close together**. We can do this with words too, but how do we find their best representations?\n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/zv5V5FWJ/embed.webp\" width=700>\n</center>\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Distributional Hypothesis</p>\n\nTo train a word embedding, we first need to ask ourselves what makes two words **semantically similar**? One popular answer to this is the **distributional hypothesis**, which says that \"words which appear in **similar contexts** (i.e. share similar surrounding words) have **similar meanings**\". \n\nFor example, consider the sentence \"My family enjoys eating bacalhau at Christmas\". You probably have no idea what 'bacalhau' is but just from the **context** we can **infer** that it is some kind of food. (In fact, the word refers to Portuguese salted cod.)\n\nThat are many algorithms that use this idea to **train word embeddings**. We are going to focus on the main one, namely **Word2vec**, which was developed in 2013 at Google and was a breakthrough in NLP at the time. ([link to paper](https://arxiv.org/abs/1301.3781))","metadata":{"papermill":{"duration":0.014166,"end_time":"2023-02-20T21:20:59.003004","exception":false,"start_time":"2023-02-20T21:20:58.988838","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Word2vec\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Word addition</p>\n\nWord2vec is a way of **learning** word embeddings by **training shallow neural networks** on either the continuous bag-of-words or the skip-gram task. We will discuss both of these shortly. The **dimension** of the resulting dense vectors is usually between 50-1000 and a common value is 300.  \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/fLGfqNnV/word2vec.png\" width=500>\n</center>\n<br>\n\nThe amazing thing to come out of Word2vec is not only that similar words are close together, but that we can perform **addition and subtraction** on these word vectors. For example, **“king” - “man” + “woman” = \"queen\"**. That is, if we take the vectors for king, man, woman and add/subtract them in this way, we will end up with a vector close to the one corresponding to queen. This means that these vectors can **capture very precisely** abstract concepts (like gender and royalty) without any input from us.\n\nNote that the values in each vector don't necessarily correspond to anything like fantasy, strategy, etc like in the video game example from before. The algorithm learns the best representation even if the **numbers don't correspond to anything tangible**. The most important thing though is that similar words are close together. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Training Word2vec</p>\n\nWord2vec is a collection of **two algorithms** that each learn word vectors **indirectly** through a word prediction task. One option is **Continuous Bag-of-Words** (CBOW) and the other is **skip-gram**. \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/PfZQNzqb/cbow.png\" width=600>\n</center>\n<br>\n\nIn CBOW, the task is to **predict the missing word** from its surrounding words whereas in skip-gram the task is to **predict the surrounging words** given a single word. They are both quite similar so we will just focus on skip-gram (with an optimized adjustment called **negative sampling**) in this notebook.\n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Skip-Gram with Negative Sampling</p>\n\nThe goal of **Skip-Gram with Negative Sampling** (SGNS) is to maximise the similarity between words that appear in **similar contexts** and conversely minize the similar between words that do not appear in similar contexts. Note that depending on the corpus you train on you will get different results. \n\nConsider the following sentence: \"We ate pizza at the Italian restaurant\". In SGNS we designate a word, for example \"pizza\", to be the **input word**. The words around the input word become the **context words**. From this, we generate **input/context pairs**: (pizza, We), (pizza,ate), (pizza,at), (pizza,the), (pizza,Italian) and (pizza,restaurant). These pairs are considered **positive examples**.\n\nNote that in practice there is a **context window** that limits context words to only e.g. 2 either side the input word. This is treated as a hyperparamenter that can be tuned. Furthermore, generating positive samples can be done in a **sliding window fashion**, where we move by one word to the right each time. \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/ydNHPN2T/skipgram.png\" width=600>\n</center>\n<br>\n\nWe also generate **negative samples** by selecting words (called **negative context words**) at random from the corpus, e.g. (pizza,hero), (pizza,skip), (pizza,restoration), (pizza,crew), etc. It is ok if context words also appear as negative context words - the chance of this happening is small anyway.\n\nDoing this repeatedly for the whole corpus generates a **training set** of positive and negative examples. What is amazing is that **no labelling was needed** at any point. (This is an example of **self-supervised learning**.)\n\nThis training set gives rise to a **binary prediction task**, which is to predict if two words are either likely or unlikely to appear close to each other in text. We will see that learning to do this task well will lead us to our desired word embeddings. \n\nWe start by creating **two matrices initialised randomly** both with **dimensions |V| x d**, where |V| is the size of the vocabulary and d is the size of the word vectors (e.g. d=300). The first matrix will hold the **input word embeddings** and the second will hold the **context word embeddings**. The word \"pizza\" (and any other word) will have a vector in each matrix but the idea is that when \"pizza\" is an input word, the input matrix will get updated whereas when it is a context word, the context matrix will get updated. \n\nAt each training iteration we pick **one positive sample** $(w, c_{pos})$ and **k negative samples** $(w,c_{neg(i)})$ (k is a hyperparameter, k=3 is common). For example, let (pizza,restaurant) be the positive sample. The important thing is that we take vector for \"pizza\" from the input matrix and the vector for \"restaurant\" from the context matrix. Similarly we do the same for the negative samples. \n\nThe **probability** that words in a positive (resp. negative) example appear close to each other (resp. don't appear close to each other) is calculated as follows\n\n<br>\n$$\n\\large\n\\mathbb{P}(+|w,c_{pos}) = \\sigma (w \\cdot c_{pos}), \\qquad \\quad \\mathbb{P}(-|w,c_{neg}) = 1 - \\mathbb{P}(+|w,c_{neg}) = \\sigma (- w \\cdot c_{neg})\n$$\n<br>\n\nThe **dot product** is used to measure similarity (like in cosine similarity) and the **sigmoid function** is there to convert the numbers into probabilities. We want to **maximize** both quantities so we use the loss function (working in log space makes derivates easier to calculate)\n\n<br>\n$$\n\\large\n\\begin{align*}\nL &= - [\\log \\mathbb{P}(+|w,c_{pos}) + \\sum_{i}^{k} \\log \\mathbb{P}(-|w,c_{neg(i)})] \\\\\n&= - [\\log \\sigma (w \\cdot c_{pos}) + \\sum_{i}^{k} \\log \\sigma (- w \\cdot c_{neg(i)})] \\\\\n\\end{align*}\n$$\n<br>\n\n**Minimizing** this loss function will maximise the probabilities above, which have been constructed to force similar words to be close to each other and disimilar words to be far away from each other. We can use any **optimization algorithm** we want to do this, for example stochastic gradient descent or the Adam optimizer. During training, the two matrices (corresponding input and context) get updated at each iteration. Once training is completed, we will have arrived at the word embeddings we wanted. Usually just the input matrix embedding is used but you could also average the two matrices together.\n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Training tricks</p>\n\nThere are a few tricks used in practice to **maximise the performance** of the skip-gram model and thus **improve the quality** of the embeddings. First, the following formula is commonly used when sampling **negative context words** in negative examples. This is basically the **relative frequency** of each word but with a modification that puts **more weight on rare words** and less weight on common words. The authors suggest $\\alpha=0.75$ is a good choice for this hyperparameter. \n\n<br>\n$$\n\\large\n\\mathbb{P}_{\\alpha}(w) = \\frac{\\text{count}(w)^{\\alpha}}{\\sum_i \\text{count}(w_i)^{\\alpha}}\n$$\n<br>\n\nThe **window size** when sampling **positive examples** is also a hyperparameter as we said earlier. Researchers have found that **narrower windows** are better at capturing **functional similarity** better whereas wider windows are better at capturing **relatedness**. \n\nThe **number of negative samples** selected at each iteration is controlled by the hyperparameter **k**. For large datasets, k is recommended to be between **2-5** whereas for smaller datasets a better choice is between **5-20**. \n\nFinally, in some implementations of Word2vec each word vector (input/context) gets passed through a **layer** of a **connected neural network** before the dot product is taken. This can improve the performance of the model but conceptually the process of learning the word embeddings is the same. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Using word vectors</p>\n\nTraining accurate word embeddings from scratch takes a **lot of computational resources**. If we want to use word vectors in our models then we have a few options depending on the requirements of our task.\n\n1. Use **pre-trained word vectors** as model inputs and keep them **constant during training**. \n2. Use **pre-trained word vectors** as model inputs and allow the model to **tune** them during training. \n3. **Train word embeddings from scratch** at the same time as training the model. \n\nOption 1 will be the **fastest** and will usually lead to very good results. Option 2 can produce **marginally better results** if you are willing to take longer to train your model. Option 3 can be useful if you have **lots of training data** and want to train a model for a very **specific task** that pretrained embeddings don't perform well on. We will explore these different options with code in the next section. ","metadata":{"papermill":{"duration":0.013414,"end_time":"2023-02-20T21:20:59.036544","exception":false,"start_time":"2023-02-20T21:20:59.02313","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Application\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Word vectors</p>\n\nWe're going to be using **pre-trained word vectors** via the **Gensim** library, which we came across last time. These particular vectors are known as the **Google News vectors** as they were trained on a 3 billion word Google News corpus in 2015. In total, there are **3 million, 300-dimension vectors**. \n\n<br>\n\n**Import libraries**","metadata":{"papermill":{"duration":0.016019,"end_time":"2023-02-20T21:20:59.066491","exception":false,"start_time":"2023-02-20T21:20:59.050472","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('max_colwidth', 600)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.6)\nimport spacy\nimport random\nimport collections\nimport re\nimport emoji\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport gensim","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:20:59.098509Z","iopub.status.busy":"2023-02-20T21:20:59.097364Z","iopub.status.idle":"2023-02-20T21:21:12.930384Z","shell.execute_reply":"2023-02-20T21:21:12.92912Z"},"papermill":{"duration":13.852444,"end_time":"2023-02-20T21:21:12.933796","exception":false,"start_time":"2023-02-20T21:20:59.081352","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load word vectors**\n\nThe word vectors are stored in a kaggle dataset. We use the gensim library to load them.","metadata":{"papermill":{"duration":0.013457,"end_time":"2023-02-20T21:21:12.961165","exception":false,"start_time":"2023-02-20T21:21:12.947708","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\n# Path to dataset\nword2vec_path = \"/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\"\n\n# Load 200,000 most common words\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=200000) ","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:12.992517Z","iopub.status.busy":"2023-02-20T21:21:12.991821Z","iopub.status.idle":"2023-02-20T21:21:18.177764Z","shell.execute_reply":"2023-02-20T21:21:18.176329Z"},"papermill":{"duration":5.205157,"end_time":"2023-02-20T21:21:18.180484","exception":false,"start_time":"2023-02-20T21:21:12.975327","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can **retrieve** a single word vector by passing the token as the key.","metadata":{"papermill":{"duration":0.013274,"end_time":"2023-02-20T21:21:18.207768","exception":false,"start_time":"2023-02-20T21:21:18.194494","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Get word vector\nscience = word2vec_model['science']\n\n# Print shape and vector\nprint('Vector dimension:', science.shape)\nprint(science)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:18.236914Z","iopub.status.busy":"2023-02-20T21:21:18.23642Z","iopub.status.idle":"2023-02-20T21:21:18.253016Z","shell.execute_reply":"2023-02-20T21:21:18.250567Z"},"papermill":{"duration":0.034944,"end_time":"2023-02-20T21:21:18.256127","exception":false,"start_time":"2023-02-20T21:21:18.221183","status":"completed"},"tags":[],"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can measure the similarity between any two words by using the `.similarity` method.","metadata":{"papermill":{"duration":0.013778,"end_time":"2023-02-20T21:21:18.283929","exception":false,"start_time":"2023-02-20T21:21:18.270151","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(word2vec_model.similarity('science', 'mathematics'))\nprint(word2vec_model.similarity('science', 'space'))\nprint(word2vec_model.similarity('science', 'beard'))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:18.313219Z","iopub.status.busy":"2023-02-20T21:21:18.312817Z","iopub.status.idle":"2023-02-20T21:21:18.322242Z","shell.execute_reply":"2023-02-20T21:21:18.320169Z"},"papermill":{"duration":0.02753,"end_time":"2023-02-20T21:21:18.324973","exception":false,"start_time":"2023-02-20T21:21:18.297443","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can measure the similarity between sentences by using the `.n_similarity` method. This expects a list of words and works by **averaging** the word vectors in each sentence and computing the cosine similarity between the resulting two vectors. This doesn't take word order into account but works quite well.","metadata":{"papermill":{"duration":0.013188,"end_time":"2023-02-20T21:21:18.351918","exception":false,"start_time":"2023-02-20T21:21:18.33873","status":"completed"},"tags":[]}},{"cell_type":"code","source":"word2vec_model.n_similarity(\"Man goes running\".split(),\"Woman goes swimming\".split())","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:18.3819Z","iopub.status.busy":"2023-02-20T21:21:18.381344Z","iopub.status.idle":"2023-02-20T21:21:18.395052Z","shell.execute_reply":"2023-02-20T21:21:18.393668Z"},"papermill":{"duration":0.031766,"end_time":"2023-02-20T21:21:18.397874","exception":false,"start_time":"2023-02-20T21:21:18.366108","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gensim also has a `.most_similar` method, which returns the **closest** word vectors to the input.","metadata":{"papermill":{"duration":0.014018,"end_time":"2023-02-20T21:21:18.426483","exception":false,"start_time":"2023-02-20T21:21:18.412465","status":"completed"},"tags":[]}},{"cell_type":"code","source":"word2vec_model.most_similar(positive=['jelly'], topn=10)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:18.460469Z","iopub.status.busy":"2023-02-20T21:21:18.460042Z","iopub.status.idle":"2023-02-20T21:21:18.630664Z","shell.execute_reply":"2023-02-20T21:21:18.628506Z"},"papermill":{"duration":0.194201,"end_time":"2023-02-20T21:21:18.636054","exception":false,"start_time":"2023-02-20T21:21:18.441853","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize the word vectors in **2 dimensions by using PCA**, which is **dimensionality reduction technique**. This finds the subspace that **maximizes the variance** of the data, in other words it finds the 2-D plane where the data points are as spread out as possible. This allows us to get a better idea of the structure and patterns in the word vectors.","metadata":{"papermill":{"duration":0.035645,"end_time":"2023-02-20T21:21:18.708408","exception":false,"start_time":"2023-02-20T21:21:18.672763","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Vectors to transform\nwords = [\"football\",\"tennis\",\"rugby\",\"river\",\"ocean\",\"water\",\"brownie\",\"jelly\",\"cake\"] #['swim', 'swimming', 'cat', 'dog', 'feline', 'road', 'car', 'bus']\nword_vectors = np.array([word2vec_model[w] for w in words])\n\n# PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(word_vectors)\n\n# Plot transformed vectors\nplt.figure(figsize=(12,6))\nplt.scatter(X_pca[:,0], X_pca[:,1], edgecolors='k', c='g', s=128)\nfor word, (x,y) in zip(words, X_pca):\n    plt.text(x+0.01,y+0.01, word)\nplt.title(\"PCA plot of word vectors\", y=1.02)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:18.752852Z","iopub.status.busy":"2023-02-20T21:21:18.752416Z","iopub.status.idle":"2023-02-20T21:21:19.121401Z","shell.execute_reply":"2023-02-20T21:21:19.119965Z"},"papermill":{"duration":0.388192,"end_time":"2023-02-20T21:21:19.124268","exception":false,"start_time":"2023-02-20T21:21:18.736076","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like we discussed previously, Word2vec can solve **analogies**. In gensim, we just need to specify the **positive terms** and the **negative terms** and it will retrieve the closest vector to the result.\n\nFor example, \"France\"-\"Paris\"+\"Tokyo\" should give us \"Japan\" as Tokyo is the capital of Japan and indeed it does.","metadata":{"papermill":{"duration":0.015706,"end_time":"2023-02-20T21:21:19.155611","exception":false,"start_time":"2023-02-20T21:21:19.139905","status":"completed"},"tags":[]}},{"cell_type":"code","source":"word2vec_model.most_similar(positive=[\"France\", \"Tokyo\"], negative=[\"Paris\"], topn=3)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:19.188588Z","iopub.status.busy":"2023-02-20T21:21:19.187493Z","iopub.status.idle":"2023-02-20T21:21:19.230464Z","shell.execute_reply":"2023-02-20T21:21:19.228501Z"},"papermill":{"duration":0.064548,"end_time":"2023-02-20T21:21:19.2355","exception":false,"start_time":"2023-02-20T21:21:19.170952","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Modelling with pre-trained word embeddings</p>\n\nTo showcase how word embeddings can be used in neural networks, we are going to perform sentiment analysis on a **spotify app reviews dataset**. Each review has a corresponding number of **stars** awarded **between 1 and 5**, which is what we are going to try to predict.","metadata":{"papermill":{"duration":0.037347,"end_time":"2023-02-20T21:21:19.311575","exception":false,"start_time":"2023-02-20T21:21:19.274228","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load data\ndata = pd.read_csv(\"/kaggle/input/spotify-app-reviews-2022/reviews.csv\")\ndata = data[[\"Review\",\"Rating\"]]\n\n# Print shape and preview\nprint(data.shape)\ndata.head()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:19.354738Z","iopub.status.busy":"2023-02-20T21:21:19.354219Z","iopub.status.idle":"2023-02-20T21:21:19.853772Z","shell.execute_reply":"2023-02-20T21:21:19.852015Z"},"papermill":{"duration":0.519464,"end_time":"2023-02-20T21:21:19.856805","exception":false,"start_time":"2023-02-20T21:21:19.337341","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Currently some of the reviews are very **long** (the maximum is over 3000 characters!). Later in our keras model we will truncate these so they are **not longer than 250 characters**, which will speed up training time.","metadata":{"papermill":{"duration":0.014371,"end_time":"2023-02-20T21:21:19.886321","exception":false,"start_time":"2023-02-20T21:21:19.87195","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Lenght of reviews\nplt.figure(figsize=(10,4))\nsns.histplot(data[\"Review\"].apply(lambda x : len(x)))\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:19.918962Z","iopub.status.busy":"2023-02-20T21:21:19.918006Z","iopub.status.idle":"2023-02-20T21:21:21.12399Z","shell.execute_reply":"2023-02-20T21:21:21.121997Z"},"papermill":{"duration":1.226081,"end_time":"2023-02-20T21:21:21.12708","exception":false,"start_time":"2023-02-20T21:21:19.900999","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll also do some basic **data cleaning**.","metadata":{"papermill":{"duration":0.016087,"end_time":"2023-02-20T21:21:21.159612","exception":false,"start_time":"2023-02-20T21:21:21.143525","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Clean text function\ndef clean_text(text):\n    text = emoji.demojize(text)                # Remove emojis\n    text = text.replace('_', ' ')              # Replace underscores with space\n    text = text.strip()                        # Remove extra spaces\n    return text\n\n# Applying cleaning function\ndata[\"Review\"] = data[\"Review\"].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:21.195307Z","iopub.status.busy":"2023-02-20T21:21:21.194221Z","iopub.status.idle":"2023-02-20T21:21:24.536291Z","shell.execute_reply":"2023-02-20T21:21:24.53493Z"},"papermill":{"duration":3.362991,"end_time":"2023-02-20T21:21:24.539498","exception":false,"start_time":"2023-02-20T21:21:21.176507","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the **target distribution**. We can see there is an **imbalance** with more ratings leaning towards the **extremes**. We going to treat this problem as a **multiclass classification** one. ","metadata":{"papermill":{"duration":0.015553,"end_time":"2023-02-20T21:21:24.571433","exception":false,"start_time":"2023-02-20T21:21:24.55588","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Lenght of reviews\nplt.figure(figsize=(12,5))\nsns.countplot(data=data, x=\"Rating\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:24.605759Z","iopub.status.busy":"2023-02-20T21:21:24.605315Z","iopub.status.idle":"2023-02-20T21:21:24.844548Z","shell.execute_reply":"2023-02-20T21:21:24.843154Z"},"papermill":{"duration":0.259909,"end_time":"2023-02-20T21:21:24.847314","exception":false,"start_time":"2023-02-20T21:21:24.587405","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features and labels**","metadata":{"papermill":{"duration":0.016929,"end_time":"2023-02-20T21:21:24.880718","exception":false,"start_time":"2023-02-20T21:21:24.863789","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define features and labels\nX = data[\"Review\"]\ny = data[\"Rating\"] - 1         # map to 0,1,2,3,4","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:24.917787Z","iopub.status.busy":"2023-02-20T21:21:24.917292Z","iopub.status.idle":"2023-02-20T21:21:24.924001Z","shell.execute_reply":"2023-02-20T21:21:24.922716Z"},"papermill":{"duration":0.028288,"end_time":"2023-02-20T21:21:24.926574","exception":false,"start_time":"2023-02-20T21:21:24.898286","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train-valid-test split**","metadata":{"papermill":{"duration":0.016091,"end_time":"2023-02-20T21:21:24.959146","exception":false,"start_time":"2023-02-20T21:21:24.943055","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create train, validation and test sets\nX_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=0)\nX_train, X_valid, y_train, y_valid = train_test_split(X_tr, y_tr, test_size=0.25, shuffle=True, stratify=y_tr, random_state=0) # 0.25 * 0.8 = 0.2\n\n# Print shapes\nprint(\"Train size:\", X_train.shape)\nprint(\"Valid size:\", X_valid.shape)\nprint(\"Test size:\", X_test.shape)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:24.994537Z","iopub.status.busy":"2023-02-20T21:21:24.994047Z","iopub.status.idle":"2023-02-20T21:21:25.066155Z","shell.execute_reply":"2023-02-20T21:21:25.063856Z"},"papermill":{"duration":0.093515,"end_time":"2023-02-20T21:21:25.069095","exception":false,"start_time":"2023-02-20T21:21:24.97558","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokenizer**\n\n<br>\n\nInstead of using spacy, this time we will create a **tokenizer using keras** because it will more easily integrate with our keras model. We set it to **remove special characters**, **lower case** all words and only use the **most frequent 20000 words**. This allows the model to focus only on the most frequent words that are **relevant to the sentiment** and speeds up training time. ","metadata":{"papermill":{"duration":0.015428,"end_time":"2023-02-20T21:21:25.100799","exception":false,"start_time":"2023-02-20T21:21:25.085371","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Keras tokenizer\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=20000, filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n\n# Build vocabulary on train set\ntokenizer.fit_on_texts(X_train)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:25.133977Z","iopub.status.busy":"2023-02-20T21:21:25.133311Z","iopub.status.idle":"2023-02-20T21:21:26.468196Z","shell.execute_reply":"2023-02-20T21:21:26.467055Z"},"papermill":{"duration":1.354861,"end_time":"2023-02-20T21:21:26.471257","exception":false,"start_time":"2023-02-20T21:21:25.116396","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Vectorize**\n\nWe are going to vectorize by mapping every token to an **integer ID** that we can use to look up word vectors.","metadata":{"papermill":{"duration":0.016877,"end_time":"2023-02-20T21:21:26.504714","exception":false,"start_time":"2023-02-20T21:21:26.487837","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Vectorize\nX_train_ids = tokenizer.texts_to_sequences(X_train)\nX_valid_ids = tokenizer.texts_to_sequences(X_valid)\nX_test_ids = tokenizer.texts_to_sequences(X_test)\n\n# Example\nprint(X_train_ids[0][:10])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:26.540411Z","iopub.status.busy":"2023-02-20T21:21:26.539991Z","iopub.status.idle":"2023-02-20T21:21:28.538066Z","shell.execute_reply":"2023-02-20T21:21:28.536815Z"},"papermill":{"duration":2.019157,"end_time":"2023-02-20T21:21:28.540962","exception":false,"start_time":"2023-02-20T21:21:26.521805","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Padding is a common technique used to convert all the sequences to the **same length**. Longer sequences are truncated and shorter sequences are padded with 0's at the beginning. This is required for some models (not here) but it can also make it **easier to optimize** the loss function. ","metadata":{"papermill":{"duration":0.015793,"end_time":"2023-02-20T21:21:28.573004","exception":false,"start_time":"2023-02-20T21:21:28.557211","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Max length of sequences\nmax_length = 250\n\n# Pad sequences\nX_train_pad = keras.preprocessing.sequence.pad_sequences(X_train_ids, maxlen=max_length)\nX_valid_pad = keras.preprocessing.sequence.pad_sequences(X_valid_ids, maxlen=max_length)\nX_test_pad = keras.preprocessing.sequence.pad_sequences(X_test_ids, maxlen=max_length)\n\n# Example\nprint(X_train_pad[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:28.608001Z","iopub.status.busy":"2023-02-20T21:21:28.60749Z","iopub.status.idle":"2023-02-20T21:21:28.892419Z","shell.execute_reply":"2023-02-20T21:21:28.891232Z"},"papermill":{"duration":0.305981,"end_time":"2023-02-20T21:21:28.895321","exception":false,"start_time":"2023-02-20T21:21:28.58934","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lookup dictionary**\n\n<br>\n\nWe now to **map** the word ids to the word vectors in gensim so we can use them in our model. We do this by creating an **embedding matrix** that stores the word vectors in the right order if they exist (otherwise fill that row with 0's).","metadata":{"papermill":{"duration":0.016831,"end_time":"2023-02-20T21:21:28.928617","exception":false,"start_time":"2023-02-20T21:21:28.911786","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load 1,000,000 most common words\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1_000_000)\n\n# Add one for padding token\nnum_tokens = len(tokenizer.word_index) + 1\n\n# Word vector dimension\nembedding_dim = 300\n\n# Initialize matrix of zeros\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\n\n# Fill embedding matrix\nfor word, i in tokenizer.word_index.items():\n    if word2vec_model.has_index_for(word):\n        embedding_matrix[i] = word2vec_model[word].copy()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:28.967392Z","iopub.status.busy":"2023-02-20T21:21:28.966969Z","iopub.status.idle":"2023-02-20T21:21:51.509252Z","shell.execute_reply":"2023-02-20T21:21:51.507615Z"},"papermill":{"duration":22.56566,"end_time":"2023-02-20T21:21:51.512466","exception":false,"start_time":"2023-02-20T21:21:28.946806","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that there are **many words** which we **could not find word vectors for**. This is likely because they were trained on a news corpus whereas our dataset is a corpus of app reviews and they each use quite a different vocabulary. ","metadata":{"papermill":{"duration":0.015453,"end_time":"2023-02-20T21:21:51.544367","exception":false,"start_time":"2023-02-20T21:21:51.528914","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"Embedding matrix shape:\", embedding_matrix.shape)\nprint(\"Number of words without word vectors\", int((embedding_matrix==0).sum()/300))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:51.578186Z","iopub.status.busy":"2023-02-20T21:21:51.577716Z","iopub.status.idle":"2023-02-20T21:21:51.59866Z","shell.execute_reply":"2023-02-20T21:21:51.596731Z"},"papermill":{"duration":0.042215,"end_time":"2023-02-20T21:21:51.602274","exception":false,"start_time":"2023-02-20T21:21:51.560059","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build model**\n\nWe're now ready to build the model. The first layer of the neural network will be an **embedding layer** with our embedding matrix as input. We'll set `trainable = True` so the model can **fine tune** these word vectors. This is a good idea since we found out many of the vectors are 0's. ","metadata":{"papermill":{"duration":0.015441,"end_time":"2023-02-20T21:21:51.634056","exception":false,"start_time":"2023-02-20T21:21:51.618615","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Embedding layer with pre-trained word vectors\nembedding_layer = layers.Embedding(\n    input_dim = num_tokens,\n    output_dim = embedding_dim,\n    embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n    input_length = max_length,\n    trainable = True\n)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:51.668651Z","iopub.status.busy":"2023-02-20T21:21:51.667406Z","iopub.status.idle":"2023-02-20T21:21:51.723562Z","shell.execute_reply":"2023-02-20T21:21:51.722107Z"},"papermill":{"duration":0.076781,"end_time":"2023-02-20T21:21:51.72653","exception":false,"start_time":"2023-02-20T21:21:51.649749","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build model\ndef build_model(embedding_layer):\n    model = keras.Sequential([\n\n        # Look up word vectors from integer ids\n        embedding_layer,\n        \n        # Average all word vectors in sequence\n        layers.GlobalAveragePooling1D(),\n        \n        # Fully connected layers\n        layers.Dense(units=256, activation='relu', kernel_initializer=tf.keras.initializers.random_normal()),\n        layers.Dense(units=128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal()),\n        \n        # Output layer (softmax returns a probability distribution)\n        layers.Dense(units=5, activation='softmax', kernel_initializer=tf.keras.initializers.random_normal())\n    ])\n    \n    # Define optimizer, loss function and accuracy metric\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['sparse_categorical_accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:51.761332Z","iopub.status.busy":"2023-02-20T21:21:51.760762Z","iopub.status.idle":"2023-02-20T21:21:51.769574Z","shell.execute_reply":"2023-02-20T21:21:51.768191Z"},"papermill":{"duration":0.028571,"end_time":"2023-02-20T21:21:51.771829","exception":false,"start_time":"2023-02-20T21:21:51.743258","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how most of the **model parameters** come from the embedding layer.","metadata":{"papermill":{"duration":0.015489,"end_time":"2023-02-20T21:21:51.803329","exception":false,"start_time":"2023-02-20T21:21:51.78784","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build model\nmodel = build_model(embedding_layer)\n\n# Model summary\nmodel.summary()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:51.837581Z","iopub.status.busy":"2023-02-20T21:21:51.837051Z","iopub.status.idle":"2023-02-20T21:21:52.070422Z","shell.execute_reply":"2023-02-20T21:21:52.067575Z"},"papermill":{"duration":0.255044,"end_time":"2023-02-20T21:21:52.074169","exception":false,"start_time":"2023-02-20T21:21:51.819125","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{"papermill":{"duration":0.016271,"end_time":"2023-02-20T21:21:52.107279","exception":false,"start_time":"2023-02-20T21:21:52.091008","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train model\nhistory = model.fit(\n    X_train_pad, y_train,\n    validation_data=(X_valid_pad, y_valid),\n    epochs=10,\n    batch_size=128,\n    verbose=True\n)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:21:52.141752Z","iopub.status.busy":"2023-02-20T21:21:52.141282Z","iopub.status.idle":"2023-02-20T21:25:12.396059Z","shell.execute_reply":"2023-02-20T21:25:12.394296Z"},"papermill":{"duration":200.275513,"end_time":"2023-02-20T21:25:12.399342","exception":false,"start_time":"2023-02-20T21:21:52.123829","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Learning curves**\n\n<br>\n\nIt is clear that the model begins to **overfit very quickly** because of the large number of **trainable parameters** in our model. If we set `trainable = False` then we would see less overfitting, although the accuracy wouldn't necessarily improve by much. In future notebooks, we will explore how to reach better performance with sequential neural networks.","metadata":{"papermill":{"duration":0.20448,"end_time":"2023-02-20T21:25:12.809518","exception":false,"start_time":"2023-02-20T21:25:12.605038","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Plot learning curves\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot(title=\"Accuracy\")\n\nprint('Final accuracy on validation set:', history_df.loc[len(history_df)-1,'val_sparse_categorical_accuracy'])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:25:13.304174Z","iopub.status.busy":"2023-02-20T21:25:13.303758Z","iopub.status.idle":"2023-02-20T21:25:13.84766Z","shell.execute_reply":"2023-02-20T21:25:13.846354Z"},"papermill":{"duration":0.746432,"end_time":"2023-02-20T21:25:13.850119","exception":false,"start_time":"2023-02-20T21:25:13.103687","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate on test set**","metadata":{"papermill":{"duration":0.204541,"end_time":"2023-02-20T21:25:14.256475","exception":false,"start_time":"2023-02-20T21:25:14.051934","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Make predictions on test set\npreds = model.predict(X_test_pad)\n\n# Retrieve most likely class\nclass_preds = np.argmax(preds,axis=1)\n\n# Print accuracy\nprint(f\"Test set accuracy: {accuracy_score(y_test, class_preds):.4f}\")","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:25:14.676855Z","iopub.status.busy":"2023-02-20T21:25:14.676352Z","iopub.status.idle":"2023-02-20T21:25:16.427765Z","shell.execute_reply":"2023-02-20T21:25:16.426421Z"},"papermill":{"duration":1.96473,"end_time":"2023-02-20T21:25:16.430465","exception":false,"start_time":"2023-02-20T21:25:14.465735","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though the accuracy is low, our **model did fairly well** considering there are **5 classes to predict**. In general it managed to work out if a review was positive or negative quite well.","metadata":{"papermill":{"duration":0.255709,"end_time":"2023-02-20T21:25:16.888964","exception":false,"start_time":"2023-02-20T21:25:16.633255","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Confusion matrix\ncm = confusion_matrix(y_test, class_preds)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:25:17.2918Z","iopub.status.busy":"2023-02-20T21:25:17.29107Z","iopub.status.idle":"2023-02-20T21:25:17.645902Z","shell.execute_reply":"2023-02-20T21:25:17.644656Z"},"papermill":{"duration":0.558106,"end_time":"2023-02-20T21:25:17.648859","exception":false,"start_time":"2023-02-20T21:25:17.090753","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Modelling with word embeddings from scratch</p>\n\nSometimes we might not want to use pretrained word embeddings and instead let our model figure out the **best word representations for our task** at hand. By default, the keras embedding layer does this **automatically**. ","metadata":{"papermill":{"duration":0.203559,"end_time":"2023-02-20T21:25:18.057559","exception":false,"start_time":"2023-02-20T21:25:17.854","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Embedding layer to train from scratch\nembedding_layer = layers.Embedding(\n    input_dim = num_tokens,\n    output_dim = embedding_dim,\n    input_length = max_length\n)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:25:18.464421Z","iopub.status.busy":"2023-02-20T21:25:18.462972Z","iopub.status.idle":"2023-02-20T21:25:18.471361Z","shell.execute_reply":"2023-02-20T21:25:18.47048Z"},"papermill":{"duration":0.216921,"end_time":"2023-02-20T21:25:18.473924","exception":false,"start_time":"2023-02-20T21:25:18.257003","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{"papermill":{"duration":0.198459,"end_time":"2023-02-20T21:25:18.871997","exception":false,"start_time":"2023-02-20T21:25:18.673538","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build and train model\nmodel_sc = build_model(embedding_layer)\nhistory_sc = model_sc.fit(\n    X_train_pad, y_train,\n    validation_data=(X_valid_pad, y_valid),\n    epochs=10,\n    batch_size=128,\n    verbose=True\n)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:25:19.343735Z","iopub.status.busy":"2023-02-20T21:25:19.343327Z","iopub.status.idle":"2023-02-20T21:28:37.43591Z","shell.execute_reply":"2023-02-20T21:28:37.434743Z"},"papermill":{"duration":198.352786,"end_time":"2023-02-20T21:28:37.438552","exception":false,"start_time":"2023-02-20T21:25:19.085766","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Learning curves**","metadata":{"papermill":{"duration":0.391891,"end_time":"2023-02-20T21:28:38.22383","exception":false,"start_time":"2023-02-20T21:28:37.831939","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Plot learning curves\nhistory_sc_df = pd.DataFrame(history_sc.history)\nhistory_sc_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_sc_df.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot(title=\"Accuracy\")\n\nprint('Final accuracy on validation set:', history_sc_df.loc[len(history_sc_df)-1,'val_sparse_categorical_accuracy'])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:28:39.0863Z","iopub.status.busy":"2023-02-20T21:28:39.085454Z","iopub.status.idle":"2023-02-20T21:28:39.608228Z","shell.execute_reply":"2023-02-20T21:28:39.606681Z"},"papermill":{"duration":0.93923,"end_time":"2023-02-20T21:28:39.610823","exception":false,"start_time":"2023-02-20T21:28:38.671593","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate on test set**","metadata":{"papermill":{"duration":0.383644,"end_time":"2023-02-20T21:28:40.377577","exception":false,"start_time":"2023-02-20T21:28:39.993933","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Make predictions on test set\npreds_sc = model_sc.predict(X_test_pad)\n\n# Retrieve most likely class\nclass_preds_sc = np.argmax(preds_sc,axis=1)\n\n# Print accuracy\nprint(f\"Test set accuracy: {accuracy_score(y_test, class_preds_sc):.4f}\")","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:28:41.201335Z","iopub.status.busy":"2023-02-20T21:28:41.200889Z","iopub.status.idle":"2023-02-20T21:28:42.92682Z","shell.execute_reply":"2023-02-20T21:28:42.925216Z"},"papermill":{"duration":2.175019,"end_time":"2023-02-20T21:28:42.929843","exception":false,"start_time":"2023-02-20T21:28:40.754824","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this task, training our own word embeddings from scratch **performed roughly the same** as using word2vec embeddings. This means there is **sufficient data** to learn good representations for our task. ","metadata":{"papermill":{"duration":0.395884,"end_time":"2023-02-20T21:28:43.731416","exception":false,"start_time":"2023-02-20T21:28:43.335532","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Confusion matrix\ncm_sc = confusion_matrix(y_test, class_preds_sc)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_sc, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:28:44.566349Z","iopub.status.busy":"2023-02-20T21:28:44.565871Z","iopub.status.idle":"2023-02-20T21:28:44.929589Z","shell.execute_reply":"2023-02-20T21:28:44.928177Z"},"papermill":{"duration":0.810151,"end_time":"2023-02-20T21:28:44.932314","exception":false,"start_time":"2023-02-20T21:28:44.122163","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import needed modules","metadata":{"id":"CKeVGxZ5GG6o"}},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport shutil\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nsns.set_style('darkgrid')\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\nprint ('modules loaded')","metadata":{"id":"CeMcAy_5GG6s","execution":{"iopub.status.busy":"2022-11-04T20:16:38.972589Z","iopub.execute_input":"2022-11-04T20:16:38.973001Z","iopub.status.idle":"2022-11-04T20:16:44.374993Z","shell.execute_reply.started":"2022-11-04T20:16:38.972919Z","shell.execute_reply":"2022-11-04T20:16:44.374024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create needed functions","metadata":{"id":"SA_gwvwnGG6v"}},{"cell_type":"markdown","source":"Function to create dataframe","metadata":{"id":"JQdhl_CRGG6v"}},{"cell_type":"code","source":"def define_paths(dir):\n    filepaths = []\n    labels = []\n    folds = os.listdir(dir)\n    for fold in folds:\n        foldpath = os.path.join(dir, fold)\n        filelist = os.listdir(foldpath)\n        for file in filelist:\n            fpath = os.path.join(foldpath, file)\n            filepaths.append(fpath)\n            labels.append(fold)\n    return filepaths, labels\n\ndef define_df(files, classes):\n    Fseries = pd.Series(files, name= 'filepaths')\n    Lseries = pd.Series(classes, name='labels')\n    return pd.concat([Fseries, Lseries], axis= 1)\n\ndef create_df(tr_dir, val_dir):\n    # train dataframe\n    files, classes = define_paths(tr_dir)\n    train_df = define_df(files, classes)\n    \n    # valid and test dataframe\n    files, classes = define_paths(tr_dir)\n    dummy_df = define_df(files, classes)\n    strat = dummy_df['labels']\n    valid_df, test_df = train_test_split(dummy_df,  train_size= 0.6, shuffle= True, random_state= 123, stratify= strat)\n    return train_df, valid_df, test_df","metadata":{"id":"La4bEbHlGG6w","execution":{"iopub.status.busy":"2022-11-04T20:20:41.50362Z","iopub.execute_input":"2022-11-04T20:20:41.503988Z","iopub.status.idle":"2022-11-04T20:20:41.512594Z","shell.execute_reply.started":"2022-11-04T20:20:41.50395Z","shell.execute_reply":"2022-11-04T20:20:41.511602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to generate images from dataframe","metadata":{"id":"JZaHdeFxGG6x"}},{"cell_type":"code","source":"def create_gens(train_df, valid_df, test_df, batch_size):\n    img_size = (224, 224)\n    channels = 3\n    img_shape = (img_size[0], img_size[1], channels)\n    ts_length = len(test_df)\n    test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n    test_steps = ts_length // test_batch_size\n    def scalar(img):\n        return img\n    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                        color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                        color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                        color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)\n    return train_gen, valid_gen, test_gen","metadata":{"id":"iLL8hHQcGG6x","execution":{"iopub.status.busy":"2022-11-04T20:20:46.47991Z","iopub.execute_input":"2022-11-04T20:20:46.480294Z","iopub.status.idle":"2022-11-04T20:20:46.489914Z","shell.execute_reply.started":"2022-11-04T20:20:46.480261Z","shell.execute_reply":"2022-11-04T20:20:46.488872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to show images","metadata":{"id":"8ifXox4SGG6y"}},{"cell_type":"code","source":"def show_images(gen):\n    g_dict = gen.class_indices\n    classes = list(g_dict.keys())\n    images, labels = next(gen)\n    plt.figure(figsize= (20, 20))\n    length = len(labels)\n    sample = min(length, 25)\n    for i in range(sample):\n        plt.subplot(5, 5, i + 1)\n        image = images[i] / 255\n        plt.imshow(image)\n        index = np.argmax(labels[i])\n        class_name = classes[index]\n        plt.title(class_name, color= 'blue', fontsize= 12)\n        plt.axis('off')\n    plt.show()","metadata":{"id":"IAGbj3ZyGG6y","execution":{"iopub.status.busy":"2022-11-04T20:20:48.767558Z","iopub.execute_input":"2022-11-04T20:20:48.767908Z","iopub.status.idle":"2022-11-04T20:20:48.775844Z","shell.execute_reply.started":"2022-11-04T20:20:48.767878Z","shell.execute_reply":"2022-11-04T20:20:48.774833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Callback Class","metadata":{"id":"_K-ryg0DGG6z"}},{"cell_type":"code","source":"class MyCallback(keras.callbacks.Callback):\n    def __init__(self, model, base_model, patience, stop_patience, threshold, factor, batches, initial_epoch, epochs):\n        super(MyCallback, self).__init__()\n        self.model = model\n        self.base_model = base_model\n        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor = factor # factor by which to reduce the learning rate\n        self.batches = batches # number of training batch to runn per epoch\n        self.initial_epoch = initial_epoch\n        self.epochs = epochs\n        # callback variables\n        self.count = 0 # how many times lr has been reduced without improvement\n        self.stop_count = 0\n        self.best_epoch = 1   # epoch with the lowest loss\n        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n\n    # Define a function that will run when train begins\n    def on_train_begin(self, logs= None):\n        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n        print(msg)\n        self.start_time = time.time()\n\n    def on_train_end(self, logs= None):\n        stop_time = time.time()\n        tr_duration = stop_time - self.start_time\n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print(msg)\n        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n\n    def on_train_batch_end(self, batch, logs= None):\n        acc = logs.get('accuracy') * 100 # get batch accuracy\n        loss = logs.get('loss')\n        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n        print(msg, '\\r', end= '') # prints over on the same line to show running batch count\n\n    def on_epoch_begin(self, epoch, logs= None):\n        self.ep_start = time.time()\n\n    # Define method runs on the end of each epoch\n    def on_epoch_end(self, epoch, logs= None):\n        ep_end = time.time()\n        duration = ep_end - self.ep_start\n\n        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr = lr\n        acc = logs.get('accuracy')  # get training accuracy\n        v_acc = logs.get('val_accuracy')  # get validation accuracy\n        loss = logs.get('loss')  # get training loss for this epoch\n        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n\n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            monitor = 'accuracy'\n            if epoch == 0:\n                pimprov = 0.0\n            else:\n                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n\n            if acc > self.highest_tracc: # training accuracy improved in the epoch\n                self.highest_tracc = acc # set new highest training accuracy\n                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n                self.count = 0 # set count to 0 since training accuracy improved\n                self.stop_count = 0 # set stop counter to 0\n                if v_loss < self.lowest_vloss:\n                    self.lowest_vloss = v_loss\n                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n\n            else:\n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count >= self.patience - 1: # lr should be adjusted\n                    lr = lr * self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    self.count = 0 # reset the count to 0\n                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n                    self.count = 0 # reset counter\n                    if v_loss < self.lowest_vloss:\n                        self.lowest_vloss = v_loss\n                else:\n                    self.count = self.count + 1 # increment patience counter\n\n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            monitor = 'val_loss'\n            if epoch == 0:\n                pimprov = 0.0\n            else:\n                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n            if v_loss < self.lowest_vloss: # check if the validation loss improved\n                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n                self.count = 0 # reset count since validation loss improved\n                self.stop_count = 0\n                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n            else: # validation loss did not improve\n                if self.count >= self.patience - 1: # need to adjust lr\n                    lr = lr * self.factor # adjust the learning rate\n                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n                    self.count = 0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                else:\n                    self.count = self.count + 1 # increment the patience counter\n                if acc > self.highest_tracc:\n                    self.highest_tracc = acc\n\n        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n        print(msg)\n\n        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print(msg)\n            self.model.stop_training = True # stop training","metadata":{"id":"d5HiN8XDGG60","execution":{"iopub.status.busy":"2022-11-04T20:20:55.061813Z","iopub.execute_input":"2022-11-04T20:20:55.06216Z","iopub.status.idle":"2022-11-04T20:20:55.088504Z","shell.execute_reply.started":"2022-11-04T20:20:55.062128Z","shell.execute_reply":"2022-11-04T20:20:55.087214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to plot history of training","metadata":{"id":"2zwhoj3zGG61"}},{"cell_type":"code","source":"def plot_training(hist):\n    tr_acc = hist.history['accuracy']\n    tr_loss = hist.history['loss']\n    val_acc = hist.history['val_accuracy']\n    val_loss = hist.history['val_loss']\n    index_loss = np.argmin(val_loss)\n    val_lowest = val_loss[index_loss]\n    index_acc = np.argmax(val_acc)\n    acc_highest = val_acc[index_acc]\n\n    plt.figure(figsize= (20, 8))\n    plt.style.use('fivethirtyeight')\n    Epochs = [i+1 for i in range(len(tr_acc))]\n    loss_label = f'best epoch= {str(index_loss + 1)}'\n    acc_label = f'best epoch= {str(index_acc + 1)}'\n    plt.subplot(1, 2, 1)\n    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.tight_layout\n    plt.show()\n","metadata":{"id":"pU3eAW5jGG62","execution":{"iopub.status.busy":"2022-11-04T20:20:56.719148Z","iopub.execute_input":"2022-11-04T20:20:56.720059Z","iopub.status.idle":"2022-11-04T20:20:56.731414Z","shell.execute_reply.started":"2022-11-04T20:20:56.720024Z","shell.execute_reply":"2022-11-04T20:20:56.730369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to create Confusion Matrix","metadata":{"id":"pK6cgu7LGG63"}},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n\tplt.figure(figsize= (10, 10))\n\tplt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n\tplt.title(title)\n\tplt.colorbar()\n\ttick_marks = np.arange(len(classes))\n\tplt.xticks(tick_marks, classes, rotation= 45)\n\tplt.yticks(tick_marks, classes)\n\tif normalize:\n\t\tcm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n\t\tprint('Normalized Confusion Matrix')\n\telse:\n\t\tprint('Confusion Matrix, Without Normalization')\n\tprint(cm)\n\tthresh = cm.max() / 2.\n\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n\t\tplt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n\tplt.tight_layout()\n\tplt.ylabel('True Label')\n\tplt.xlabel('Predicted Label')","metadata":{"id":"_4mPYHnzGG64","execution":{"iopub.status.busy":"2022-11-04T20:20:58.16218Z","iopub.execute_input":"2022-11-04T20:20:58.162832Z","iopub.status.idle":"2022-11-04T20:20:58.17074Z","shell.execute_reply.started":"2022-11-04T20:20:58.162798Z","shell.execute_reply":"2022-11-04T20:20:58.16981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Structure","metadata":{"id":"57eDFl3oGG65"}},{"cell_type":"markdown","source":"Show images sample","metadata":{"id":"2GHNMVrhGG65"}},{"cell_type":"code","source":"# Get Dataframes\ntrain_dir = '../input/tomato-disease-multiple-sources/train'\nvalid_dir = '../input/tomato-disease-multiple-sources/valid'\ntrain_df, valid_df, test_df = create_df(train_dir, valid_dir)\n\n# Get Generators\nbatch_size = 40\ntrain_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n\nshow_images(train_gen)","metadata":{"id":"1xfcIPMeGG65","execution":{"iopub.status.busy":"2022-11-04T20:21:28.646746Z","iopub.execute_input":"2022-11-04T20:21:28.647102Z","iopub.status.idle":"2022-11-04T20:22:52.318389Z","shell.execute_reply.started":"2022-11-04T20:21:28.647071Z","shell.execute_reply":"2022-11-04T20:22:52.317094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create Pre-trained model","metadata":{"id":"3wvOKjeRGG65"}},{"cell_type":"code","source":"# Create Model Structure\nimg_size = (224, 224)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\nclass_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n\n# create pre-trained model\nbase_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n\nmodel = Sequential([\n    base_model,\n    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n    Dropout(rate= 0.45, seed= 123),\n    Dense(class_count, activation= 'softmax')\n])\n\nmodel.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n\nmodel.summary()","metadata":{"id":"e0JI_Zd_GG66","execution":{"iopub.status.busy":"2022-11-04T20:23:02.020791Z","iopub.execute_input":"2022-11-04T20:23:02.021153Z","iopub.status.idle":"2022-11-04T20:23:11.377585Z","shell.execute_reply.started":"2022-11-04T20:23:02.02112Z","shell.execute_reply":"2022-11-04T20:23:11.376554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get custom callbacks parameters","metadata":{"id":"TciwhdM1GG66"}},{"cell_type":"code","source":"batch_size = 40\nepochs = 5\npatience = 1\nstop_patience = 3\nthreshold = 0.9\nfactor = 0.5\nfreeze = False\nbatches = int(np.ceil(len(train_gen.labels) / batch_size))\n\ncallbacks = [MyCallback(model= model, base_model= base_model, patience= patience,\n            stop_patience= stop_patience, threshold= threshold, factor= factor,\n            batches= batches, initial_epoch= 0, epochs= epochs)]","metadata":{"id":"7abvdv7mGG66","execution":{"iopub.status.busy":"2022-11-04T20:23:16.963558Z","iopub.execute_input":"2022-11-04T20:23:16.963925Z","iopub.status.idle":"2022-11-04T20:23:17.245537Z","shell.execute_reply.started":"2022-11-04T20:23:16.963893Z","shell.execute_reply":"2022-11-04T20:23:17.244506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train model","metadata":{"id":"ap89fjdxGG67"}},{"cell_type":"code","source":"history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n                    validation_data= valid_gen, validation_steps= None, shuffle= False,\n                    initial_epoch= 0)","metadata":{"id":"0Uk3BTERGG67","execution":{"iopub.status.busy":"2022-11-04T20:23:18.411041Z","iopub.execute_input":"2022-11-04T20:23:18.411469Z","iopub.status.idle":"2022-11-04T20:56:16.543605Z","shell.execute_reply.started":"2022-11-04T20:23:18.411433Z","shell.execute_reply":"2022-11-04T20:56:16.542601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot training history","metadata":{"id":"dNKq6ebOGG67"}},{"cell_type":"code","source":"plot_training(history)","metadata":{"id":"L0Bj0Sp_GG68","execution":{"iopub.status.busy":"2022-11-04T20:56:22.744706Z","iopub.execute_input":"2022-11-04T20:56:22.745147Z","iopub.status.idle":"2022-11-04T20:56:23.413698Z","shell.execute_reply.started":"2022-11-04T20:56:22.745108Z","shell.execute_reply":"2022-11-04T20:56:23.412817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate model","metadata":{"id":"MySXhfAJGG68"}},{"cell_type":"code","source":"ts_length = len(test_df)\ntest_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\ntest_steps = ts_length // test_batch_size\ntrain_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\nvalid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\ntest_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n\nprint(\"Train Loss: \", train_score[0])\nprint(\"Train Accuracy: \", train_score[1])\nprint('-' * 20)\nprint(\"Validation Loss: \", valid_score[0])\nprint(\"Validation Accuracy: \", valid_score[1])\nprint('-' * 20)\nprint(\"Test Loss: \", test_score[0])\nprint(\"Test Accuracy: \", test_score[1])","metadata":{"id":"wSKDkyXXGG68","execution":{"iopub.status.busy":"2022-11-04T20:56:37.995432Z","iopub.execute_input":"2022-11-04T20:56:37.995796Z","iopub.status.idle":"2022-11-04T21:00:03.316853Z","shell.execute_reply.started":"2022-11-04T20:56:37.995766Z","shell.execute_reply":"2022-11-04T21:00:03.315848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make prediction ","metadata":{"id":"4l-DABtFGG68"}},{"cell_type":"code","source":"preds = model.predict_generator(test_gen)\ny_pred = np.argmax(preds, axis=1)\nprint(y_pred)","metadata":{"id":"GDFj7MZdGG69","execution":{"iopub.status.busy":"2022-11-04T21:00:16.29727Z","iopub.execute_input":"2022-11-04T21:00:16.298008Z","iopub.status.idle":"2022-11-04T21:00:54.197621Z","shell.execute_reply.started":"2022-11-04T21:00:16.297972Z","shell.execute_reply":"2022-11-04T21:00:54.196638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix and Classification Report","metadata":{"id":"aJscUTF6GG69"}},{"cell_type":"code","source":"target_names = ['Bacterial_spot', 'Early_blight', 'Late_blight', 'Leaf_Mold', 'Septoria_leaf_spot', 'Spider_mites', 'Target_Spot', 'Tomato_Yellow_Leaf_Curl_Virus', 'Tomato_mosaic_virus', 'healthy', 'powdery_mildew']\n# Confusion matrix\ncm = confusion_matrix(test_gen.classes, y_pred)\nplot_confusion_matrix(cm= cm, classes= target_names, title = 'Confusion Matrix')\n# Classification report\nprint(classification_report(test_gen.classes, y_pred, target_names= target_names))","metadata":{"id":"tQR-UlD6GG69","execution":{"iopub.status.busy":"2022-11-04T21:01:04.782345Z","iopub.execute_input":"2022-11-04T21:01:04.782697Z","iopub.status.idle":"2022-11-04T21:01:05.658007Z","shell.execute_reply.started":"2022-11-04T21:01:04.782667Z","shell.execute_reply":"2022-11-04T21:01:05.657097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save model","metadata":{"id":"SsIK5v0lGG69"}},{"cell_type":"code","source":"model_name = 'EffecientNetB3'\nsubject = 'Tomato-Disease-Detection'\nacc = test_score[1] * 100\nsave_path = ''\n\nsave_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.h5')\nmodel_save_loc = os.path.join(save_path, save_id)\nmodel.save(model_save_loc)\nprint(f'model was saved as {model_save_loc}')","metadata":{"id":"oy5ShUciGG6-","execution":{"iopub.status.busy":"2022-11-04T21:01:44.046861Z","iopub.execute_input":"2022-11-04T21:01:44.047239Z","iopub.status.idle":"2022-11-04T21:01:45.070329Z","shell.execute_reply.started":"2022-11-04T21:01:44.047205Z","shell.execute_reply":"2022-11-04T21:01:45.069178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save class indices in CSV","metadata":{"id":"q2fsiEtEGG6-"}},{"cell_type":"code","source":"class_dict = train_gen.class_indices\nheight = []\nwidth = []\nfor _ in range(len(class_dict)):\n    height.append(img_size[0])\n    width.append(img_size[1])\n\nIndex_series = pd.Series(list(class_dict.values()), name= 'class_index')\nClass_series = pd.Series(list(class_dict.keys()), name= 'class')\nHeight_series = pd.Series(height, name= 'height')\nWidth_series = pd.Series(width, name= 'width')\nclass_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis= 1)\nsubject = ''\ncsv_name = f'{subject}-class_dict.csv'\ncsv_save_loc = os.path.join(save_path, csv_name)\nclass_df.to_csv(csv_save_loc, index= False)\nprint(f'class csv file was saved as {csv_save_loc}')","metadata":{"id":"UiHQzq8XGG6-","execution":{"iopub.status.busy":"2022-11-04T21:01:52.167161Z","iopub.execute_input":"2022-11-04T21:01:52.167926Z","iopub.status.idle":"2022-11-04T21:01:52.182717Z","shell.execute_reply.started":"2022-11-04T21:01:52.167887Z","shell.execute_reply":"2022-11-04T21:01:52.181421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
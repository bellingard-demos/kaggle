{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Recurrent Neural Networks RNN\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Drawbacks to non-sequential models</p>\n\nIn earlier notebooks, we saw how to use **Bag-of-Words** like approaches to vectorize text. This worked well for simple applications but it does have a number of drawbacks. The main ones include:\n\n* There is no way to handle **Out-of-Vocabulary** (OOV) words. If a new word appears in a later document, it will just be dropped. \n* It creates **sparse matrices** which can be inefficient, although we can overcome this by using a dictionary representation. \n* It isn't able to capture similarity between **synonyms**. \n* Word order is lost so words have **no relationship** to each other. For example, \"man eats bread\" is very different to \"bread eats man\" but they would have the same representations.\n\nWe also came across **static word embeddings**. Whilst this was an improvement to Bag-of-Words because of its ability to capture word meaning, we still had no way to model the sequential nature of language. \n\nThe main challenge with taking word order into account is that sentences can be of **different lengths**. So we need a model that can **automically scale** depending on the sequence length. This is where RNNs come in. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Recurrence</p>\n\n<center>\n<img src=\"https://i.postimg.cc/8cdHgR82/RNN.png\" width=650>\n</center>\n<br>\n\nThe **recurrence** in an RNN refers to the **repeating** design of the model. An input word vector ($x_t$) is passed through a **fully connected layer** to generate some activations ($a_t$), which can be used to make predictions ($y_t$) at that time step. But these activations are also passed to the next time step acting as a form of **memory**, which are combined to make the next prediction ($y_{t+1}$). This design is repeated at each time step, so there can be any number of units in the sequence. \n\nThere are **3 weights matrices**, which are used repeatedly used in each cell. $W_x$ are the weights for the input, $W_y$ are the weights for the output and $W_h$ are the weights for the memory. They are combined using the following equations:\n\n<br>\n$$\n\\large\nh_t = f(W_x x_t + W_h h_{t-1} + b), \\qquad y_t = g(W_y a_t + c)\n$$\n<br>\n\nwhere $f$ and $g$ are **activation functions** like ReLU or tanh and $b$ and $c$ are the **biases**. Note that it is common to set $h_0$ to be the zero vector. And not every task requires a prediction at each time step ($y_t$), some only require one at the end of each sentence. \n\nAs an example, if we pass the sentence \"She has a cat\" through an RNN for part-of-speech tagging; at $t_1$ a prediction is made using \"She\", at $t_2$ \"She has\" is used, similarly at $t_3$ \"She has a\" is used and at $t_4$ the whole sentence is used to make a prediction. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Training RNNs</p>\n\n<center>\n<img src=\"https://i.postimg.cc/pXcZbYpF/BPTT.png\" width=600>\n</center>\n<br>\n\nTraining an RNN is **trickier** than a standard feed-forward neural network because of the time aspect. The algorithm used is called **Backpropagation Through Time** (BPTT). The main idea is that when we compute gradients, we need to **reverse all the arrows** in the computation graph and multiply them using the chain rule. \n\nFor example at the final time step, all the input vectors ($x_t$) and all the hidden states ($h_t$) were used to make this prediction so we have to update all of their corresponding weights. As they share weights matrices, we can add each component together. You can find more details in [this article](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html), but luckily all major deep learning frameworks will **implement this for us**. ","metadata":{"papermill":{"duration":0.011377,"end_time":"2023-02-20T21:43:40.154275","exception":false,"start_time":"2023-02-20T21:43:40.142898","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Language Models\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Word prediction</p>\n\nA **Language model** determines the **probability** of a sequence of words. For example, if you consider the incomplete sentence \"I'm going to walk the ...\", a reasonable guess for the next word would be \"dog\" as opposed to \"ice-cream\" or \"mountain\". We can see that language is **predictable** and some words are more likely to appear than others. \n\nMathematically, we can use the **multiplication rule** to represent the probability of a sequence as a product of conditional probabilities:\n\n<br>\n$$\n\\large\n\\begin{align*}\n\\mathbb{P}(w_1,...,w_n) &= \\prod_{i=1}^{n} \\mathbb{P} (w_i | w_1,...,w_{i-1}) \\\\\n& = \\mathbb{P}(w_1) \\mathbb{P}(w_2|w_1) ... \\mathbb{P}(w_n|w_1,...,w_{n-1})\n\\end{align*}\n$$\n<br>\n\nThis is a **difficult task** because it is impossible to get a training corpus with every combination of words in a sequence. We will see that we can use an RNN to **train** a language model by using a clever trick. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Training an RNN for language modelling</p>\n\nTo train an RNN for language modelling, we start by **segmenting** a corpus into chunks, sentences or paragraphs to feed into the model. The longer the sequence length the more **computationally expensive** it is to train but the better the model is at capturing **long range dependencies**. \n\nWe then use a **self-supervised** learning approach. At every time step, we get the RNN to try to **predict the next word** in the sentence in the training corpus. What is great is that we don't need to label the data, as we use the words as the labels themselves. \n\nThe outputs $y_t$ are **probability distributions** over the whole vocabulary. They assign a probability to each word, which represents how likely that word is to appear next given the previous words. A **cross-entropy loss** can be used to measure how good each prediction is. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Text generation</p>\n\nTo **generate new text** after training, we modify the RNN to **sample** the output probability distribution at each time step and feed this to be **input** word for following time step, that is $x_{t+1} \\sim y_{t}$. \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/0N0kKYXy/RNN-for-LM.jpg\" width=600>\n</center>\n<br>\n\nSo given an input word (this can fixed or randomly sampled), we can generated any amount of text we like. It won't be perfect or grammatically correct by default but it will produce text that has a **similar distribution** to the one it was trained on.\n\nFor example, if we use a training corpus comprising of the Lord of the Rings books, then the generated text will look somewhat similar to languge in these books. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Evaluating language models</p>\n\nIf we **measure** the output **probabilities** of a language model on a **test set**, we expect better models to produce **higher** probabilities for the correct words. \n\nA common **metric** to evaluate language models is called **Perplexity** and is defined as:\n\n<br>\n$$\n\\large\n\\text{Perplexity} = \\left(\\prod_{i=1}^{T} \\mathbb{P}_{LM}(w_i|w_1,...,w_{i-1})\\right)^{-1/T} = \\sqrt[T]{\\prod_{i=1}^{T} \\frac{1}{\\mathbb{P}_{LM}(w_i|w_1,...,w_{i-1})}}\n$$\n<br>\n\nBecause of the inverse probabilities, the **lower** the perplexity score the **better** and the more natural the language produced. The T-th root is there to **control** for different test lengths.\n\nNote that **humans** have a certain perplexity score too, depending on the corpus, so getting a model's perplexity score too low below that **benchmark** may also produce unnatural text.","metadata":{"papermill":{"duration":0.009757,"end_time":"2023-02-20T21:43:40.17402","exception":false,"start_time":"2023-02-20T21:43:40.164263","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# RNN variants\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\"> Drawbracks of simple RNNs</p>\n\nWe've now seen the architecture of a **standard** recurrent neural network. Whilst they are big **improvement** in terms of dealing with word order, they do have some shortcomings in their current state. These include:\n\n* Struggling to capture **long range dependencies** because the signal gets **weakens** over time. Simple RNNs have no way of deciding what information is important to 'remember' and what can be 'forgetten'.\n* **Vanishing and/or exploding gradients** can occur during training because of the many components affecting each matrix of weights.\n\nThere are some ways to **dampen** the affects of these problems, like **gradient clipping**, **batch normalization**, **weight initialization** etc. However, better **cell architectures** have been found, which avoid these problems and produce better results. These include **GRU** and **LSTM**, which we will discuss now. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">GRU</p>\n\n<center>\n<img src=\"https://i.postimg.cc/RV921t3J/gru-cell.png\" width=600>\n</center>\n<br>\n\nGRU stands for **Gated Recurrent Unit** and is a variant of a RNN where the cell/unit has the ability to **forget** information it deems irrelevant. As a result, it tends to do a better job at capturing **long range dependencies** at the cost of increasing the number of **parameters** in the model. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">LSTM</p>\n\n<center>\n<img src=\"https://i.postimg.cc/d1rSrQw9/lstm-cell.jpg\" width=500>\n</center>\n<br>\n\nLSTM stands for **Long Short Term Memory** and is similar to GRU but has more parameters and can manage more complex information transfer. In addition to the normal hidden state that gets passed between units, LSTM has an additional **cell state** that captures **long term memory**. The affect is that the model can make predictions using both short term and long term memory. \n\nLSTM is more complicated than GRU, so it is able to reach **better performance** on some tasks but again at the cost of increasing the number of **parameters** in the model and therefore taking **longer to train**. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Deep RNNs</p>\n\n<center>\n<img src=\"https://i.postimg.cc/C5LhQQv9/deep-rnn.png\" width=250>\n</center>\n<br>\n\nThere are several ways to **combine** recurrent neural networks. One way is to **stack** multiple RNNs on top of each other. We can do this since each RNN takes in a sequence and outputs another sequence of the same length. \n\nJust like adding another **layer** in a feed-forward neural network, stacking RNNs allows the model to learn more useful **representations** of the data. This can be useful for very **complex** tasks that consist a large number **interactions** in the data, for example machine translation. Again, the thing to keep in mind is that this takes much **longer** to train the model. \n\n<br>\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Bidirectional RNNs</p>\n\nThere are some tasks where it makes sense to use not only previous words in the sentence but also **future words** in the sentence. For example, the word \"India\" in the two sentences \"India has a population of 1.4 billion people\" and \"India got her bag ready to go to school\" has **different meaning** based on the words that come **after** it. \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/XNCkNNg2/bidirectional-rnns.png\" width=500>\n</center>\n<br>\n\n**Bidirectional RNNs** work by adding a **second** RNN that processes the text in **reverse order**. This ensures that the model uses the **whole sentence** each time it makes a prediction. \n\nNote that the cells in this model can be either **simple RNN, GRU or LSTM units** depending on your preference. It is even possible to **stack** bidirectional RNNs to create **deep bidirectional RNNs**.","metadata":{"papermill":{"duration":0.009701,"end_time":"2023-02-20T21:43:40.19359","exception":false,"start_time":"2023-02-20T21:43:40.183889","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Application\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\"> Part-of-Speech Tagging</p>\n\nWe're going to build a **bidirectional LSTM** to perform **Part-of-Speech (PoS) tagging**. We'll use datasets from nltk to train our model\n\n<br>\n\n**Import libraries**","metadata":{"papermill":{"duration":0.00974,"end_time":"2023-02-20T21:43:40.213234","exception":false,"start_time":"2023-02-20T21:43:40.203494","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.6)\nimport requests\nimport nltk\nfrom nltk.corpus import treebank, brown, conll2000\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:43:40.235536Z","iopub.status.busy":"2023-02-20T21:43:40.234685Z","iopub.status.idle":"2023-02-20T21:43:46.988087Z","shell.execute_reply":"2023-02-20T21:43:46.987098Z"},"papermill":{"duration":6.767505,"end_time":"2023-02-20T21:43:46.99062","exception":false,"start_time":"2023-02-20T21:43:40.223115","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load data**\n\n<br>\n\nWe'll use 3 PoS datasets together. Because they all have different tagging schemes, we also download the **universal tagging schema** to ensure they are all tagged **consistently**. Note that this **simplifies** some of the tags, for example example proper noun is converted to noun. ","metadata":{"papermill":{"duration":0.009877,"end_time":"2023-02-20T21:43:47.010798","exception":false,"start_time":"2023-02-20T21:43:47.000921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# PoS datasets\nnltk.download('treebank')\nnltk.download('brown')\nnltk.download('conll2000')\n\n# Use universal tagging system\nnltk.download('universal_tagset')","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-02-20T21:43:47.032897Z","iopub.status.busy":"2023-02-20T21:43:47.031716Z","iopub.status.idle":"2023-02-20T21:43:47.224885Z","shell.execute_reply":"2023-02-20T21:43:47.22383Z"},"papermill":{"duration":0.208074,"end_time":"2023-02-20T21:43:47.228874","exception":false,"start_time":"2023-02-20T21:43:47.0208","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve all PoS-tagged sentences and join them in one list\ntagged_sentences = treebank.tagged_sents(tagset='universal') + brown.tagged_sents(tagset='universal') + conll2000.tagged_sents(tagset='universal')\n\n# Example\nprint(\"Dataset size:\", len(tagged_sentences))\nprint(tagged_sentences[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:43:47.250801Z","iopub.status.busy":"2023-02-20T21:43:47.250516Z","iopub.status.idle":"2023-02-20T21:44:03.731511Z","shell.execute_reply":"2023-02-20T21:44:03.73041Z"},"papermill":{"duration":16.494292,"end_time":"2023-02-20T21:44:03.733657","exception":false,"start_time":"2023-02-20T21:43:47.239365","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Separate features and labels**","metadata":{"papermill":{"duration":0.01016,"end_time":"2023-02-20T21:44:03.75432","exception":false,"start_time":"2023-02-20T21:44:03.74416","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initalize\nX, y = [], []\n\n# Split sentences into words and tags\nfor s in tagged_sentences:\n    sentence, tags = zip(*s)\n    X.append(list(sentence))\n    y.append(list(tags))\n    \n# Example\nprint(X[0])\nprint(y[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:03.788614Z","iopub.status.busy":"2023-02-20T21:44:03.788146Z","iopub.status.idle":"2023-02-20T21:44:10.0892Z","shell.execute_reply":"2023-02-20T21:44:10.088029Z"},"papermill":{"duration":6.324383,"end_time":"2023-02-20T21:44:10.092052","exception":false,"start_time":"2023-02-20T21:44:03.767669","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train-valid-test split**","metadata":{"papermill":{"duration":0.010196,"end_time":"2023-02-20T21:44:10.113696","exception":false,"start_time":"2023-02-20T21:44:10.1035","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create train, validation and test sets\nX_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0)\nX_train, X_valid, y_train, y_valid = train_test_split(X_tr, y_tr, test_size=0.25, shuffle=True, random_state=0) # 0.25 * 0.8 = 0.2\n\n# Print shapes\nprint(\"Train size:\", len(X_train))\nprint(\"Valid size:\", len(X_valid))\nprint(\"Test size:\", len(X_test))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:10.136143Z","iopub.status.busy":"2023-02-20T21:44:10.135252Z","iopub.status.idle":"2023-02-20T21:44:10.200843Z","shell.execute_reply":"2023-02-20T21:44:10.199112Z"},"papermill":{"duration":0.079429,"end_time":"2023-02-20T21:44:10.203479","exception":false,"start_time":"2023-02-20T21:44:10.12405","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokenizers**\n\nWe need to tokenize both the features and the labels using **two different tokenizers** as they are both sequences.","metadata":{"papermill":{"duration":0.010294,"end_time":"2023-02-20T21:44:10.224632","exception":false,"start_time":"2023-02-20T21:44:10.214338","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define tokenizers\nword_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\ntag_tokenizer = keras.preprocessing.text.Tokenizer()\n\n# Fit tokenizers\nword_tokenizer.fit_on_texts(X_train)\ntag_tokenizer.fit_on_texts(y_train)\n\n# Print vocabulary sizes\nprint(\"Word vocabulary size:\", len(word_tokenizer.word_index))\nprint(\"Tag vocabulary size:\", len(tag_tokenizer.word_index))\n\n# Print set of tags\nprint(\"\\nPossible tags:\", tag_tokenizer.word_index)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:10.247278Z","iopub.status.busy":"2023-02-20T21:44:10.24633Z","iopub.status.idle":"2023-02-20T21:44:11.505165Z","shell.execute_reply":"2023-02-20T21:44:11.50263Z"},"papermill":{"duration":1.272957,"end_time":"2023-02-20T21:44:11.507907","exception":false,"start_time":"2023-02-20T21:44:10.23495","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map tokens to integer ids\nX_train_ids = word_tokenizer.texts_to_sequences(X_train)\nX_valid_ids = word_tokenizer.texts_to_sequences(X_valid)\nX_test_ids = word_tokenizer.texts_to_sequences(X_test)\n\ny_train_ids = tag_tokenizer.texts_to_sequences(y_train)\ny_valid_ids = tag_tokenizer.texts_to_sequences(y_valid)\ny_test_ids = tag_tokenizer.texts_to_sequences(y_test)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:11.531227Z","iopub.status.busy":"2023-02-20T21:44:11.530898Z","iopub.status.idle":"2023-02-20T21:44:13.322905Z","shell.execute_reply":"2023-02-20T21:44:13.321853Z"},"papermill":{"duration":1.806274,"end_time":"2023-02-20T21:44:13.325533","exception":false,"start_time":"2023-02-20T21:44:11.519259","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Padding**\n\n<br>\n\nPadding is a way of making all input sequences the **same length**, by filling shorter sequences with 0's and truncating longer sequences. Although is isn't necessarily required for RNNs, it usually **speeds up training** because it is easier to **batch** training examples together. ","metadata":{"papermill":{"duration":0.010552,"end_time":"2023-02-20T21:44:13.34717","exception":false,"start_time":"2023-02-20T21:44:13.336618","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.histplot([len(X_train_ids[i]) for i in range(len(X_train_ids))], binwidth=1, kde=True)\nplt.title('Sentence lengths')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:13.370643Z","iopub.status.busy":"2023-02-20T21:44:13.369758Z","iopub.status.idle":"2023-02-20T21:44:14.168119Z","shell.execute_reply":"2023-02-20T21:44:14.167199Z"},"papermill":{"duration":0.812726,"end_time":"2023-02-20T21:44:14.170593","exception":false,"start_time":"2023-02-20T21:44:13.357867","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since most sentences have less than 75 words, we'll **truncate** longer sentences to only include the first 75 words. A more sophisticated approach would be to truncate on a batch-by-batch basis to reduce the number of unnecessary 0's but this will still work well.","metadata":{"papermill":{"duration":0.010879,"end_time":"2023-02-20T21:44:14.192789","exception":false,"start_time":"2023-02-20T21:44:14.18191","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Maximum length of sequences\nmax_length = 75\n\n# Pad sequences\nX_train_pad = keras.preprocessing.sequence.pad_sequences(X_train_ids, padding='post', maxlen=max_length)\nX_valid_pad = keras.preprocessing.sequence.pad_sequences(X_valid_ids, padding='post', maxlen=max_length)\nX_test_pad = keras.preprocessing.sequence.pad_sequences(X_test_ids, padding='post', maxlen=max_length)\n\ny_train_pad = keras.preprocessing.sequence.pad_sequences(y_train_ids, padding='post', maxlen=max_length)\ny_valid_pad = keras.preprocessing.sequence.pad_sequences(y_valid_ids, padding='post', maxlen=max_length)\ny_test_pad = keras.preprocessing.sequence.pad_sequences(y_test_ids, padding='post', maxlen=max_length)\n\n# Example\nprint(X_train_pad[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:14.215758Z","iopub.status.busy":"2023-02-20T21:44:14.215448Z","iopub.status.idle":"2023-02-20T21:44:14.666998Z","shell.execute_reply":"2023-02-20T21:44:14.665897Z"},"papermill":{"duration":0.465906,"end_time":"2023-02-20T21:44:14.669444","exception":false,"start_time":"2023-02-20T21:44:14.203538","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build model**\n\n<br>\n\nOur model will consist of an **embedding layer** followed by a **bidirectional LSTM**. \n\nNote that `mask_zero = True` tells the embedding layer to **ignore padding values** so the model won't make predictions at these time steps. This will make it easier to **properly evaluate** our model as we only care about its performance on non-padded values.\n\nFurthermore, since this is a sequence labelling task we want our model to make a **prediction at each time step**. Setting `return_sequences = True` accomplishes that. ","metadata":{"papermill":{"duration":0.010948,"end_time":"2023-02-20T21:44:14.691566","exception":false,"start_time":"2023-02-20T21:44:14.680618","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Input/output size (+ 1 for padding token)\nnum_tokens = len(word_tokenizer.word_index) + 1\nnum_classes = len(tag_tokenizer.word_index) + 1\n\n# Embedding dimension\nembedding_dim = 128","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:14.715683Z","iopub.status.busy":"2023-02-20T21:44:14.71422Z","iopub.status.idle":"2023-02-20T21:44:14.719426Z","shell.execute_reply":"2023-02-20T21:44:14.718566Z"},"papermill":{"duration":0.019015,"end_time":"2023-02-20T21:44:14.721361","exception":false,"start_time":"2023-02-20T21:44:14.702346","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build model\nmodel = keras.Sequential([\n\n    # Map integer ids to trainable word vectors\n    layers.Embedding(input_dim = num_tokens, \n                           output_dim = embedding_dim, \n                           input_length = max_length,\n                           mask_zero = True),\n    \n    # Bidirectional LSTM\n    layers.Bidirectional(layers.LSTM(128, return_sequences = True, \n                           kernel_initializer = tf.keras.initializers.random_normal(seed=0))),\n    \n    # Output layer\n    layers.Dense(num_classes, activation='softmax', \n                           kernel_initializer=tf.keras.initializers.random_normal(seed=0))\n])\n\n# Define optimizer, loss function and accuracy metric\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['sparse_categorical_accuracy'])\n\n# Model summary\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-02-20T21:44:14.745264Z","iopub.status.busy":"2023-02-20T21:44:14.744474Z","iopub.status.idle":"2023-02-20T21:44:19.622316Z","shell.execute_reply":"2023-02-20T21:44:19.62079Z"},"papermill":{"duration":4.89249,"end_time":"2023-02-20T21:44:19.625113","exception":false,"start_time":"2023-02-20T21:44:14.732623","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**\n\n<br>\n\nWe'll use **early stopping** to prevent overfitting to the train set.","metadata":{"papermill":{"duration":0.01108,"end_time":"2023-02-20T21:44:19.64837","exception":false,"start_time":"2023-02-20T21:44:19.63729","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Early stopping\nearly_stopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)\n\n# Train model\nhistory = model.fit(\n    X_train_pad, y_train_pad,\n    validation_data = (X_valid_pad, y_valid_pad),\n    epochs = 20,\n    batch_size = 128,\n    callbacks = [early_stopping],\n    verbose = True\n)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:44:19.672129Z","iopub.status.busy":"2023-02-20T21:44:19.671812Z","iopub.status.idle":"2023-02-20T21:45:29.213318Z","shell.execute_reply":"2023-02-20T21:45:29.210279Z"},"papermill":{"duration":69.556226,"end_time":"2023-02-20T21:45:29.215812","exception":false,"start_time":"2023-02-20T21:44:19.659586","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model reaches a reasonably **high accuracy** very **quickly**. After only a few epochs, early stopping kicks in.","metadata":{"papermill":{"duration":0.05747,"end_time":"2023-02-20T21:45:29.331674","exception":false,"start_time":"2023-02-20T21:45:29.274204","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Evaluate on test set**","metadata":{"papermill":{"duration":0.05736,"end_time":"2023-02-20T21:45:29.446181","exception":false,"start_time":"2023-02-20T21:45:29.388821","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Evaluate model\nmodel.evaluate(X_test_pad, y_test_pad)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:45:29.562865Z","iopub.status.busy":"2023-02-20T21:45:29.562512Z","iopub.status.idle":"2023-02-20T21:45:50.053147Z","shell.execute_reply":"2023-02-20T21:45:50.052186Z"},"papermill":{"duration":20.551779,"end_time":"2023-02-20T21:45:50.05529","exception":false,"start_time":"2023-02-20T21:45:29.503511","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tag new samples**\n\n<br>\n\nThe following function takes a list of sentences as input and returns the models **predicted tags** for each word.","metadata":{"papermill":{"duration":0.060135,"end_time":"2023-02-20T21:45:50.176836","exception":false,"start_time":"2023-02-20T21:45:50.116701","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Tag new sentences\ndef tag_new_sentences(sentences):\n    # Tokenize new sentences\n    X_new_ids = word_tokenizer.texts_to_sequences(sentences)\n    \n    # Pad sequences\n    X_new_pad = keras.preprocessing.sequence.pad_sequences(X_new_ids, padding = 'post', maxlen = max_length)\n    \n    # Make predictions\n    X_new_preds = model.predict(X_new_pad)\n    \n    # Retrieve most likely tag for each word\n    sentence_tags = []\n    for i, preds in enumerate(X_new_preds):\n        # Extract tags for only non-padded tokens\n        tags_seq = [np.argmax(p) for p in preds[:len(X_new_ids[i])]]\n        \n        # Convert ids back to tokens\n        words = [word_tokenizer.index_word[w] for w in X_new_ids[i]]\n        tags = [tag_tokenizer.index_word[t] for t in tags_seq]\n        \n        # zip words and tags together\n        sentence_tags.append(list(zip(words, tags)))\n\n    return sentence_tags","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:45:50.301572Z","iopub.status.busy":"2023-02-20T21:45:50.299828Z","iopub.status.idle":"2023-02-20T21:45:50.308008Z","shell.execute_reply":"2023-02-20T21:45:50.307087Z"},"papermill":{"duration":0.072267,"end_time":"2023-02-20T21:45:50.310024","exception":false,"start_time":"2023-02-20T21:45:50.237757","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example sentences\nex = [\n    \"Newly discovered green comet comes close to Earth.\",\n    \"Nasa's Mars rover Perseverance completes rock depot.\",\n]\n\n# Tag sentences\nex_tagged = tag_new_sentences(ex)\n\n# Print results\nprint(ex_tagged[0])\nprint('\\n',ex_tagged[1])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:45:50.43282Z","iopub.status.busy":"2023-02-20T21:45:50.431087Z","iopub.status.idle":"2023-02-20T21:45:52.801967Z","shell.execute_reply":"2023-02-20T21:45:52.801025Z"},"papermill":{"duration":2.434741,"end_time":"2023-02-20T21:45:52.804588","exception":false,"start_time":"2023-02-20T21:45:50.369847","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nowadays, state-of-the-art PoS taggers use **transformer** models and much **more data** but it's still impressive that we very good results using a recurrent neural network.","metadata":{"papermill":{"duration":0.059952,"end_time":"2023-02-20T21:45:52.925453","exception":false,"start_time":"2023-02-20T21:45:52.865501","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Language Modelling</p>\n\nWe're now going to build a language model to **generate text like William Shakespear**. To train this model, we'll use a **text file** that contains the scripts for all of Shakespear's plays. ","metadata":{"papermill":{"duration":0.060883,"end_time":"2023-02-20T21:45:53.047355","exception":false,"start_time":"2023-02-20T21:45:52.986472","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load text file\ntext = open('/kaggle/input/shakespeare-text/text.txt', 'r').read()\n\n# Preview text\nprint(text[:300])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:45:53.170567Z","iopub.status.busy":"2023-02-20T21:45:53.170177Z","iopub.status.idle":"2023-02-20T21:45:53.196166Z","shell.execute_reply":"2023-02-20T21:45:53.195232Z"},"papermill":{"duration":0.089296,"end_time":"2023-02-20T21:45:53.198243","exception":false,"start_time":"2023-02-20T21:45:53.108947","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're actually going to build a **character level** model because this will be **easier to train** since the target space is significantly smaller compared to a word prediction model.\n\nA downside to predicting characters is the additional challenge of **learning to spell** words correctly. It does make it easier to deal with out of vocabulary words though so there is a trade-off.","metadata":{"papermill":{"duration":0.059912,"end_time":"2023-02-20T21:45:53.318636","exception":false,"start_time":"2023-02-20T21:45:53.258724","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Tokenizer**\n\n<br>\n\nNotice how some **special characters** get included in the vocabulary but all letters has been **lower cased**.","metadata":{"papermill":{"duration":0.059786,"end_time":"2023-02-20T21:45:53.438673","exception":false,"start_time":"2023-02-20T21:45:53.378887","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Tokenize individual letters\ntokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n\n# Fit tokenizer\ntokenizer.fit_on_texts([text])\n\n# Print vocabulary size\nprint(\"Vocabulary size:\", len(tokenizer.word_index))\n\n# Print vocabulary\nprint(\"\\nVocabulary:\", tokenizer.word_index)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:45:53.561875Z","iopub.status.busy":"2023-02-20T21:45:53.560501Z","iopub.status.idle":"2023-02-20T21:45:53.841351Z","shell.execute_reply":"2023-02-20T21:45:53.840163Z"},"papermill":{"duration":0.347668,"end_time":"2023-02-20T21:45:53.846587","exception":false,"start_time":"2023-02-20T21:45:53.498919","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vectorize characters to intger ids\nids = tokenizer.texts_to_sequences([text])[0]\n\n# Print length of sequence\nprint(\"Number of tokens:\", len(ids))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:45:54.045612Z","iopub.status.busy":"2023-02-20T21:45:54.045207Z","iopub.status.idle":"2023-02-20T21:45:54.384402Z","shell.execute_reply":"2023-02-20T21:45:54.382923Z"},"papermill":{"duration":0.437054,"end_time":"2023-02-20T21:45:54.386519","exception":false,"start_time":"2023-02-20T21:45:53.949465","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Chunking**\n\n<br>\n\nWe now need to **segment** the sequence of ids into **chunks** that will be our training examples. We do this by converting into a **tensorflow dataset** object and applying a **windowing function**. \n\nWe need to **add 1** to the input sequence length because we are going to use the **next character** as the label for the current character. This will ensure we have the same input and target sequence lenghts. The `shift` parameter determines how many characters we shift to the right before creating a new chunk, i.e. it controls how much the chunks **overlap** (if at all). ","metadata":{"papermill":{"duration":0.061727,"end_time":"2023-02-20T21:45:54.509991","exception":false,"start_time":"2023-02-20T21:45:54.448264","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Tensorflow dataset\nslices = tf.data.Dataset.from_tensor_slices(ids)\n\n# Create chunks\ninput_timesteps = 100\nwindow_size = input_timesteps + 1\nwindows = slices.window(window_size, shift=50, drop_remainder=True)  # bigger shift reduces number of chunks\n\n# Convert window objects to tensors\ndataset = windows.flat_map(lambda window: window.batch(window_size))\n\n# Divide dataset into batches for training\nbatch_size = 32\nbatches = dataset.shuffle(10000).batch(batch_size)\n\n# Print example batches\nfor b in batches.take(2):\n    print(b)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:45:54.635099Z","iopub.status.busy":"2023-02-20T21:45:54.634703Z","iopub.status.idle":"2023-02-20T21:45:59.747504Z","shell.execute_reply":"2023-02-20T21:45:59.745024Z"},"papermill":{"duration":5.184406,"end_time":"2023-02-20T21:45:59.75713","exception":false,"start_time":"2023-02-20T21:45:54.572724","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features and labels**\n\n<br>\n\nNow that we have our chunks, we need to **split** them into training features and labels. Remember that we are using a **self-supervised** approach, so the label is always the character that comes after the current one. We are trying to teach our model to predict the **next character** in the sequence.","metadata":{"papermill":{"duration":0.093933,"end_time":"2023-02-20T21:45:59.957769","exception":false,"start_time":"2023-02-20T21:45:59.863836","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Split features and labels\nXy_batches = batches.map(lambda batch: (batch[:, :-1], batch[:, 1:]))\n\n# Print first training example\nfor b in Xy_batches.take(1):\n    print(\"X1 length: \", len(b[0][0].numpy()))\n    print(\"X1: \", b[0][0].numpy())\n    print(\"\\ny1 length: \", len(b[1][0].numpy()))\n    print(\"y1: \", b[1][0].numpy())","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:46:00.147264Z","iopub.status.busy":"2023-02-20T21:46:00.146832Z","iopub.status.idle":"2023-02-20T21:46:02.6123Z","shell.execute_reply":"2023-02-20T21:46:02.611283Z"},"papermill":{"duration":2.563001,"end_time":"2023-02-20T21:46:02.614763","exception":false,"start_time":"2023-02-20T21:46:00.051762","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Vectorize inputs**\n\n<br>\n\nThe last step is map our input integer ids to **vectors**. Since each token is just a character, it doesn't make much sense to use embeddings as there is no sense of character meaning. Instead, we'll just use **one-hot encoding** to be able to pass it to a RNN model. ","metadata":{"papermill":{"duration":0.06022,"end_time":"2023-02-20T21:46:02.790521","exception":false,"start_time":"2023-02-20T21:46:02.730301","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Number of tokens (+1 of oov tokens)\nnum_tokens = len(tokenizer.word_index) + 1\n\n# One-hot encode the input sequences\nXy_batches = Xy_batches.map(lambda inputs, labels: (tf.one_hot(tf.cast(inputs, tf.int32), depth=num_tokens), labels))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:46:02.912472Z","iopub.status.busy":"2023-02-20T21:46:02.912115Z","iopub.status.idle":"2023-02-20T21:46:02.942985Z","shell.execute_reply":"2023-02-20T21:46:02.942088Z"},"papermill":{"duration":0.094448,"end_time":"2023-02-20T21:46:02.945086","exception":false,"start_time":"2023-02-20T21:46:02.850638","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prefetching **loads** the data for the **next batch**, while the model is training on the current batch. It can **speed** up the training process.","metadata":{"papermill":{"duration":0.060612,"end_time":"2023-02-20T21:46:03.066611","exception":false,"start_time":"2023-02-20T21:46:03.005999","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Prefetch data\nXy_batches = Xy_batches.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:46:03.190732Z","iopub.status.busy":"2023-02-20T21:46:03.190373Z","iopub.status.idle":"2023-02-20T21:46:03.195901Z","shell.execute_reply":"2023-02-20T21:46:03.194816Z"},"papermill":{"duration":0.069274,"end_time":"2023-02-20T21:46:03.198156","exception":false,"start_time":"2023-02-20T21:46:03.128882","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build model**\n\n<br>\n\nWe're now ready to build our model. This time there will be **no imbedding layer** so the input vectors will be fed straight through to **2 stacked LSTM layers** followed by a **dense output layer**. As a result, the **number of parameters** in the model is much **lower** than before.\n\nNote that `recurrent_dropout` is added, which applies dropout **horizontally** across time steps. This will **prevent** the model from **memorizing** long strings of text in the training set. ","metadata":{"papermill":{"duration":0.061144,"end_time":"2023-02-20T21:46:03.320063","exception":false,"start_time":"2023-02-20T21:46:03.258919","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build model\nmodel = keras.Sequential([\n\n    # LSTM layers\n    layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2),\n    layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2),\n    \n    # Output layer\n    layers.Dense(num_tokens, activation='softmax')\n])\n\n# Define optimizer and loss function\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy')\n\n# Model summary\nmodel.summary()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:46:03.442904Z","iopub.status.busy":"2023-02-20T21:46:03.442559Z","iopub.status.idle":"2023-02-20T21:46:03.654704Z","shell.execute_reply":"2023-02-20T21:46:03.653304Z"},"papermill":{"duration":0.276605,"end_time":"2023-02-20T21:46:03.657368","exception":false,"start_time":"2023-02-20T21:46:03.380763","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**\n\n<br>\n\nThis might take some time so we will **save the model** after **every epoch** if it reduces the loss. We'll also include **early stopping** and **learning rate scheduler** callbacks. ","metadata":{"papermill":{"duration":0.064249,"end_time":"2023-02-20T21:46:03.786055","exception":false,"start_time":"2023-02-20T21:46:03.721806","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Number of epochs\nnum_epochs = 30\n\n# Early stopping\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience = 3,\n    min_delta = 0.0001,\n    monitor = 'loss',\n    restore_best_weights = True,\n)\n\n# Learning rate scheduler\nschedule = keras.optimizers.schedules.CosineDecay(initial_learning_rate = 0.002, decay_steps = num_epochs, alpha = 0.0001)\nscheduler = keras.callbacks.LearningRateScheduler(schedule, verbose = 0)\n\n# Save best model at every epoch\ncheckpoint = keras.callbacks.ModelCheckpoint(\n    'stacked_lstm',\n    monitor = 'loss',\n    verbose = 1,\n    save_best_only = True,\n    save_weights_only = False,\n    mode = 'auto',\n)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:46:03.909368Z","iopub.status.busy":"2023-02-20T21:46:03.908997Z","iopub.status.idle":"2023-02-20T21:46:03.915529Z","shell.execute_reply":"2023-02-20T21:46:03.91448Z"},"papermill":{"duration":0.070497,"end_time":"2023-02-20T21:46:03.917609","exception":false,"start_time":"2023-02-20T21:46:03.847112","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\nhistory = model.fit(\n    Xy_batches,\n    epochs = num_epochs, \n    callbacks = [early_stopping, scheduler, checkpoint]\n)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T21:46:04.055448Z","iopub.status.busy":"2023-02-20T21:46:04.054484Z","iopub.status.idle":"2023-02-21T02:04:32.398308Z","shell.execute_reply":"2023-02-21T02:04:32.397291Z"},"papermill":{"duration":15508.419839,"end_time":"2023-02-21T02:04:32.400864","exception":false,"start_time":"2023-02-20T21:46:03.981025","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load best model after training completed\nmodel = keras.models.load_model('stacked_lstm')","metadata":{"execution":{"iopub.execute_input":"2023-02-21T02:04:34.940273Z","iopub.status.busy":"2023-02-21T02:04:34.939781Z","iopub.status.idle":"2023-02-21T02:04:36.771649Z","shell.execute_reply":"2023-02-21T02:04:36.77059Z"},"papermill":{"duration":3.128176,"end_time":"2023-02-21T02:04:36.774179","exception":false,"start_time":"2023-02-21T02:04:33.646003","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generate new text**\n\n<br>\n\nWe can generate new text using the following function. It takes in a portion of text to **kickstart** the model, which then makes predictions up to a **pre-defined maximum** number of characters. \n\nWhen sampling from the output distribution, we can adjust it using the **temperature parameter**. The **higher** the temperature the more **random** the predictions, whereas the **lower** the temperature the more **sharpened** the original distribution gets.","metadata":{"papermill":{"duration":1.244124,"end_time":"2023-02-21T02:04:39.160526","exception":false,"start_time":"2023-02-21T02:04:37.916402","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def generate_text(model, tokenizer, seed_text, num_chars = 200, temperature = 1):\n    \n    text = seed_text\n    \n    for _ in range(num_chars):\n        \n        # Use 'input_timesteps' most recent characters as input for the model\n        X_new = np.array(tokenizer.texts_to_sequences([text[-input_timesteps:]]))\n        \n        # One-hot encode input\n        X_new = tf.one_hot(X_new, num_tokens)\n        \n        # Predict probability distribution for next character\n        preds = model.predict(X_new)[0, -1:, :]\n        \n        # Adjust probability distribution using temperature\n        preds = tf.math.log(preds) / temperature\n\n        # Sample next character\n        next_char_id = tf.random.categorical(preds, num_samples=1)\n        next_char = tokenizer.sequences_to_texts(next_char_id.numpy())[0]\n\n        # Add character to running text\n        text += next_char\n\n    return text","metadata":{"execution":{"iopub.execute_input":"2023-02-21T02:04:41.564536Z","iopub.status.busy":"2023-02-21T02:04:41.564172Z","iopub.status.idle":"2023-02-21T02:04:41.572018Z","shell.execute_reply":"2023-02-21T02:04:41.571071Z"},"papermill":{"duration":1.24776,"end_time":"2023-02-21T02:04:41.574005","exception":false,"start_time":"2023-02-21T02:04:40.326245","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nprint(generate_text(model, tokenizer, \"Romeo took a plane to visit his uncle\", num_chars = 200, temperature = 0.2))","metadata":{"execution":{"iopub.execute_input":"2023-02-21T02:04:43.981679Z","iopub.status.busy":"2023-02-21T02:04:43.981036Z","iopub.status.idle":"2023-02-21T02:05:01.210462Z","shell.execute_reply":"2023-02-21T02:05:01.209473Z"},"papermill":{"duration":18.498006,"end_time":"2023-02-21T02:05:01.212633","exception":false,"start_time":"2023-02-21T02:04:42.714627","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(generate_text(model, tokenizer, \"Juliet was enjoying her new job as a journalist\", num_chars = 200, temperature = 0.5))","metadata":{"execution":{"iopub.execute_input":"2023-02-21T02:05:03.604559Z","iopub.status.busy":"2023-02-21T02:05:03.604202Z","iopub.status.idle":"2023-02-21T02:05:19.443309Z","shell.execute_reply":"2023-02-21T02:05:19.44237Z"},"papermill":{"duration":16.977556,"end_time":"2023-02-21T02:05:19.445414","exception":false,"start_time":"2023-02-21T02:05:02.467858","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(generate_text(model, tokenizer, \"Macbeth put down the knife and walked away\", num_chars = 200, temperature = 1))","metadata":{"execution":{"iopub.execute_input":"2023-02-21T02:05:21.820915Z","iopub.status.busy":"2023-02-21T02:05:21.81989Z","iopub.status.idle":"2023-02-21T02:05:39.726079Z","shell.execute_reply":"2023-02-21T02:05:39.725118Z"},"papermill":{"duration":19.058321,"end_time":"2023-02-21T02:05:39.728244","exception":false,"start_time":"2023-02-21T02:05:20.669923","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(generate_text(model, tokenizer, \"To eat chocolate or not to eat chocolate\", num_chars = 200, temperature = 2))","metadata":{"execution":{"iopub.execute_input":"2023-02-21T02:05:42.38529Z","iopub.status.busy":"2023-02-21T02:05:42.384853Z","iopub.status.idle":"2023-02-21T02:05:58.243649Z","shell.execute_reply":"2023-02-21T02:05:58.24265Z"},"papermill":{"duration":17.116949,"end_time":"2023-02-21T02:05:58.245725","exception":false,"start_time":"2023-02-21T02:05:41.128776","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that as the **temperature increases**, the number of **spelling mistakes** increases and the text becomes closer to gibberish. That being said, it is still very impressive how the model learnt how to produce actual **words** in the style of **Shakespeare** by predicting a single character at a time.","metadata":{"papermill":{"duration":1.251965,"end_time":"2023-02-21T02:06:00.725653","exception":false,"start_time":"2023-02-21T02:05:59.473688","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# **Conclusion**\n\nIn this notebook we looked at how Recurrent Neural Networks and its variants can be used to model text as a **sequence of words**. This allowed to solve a new set of problems, namely **sequence labelling** and **language generation**.\n\nMore generally, there are **different ways to set up RNNs** to solve different kinds of problems. \n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/T3XjC3Mw/rnn-setups.jpg\" width=600>\n</center>\n<br>\n\n* **one-to-one** - is equivalent to a feed-forward neural network.\n* **one-to-many** - can be used for captioning, decoders or music generation.\n* **many-to-one** - is used for classification tasks like sentiment analysis.\n* **many-to-many** - used for sequence labelling and language modelling tasks.\n* **staggered many-to-many** - (also known as sequence-to-sequence) its applications include translation, summarization and chatbots. ","metadata":{"papermill":{"duration":1.428463,"end_time":"2023-02-21T02:06:03.717117","exception":false,"start_time":"2023-02-21T02:06:02.288654","status":"completed"},"tags":[]}}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word Embedding\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Shortcomings of Bag-of-Words</p>\n\n<center>\n<img src='https://i.postimg.cc/1zw5pqP1/wordcloud.jpg' width=500>\n</center>\n<br>\n\nLast time we discussed that with a bag-of-words we **lose word order information**, although this can be partially remedied by using n-grams to encode context. \n\nWhen working with a **binary** bag-of-words there is another significant drawback. This is that **all words are treated as equally important**, although we know this is **not the case** in language. \n\nWe could use a **frequency** bag-of-words but then some words like 'the' and 'it' **occur very frequently** and affect similarity calculations. We could remove stop words but this won't remove all of the frequent and redundant words. For example, in a corpus of food recipes, words like 'mix', 'bowl' and 'teaspoon' will appear in almost all documents and so won't be very informative.","metadata":{"papermill":{"duration":0.005962,"end_time":"2023-02-20T20:30:55.896223","exception":false,"start_time":"2023-02-20T20:30:55.890261","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Relative frequency</p>\n\nAn improvement then would be to use the **relative frequency** of each word in the corpus. This is **calculated** as the number of times a word appears in a document divided by the total number of times it appears in the corpus. \n\n<br>\n\n$$\n\\large\n\\text{relative frequency} = \\frac{\\text{frequency in document}}{\\text{frequency in corpus}}\n$$\n\n<br>\n\nThe idea is that words that appear **highly frequently** in some documents and rarely in the rest, are likely to be **meaningful to those documents** and will help distinguish between documents. On the other hand, words that appears roughly **uniformly** across all documents are **unlikely to be important**. ","metadata":{"papermill":{"duration":0.00551,"end_time":"2023-02-20T20:30:55.907812","exception":false,"start_time":"2023-02-20T20:30:55.902302","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# TF-IDF\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Term Frequency</p>\n\n<center>\n<img src='https://i.postimg.cc/xCL5Dt2p/wren.jpg' width=600>\n</center>\n<br>\n\n**TF-IDF** stands for **Term Frequency - Inverse Document Frequency** and is made up of **two components**. The first is the **term frequency**. \n\nWhilst some people define the term frequency to be the relative frequency, it is more common to use the **raw frequency** of the token/term $t$ in document $d$.\n\n<br>\n\n$$\n\\Large\n\\text{tf}(t,d) = f_{t,d}\n$$\n\n<br>\n\nHowever, some documents may be much **longer** than others and so will naturally have higher frequencies across the board. For this reason, it is standard practice to apply the **log-transform** to reduce this bias. The term frequency then becomes\n\n<br>\n\n$$\n\\Large\n\\text{tf}(t,d) = \\log(1+f_{t,d})\n$$\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Inverse Document Frequency</p>\n\nThe second part of TF-IDF is the **inverse document frequency**. This is the part that will **emphasise the more important words** in each document. \n\nGiven a token/term $t$ in a corpus $D$, we define\n\n<br>\n\n$$\n\\Large\n\\text{idf}(t,D) = \\log \\left(\\frac{N}{n_t} \\right)\n$$\n\n<br>\n\nwhere $N$ is the number of documents and $n_t$ is the number of documents $t$ appears in. Notice how as $n_t$ decreases the idf increases corresponding to a token that is more likely to be **important**.\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">TF-IDF score</p>\n\nTo get the final tf-idf score, we simply **multiply** the tf with the idf. That is,\n\n<br>\n\n$$\n\\Large\nw_{t,d} = \\text{tf}(t,d) \\times \\text{idf}(t,D)\n$$\n\n<br>\n\nSo the more frequently a word appears in a given document and the fewer times it appears in other documents the **higher** its TF-IDF score. \n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Variations</p>\n\nThere are **many variations** to the TF-IDF score. Like we discussed earlier, instead of raw frequency, we could use the **relative frequency** in the term frequency term. This is actually how Wikipedia presents the formula.\n\nThe **sklearn implementation** of tf-idf doesn't apply the log-transform in the tf term. It also adds constants to the idf term to prevent division by zero and uses the **natural logarithm**. In particular, is uses the following formulas.\n\n<br>\n\n$$\n\\Large\n\\text{tf}(t,d) = f_{t,d}\n$$\n\n<br>\n\n$$\n\\Large\n\\text{idf}(t,D) = \\ln \\left(\\frac{N+1}{n_t+1} \\right) + 1\n$$\n\n<br>\n\nIt also **normalizes** the output vector for each document so that each document has a vector of scores with **norm equal to 1**. ","metadata":{"papermill":{"duration":0.005536,"end_time":"2023-02-20T20:30:55.919134","exception":false,"start_time":"2023-02-20T20:30:55.913598","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Application\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">TF-IDF with sklearn</p>\n\n<br>\n\nImport the **libraries**.","metadata":{"papermill":{"duration":0.005552,"end_time":"2023-02-20T20:30:55.930446","exception":false,"start_time":"2023-02-20T20:30:55.924894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import spacy\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:30:55.944494Z","iopub.status.busy":"2023-02-20T20:30:55.943667Z","iopub.status.idle":"2023-02-20T20:31:09.678555Z","shell.execute_reply":"2023-02-20T20:31:09.677254Z"},"papermill":{"duration":13.745557,"end_time":"2023-02-20T20:31:09.681842","exception":false,"start_time":"2023-02-20T20:30:55.936285","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this demo, we'll use use the **20 newsgroups dataset**, which is a collection of 18,000 newsgroup posts across 20 topics. We'll take the posts relating to the `sci.space` topic as that will be enough for our application.","metadata":{"papermill":{"duration":0.005531,"end_time":"2023-02-20T20:31:09.693429","exception":false,"start_time":"2023-02-20T20:31:09.687898","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load corpus\ncorpus = fetch_20newsgroups(categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n\n# Preview data\nprint(len(corpus.data))\nprint(corpus.data[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:09.707432Z","iopub.status.busy":"2023-02-20T20:31:09.706355Z","iopub.status.idle":"2023-02-20T20:31:20.948784Z","shell.execute_reply":"2023-02-20T20:31:20.947179Z"},"papermill":{"duration":11.2536,"end_time":"2023-02-20T20:31:20.952784","exception":false,"start_time":"2023-02-20T20:31:09.699184","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to **pre-process** the text first using a **tokenizer**. We'll do this using spacy, like we have seen in previous notebooks. We apply lemmatization, remove punctuation, spaces and non-alphabetic characters. ","metadata":{"papermill":{"duration":0.005564,"end_time":"2023-02-20T20:31:20.966389","exception":false,"start_time":"2023-02-20T20:31:20.960825","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load english language model\nnlp = spacy.load('en_core_web_sm')\n\n# Disable named-entity recognition and parsing to save time\nunwanted_pipes = [\"ner\", \"parser\"]\n\n# Custom tokenizer using spacy\ndef custom_tokenizer(doc):\n    with nlp.disable_pipes(*unwanted_pipes):\n        return [t.lemma_ for t in nlp(doc) if not t.is_punct and not t.is_space and t.is_alpha]","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:20.979875Z","iopub.status.busy":"2023-02-20T20:31:20.979462Z","iopub.status.idle":"2023-02-20T20:31:21.847144Z","shell.execute_reply":"2023-02-20T20:31:21.84567Z"},"papermill":{"duration":0.87766,"end_time":"2023-02-20T20:31:21.849927","exception":false,"start_time":"2023-02-20T20:31:20.972267","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar to the bag-of-words countvectorizer, **sklearn** also provides a class to perform **TF-IDF**, namely`TfidfVectorizer`. To use our custom tokenizer, we pass it through as a parameter.","metadata":{"papermill":{"duration":0.006253,"end_time":"2023-02-20T20:31:21.862417","exception":false,"start_time":"2023-02-20T20:31:21.856164","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initialise tf-idf tokenizer\nvectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n\n# Fit vectorizer to corpus\nfeatures = vectorizer.fit_transform(corpus.data)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:21.877215Z","iopub.status.busy":"2023-02-20T20:31:21.876229Z","iopub.status.idle":"2023-02-20T20:31:36.512419Z","shell.execute_reply":"2023-02-20T20:31:36.511364Z"},"papermill":{"duration":14.646355,"end_time":"2023-02-20T20:31:36.515197","exception":false,"start_time":"2023-02-20T20:31:21.868842","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output is a **sparse matrix** with dimensions (number of documents, size of vocabulary). The entries of the matrix are the **tf-idf scores** of each **document-token pair**. ","metadata":{"papermill":{"duration":0.005667,"end_time":"2023-02-20T20:31:36.527125","exception":false,"start_time":"2023-02-20T20:31:36.521458","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Size of vocabulary\nprint(len(vectorizer.get_feature_names_out()))\n\n# Dimensions of output matrix\nprint(features.shape)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.540929Z","iopub.status.busy":"2023-02-20T20:31:36.540526Z","iopub.status.idle":"2023-02-20T20:31:36.55327Z","shell.execute_reply":"2023-02-20T20:31:36.552189Z"},"papermill":{"duration":0.02271,"end_time":"2023-02-20T20:31:36.556008","exception":false,"start_time":"2023-02-20T20:31:36.533298","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What the matrix looks like\nprint(features)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.570323Z","iopub.status.busy":"2023-02-20T20:31:36.569288Z","iopub.status.idle":"2023-02-20T20:31:36.576894Z","shell.execute_reply":"2023-02-20T20:31:36.575246Z"},"papermill":{"duration":0.017581,"end_time":"2023-02-20T20:31:36.579618","exception":false,"start_time":"2023-02-20T20:31:36.562037","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Document Search</p>\n\nNow we have the tf-idf matrix, we can **measure similarity** exactly the same as before - by using **cosine similarity**. Given a document, we can find the other documents which are **most similar to the original one**. \n\nWe will now go a step further and use it to build a basic **document search recommender system**. Given a **query** (i.e. a search term), we **transform** the query, **measure the similarity** with all the other documents and finally **return the most similar documents**. ","metadata":{"papermill":{"duration":0.005837,"end_time":"2023-02-20T20:31:36.591659","exception":false,"start_time":"2023-02-20T20:31:36.585822","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Transform the query\nquery = [\"Mars\"]\nquery_tfidf = vectorizer.transform(query)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.605359Z","iopub.status.busy":"2023-02-20T20:31:36.604892Z","iopub.status.idle":"2023-02-20T20:31:36.615288Z","shell.execute_reply":"2023-02-20T20:31:36.614308Z"},"papermill":{"duration":0.020203,"end_time":"2023-02-20T20:31:36.61782","exception":false,"start_time":"2023-02-20T20:31:36.597617","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate pairwise similarity with all documents in corpus\ncosine_similarities = cosine_similarity(features, query_tfidf).flatten()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.631712Z","iopub.status.busy":"2023-02-20T20:31:36.631316Z","iopub.status.idle":"2023-02-20T20:31:36.638288Z","shell.execute_reply":"2023-02-20T20:31:36.637293Z"},"papermill":{"duration":0.01664,"end_time":"2023-02-20T20:31:36.640575","exception":false,"start_time":"2023-02-20T20:31:36.623935","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return indices of top k matching documents\ndef top_k(arr, k):\n    kth_largest = (k + 1) * -1\n    return np.argsort(arr)[:kth_largest:-1]\n\n# Return top 5 document indices\ntop_related_indices = top_k(cosine_similarities, 5)\nprint(top_related_indices)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.654963Z","iopub.status.busy":"2023-02-20T20:31:36.654031Z","iopub.status.idle":"2023-02-20T20:31:36.661719Z","shell.execute_reply":"2023-02-20T20:31:36.660238Z"},"papermill":{"duration":0.017529,"end_time":"2023-02-20T20:31:36.664256","exception":false,"start_time":"2023-02-20T20:31:36.646727","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Corresponding cosine similarities\nprint(cosine_similarities[top_related_indices])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.678864Z","iopub.status.busy":"2023-02-20T20:31:36.67846Z","iopub.status.idle":"2023-02-20T20:31:36.684571Z","shell.execute_reply":"2023-02-20T20:31:36.683221Z"},"papermill":{"duration":0.015816,"end_time":"2023-02-20T20:31:36.686583","exception":false,"start_time":"2023-02-20T20:31:36.670767","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top match\nprint(corpus.data[top_related_indices[0]])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.700558Z","iopub.status.busy":"2023-02-20T20:31:36.700128Z","iopub.status.idle":"2023-02-20T20:31:36.705679Z","shell.execute_reply":"2023-02-20T20:31:36.704597Z"},"papermill":{"duration":0.015619,"end_time":"2023-02-20T20:31:36.708374","exception":false,"start_time":"2023-02-20T20:31:36.692755","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second-best match\nprint(corpus.data[top_related_indices[1]])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.723733Z","iopub.status.busy":"2023-02-20T20:31:36.723315Z","iopub.status.idle":"2023-02-20T20:31:36.729769Z","shell.execute_reply":"2023-02-20T20:31:36.728345Z"},"papermill":{"duration":0.016933,"end_time":"2023-02-20T20:31:36.732227","exception":false,"start_time":"2023-02-20T20:31:36.715294","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/fake-real-data/Fake_Real_Data.csv\")\n\nprint(df.shape)\n\ndf.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and Preprocessing","metadata":{}},{"cell_type":"code","source":"# check the distribution of labels \ndf['label'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, we can see that almost the labels(classes) occured equal number of times and balanced. There is no problem of class imbalance and hence no need to apply any balancing techniques like undersampling, oversampling etc.","metadata":{}},{"cell_type":"code","source":"# Add the new column which gives a unique number to each of these labels \ndf['label_num'] = df['label'].map({'Fake' : 0, 'Real': 1})\n\n# check the results with top 5 rows\ndf.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get spacy word vectors and store them in a pandas dataframe","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This will take some time(nearly 15 minutes)\ndf['vector'] = df['Text'].apply(lambda text: nlp(text).vector)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split data into train and test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df.vector.values,\n    df.label_num,\n    test_size=0.2,\n    random_state=2022\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_2d = np.stack(X_train)\nX_test_2d = np.stack(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_train_embed = scaler.fit_transform(X_train_2d)\nscaled_test_embed = scaler.transform(X_test_2d)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes Model","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()\nclf.fit(scaled_train_embed, y_train)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Classification report","metadata":{}},{"cell_type":"code","source":"y_pred = clf.predict(scaled_test_embed)\n\nprint(classification_report(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN Model","metadata":{}},{"cell_type":"code","source":"from  sklearn.neighbors import KNeighborsClassifier\n\n#1. creating a KNN model object\nclf = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n\n#2. fit with all_train_embeddings and y_train\nclf.fit(X_train_2d, y_train)\n\n#3. get the predictions for all_test_embeddings and store it in y_pred\ny_pred = clf.predict(X_test_2d)\n\n#4. print the classfication report\nprint(classification_report(y_test, y_pred))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"#finally print the confusion matrix for the best model\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sn\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Prediction')\nplt.ylabel('Truth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Key Takeaways\n\n1. KNN model which didn't perform well in the vectorization techniques like Bag of words, and TF-IDF due to very **high dimensional vector space**, performed really well with glove vectors due to only **300-dimensional** vectors and very good embeddings(similar and related words have almost similar embeddings) for the given text data.\n\n2. MultinomialNB model performed decently well but did not come into the top list because in the 300-dimensional vectors we also have the negative values present. The Naive Bayes model does not fit the data if there are **negative values**. So, to overcome this shortcoming, we have used the **Min-Max scaler** to bring down all the values between 0 to 1. In this process, there will be a possibility of variance and information loss among the data. But anyhow we got a decent recall and f1 scores.","metadata":{}}]}
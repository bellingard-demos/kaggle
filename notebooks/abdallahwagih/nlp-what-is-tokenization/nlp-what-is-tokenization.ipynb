{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">What is tokenization?</p>\n\nWe usually start an NLP project with a large body of text, called a **corpus**. This could be a collection of tweets, website reviews or transcriptions of films, for example. We need to **pre-process** our corpus to give it enough **structure** to be used in a machine learning model and tokenization is the most common first step.\n\n**Tokenization** is the process of breaking down a corpus into **tokens**. The procedure might look like **segmenting** a piece of text into sentences and then further segmenting these sentences into individual **words, numbers and punctuation**, which would be tokens. \n\n<center>\n<img src='https://i.postimg.cc/ydcbhtkj/tokenization2.jpg' width=600>\n</center>\n\nEach token should be chosen to be as **small as possible** while still carrying carrying **meaning on its own**. For example, `\"£10\"` can be split into the two tokens `\"£\"` and `\"10\"` as each one possess its own meaning. \n\nNote that researches are still trying to find out the best way to tokenize text. There exist effective models that break down words into smaller parts like splitting `\"running\"` into `[\"run\",\"-ing\"]` (called morphemes), or even into individual letters `[\"r\", \"u\", \"n\", \"n\", \"i\", \"n\", \"g\"]` (called graphemes). ","metadata":{"papermill":{"duration":0.007543,"end_time":"2023-02-20T20:20:32.838759","exception":false,"start_time":"2023-02-20T20:20:32.831216","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Tokenization using spaCy</p>\n\n<center>\n<img src='https://i.postimg.cc/ryZxP111/spacy.png' width=250>\n</center>\n<br>\n\nLucky for us, there exist **robust NLP libraries** which can perform tokenization for us. Let's see how to do this with one of the most popular ones called **spaCy**. ","metadata":{"papermill":{"duration":0.006048,"end_time":"2023-02-20T20:20:32.851085","exception":false,"start_time":"2023-02-20T20:20:32.845037","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import spacy library\nimport spacy\nprint(spacy.__name__, spacy.__version__)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:32.86598Z","iopub.status.busy":"2023-02-20T20:20:32.865206Z","iopub.status.idle":"2023-02-20T20:20:43.728077Z","shell.execute_reply":"2023-02-20T20:20:43.726965Z"},"papermill":{"duration":10.873867,"end_time":"2023-02-20T20:20:43.731187","exception":false,"start_time":"2023-02-20T20:20:32.85732","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we load a **statistical model** of the English language **trained on web articles**. Its capabilities include tokenization, among other things. ","metadata":{"papermill":{"duration":0.006698,"end_time":"2023-02-20T20:20:43.744895","exception":false,"start_time":"2023-02-20T20:20:43.738197","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load the spacy model\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:43.761732Z","iopub.status.busy":"2023-02-20T20:20:43.761057Z","iopub.status.idle":"2023-02-20T20:20:44.627501Z","shell.execute_reply":"2023-02-20T20:20:44.626625Z"},"papermill":{"duration":0.877281,"end_time":"2023-02-20T20:20:44.629934","exception":false,"start_time":"2023-02-20T20:20:43.752653","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To tokenize a string, we simply pass it in as an **argument** to the model.","metadata":{"papermill":{"duration":0.006503,"end_time":"2023-02-20T20:20:44.644172","exception":false,"start_time":"2023-02-20T20:20:44.637669","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Tokenize string\ns = \"Noah doesn't like to run when it rains.\"\ndoc = nlp(s)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.659524Z","iopub.status.busy":"2023-02-20T20:20:44.658837Z","iopub.status.idle":"2023-02-20T20:20:44.684921Z","shell.execute_reply":"2023-02-20T20:20:44.683694Z"},"papermill":{"duration":0.036828,"end_time":"2023-02-20T20:20:44.687688","exception":false,"start_time":"2023-02-20T20:20:44.65086","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can **view the tokens** using one line of code.","metadata":{"papermill":{"duration":0.006566,"end_time":"2023-02-20T20:20:44.701114","exception":false,"start_time":"2023-02-20T20:20:44.694548","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Print tokens\nfor token in doc:\n    print(token)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.716104Z","iopub.status.busy":"2023-02-20T20:20:44.7157Z","iopub.status.idle":"2023-02-20T20:20:44.720771Z","shell.execute_reply":"2023-02-20T20:20:44.719709Z"},"papermill":{"duration":0.015242,"end_time":"2023-02-20T20:20:44.723024","exception":false,"start_time":"2023-02-20T20:20:44.707782","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* `\"doesn't\"` gets split into two tokens: `\"does\"` and `\"n't\"`.\n* the full stop `\".\"` gets its own token.","metadata":{"papermill":{"duration":0.006776,"end_time":"2023-02-20T20:20:44.736882","exception":false,"start_time":"2023-02-20T20:20:44.730106","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Types and attributes</p>\n\nThe `doc` object is a **container**, which can be indexed and sliced like a list.","metadata":{"papermill":{"duration":0.00631,"end_time":"2023-02-20T20:20:44.749934","exception":false,"start_time":"2023-02-20T20:20:44.743624","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Index and slice example\nprint(doc[0])\nprint(doc[0:3])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.764952Z","iopub.status.busy":"2023-02-20T20:20:44.764541Z","iopub.status.idle":"2023-02-20T20:20:44.770424Z","shell.execute_reply":"2023-02-20T20:20:44.769307Z"},"papermill":{"duration":0.017073,"end_time":"2023-02-20T20:20:44.773552","exception":false,"start_time":"2023-02-20T20:20:44.756479","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each entry in the doc object is a **token object**. And if you slice a doc object you get a **span object**. ","metadata":{"papermill":{"duration":0.006587,"end_time":"2023-02-20T20:20:44.788034","exception":false,"start_time":"2023-02-20T20:20:44.781447","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Object types\nprint(type(doc))\nprint(type(doc[0]))\nprint(type(doc[0:3]))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.803611Z","iopub.status.busy":"2023-02-20T20:20:44.8029Z","iopub.status.idle":"2023-02-20T20:20:44.809036Z","shell.execute_reply":"2023-02-20T20:20:44.807658Z"},"papermill":{"duration":0.016338,"end_time":"2023-02-20T20:20:44.81105","exception":false,"start_time":"2023-02-20T20:20:44.794712","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each token has several **attributes** such as language, length, index, etc. We will explore these in more detail in later notebooks but here are some examples.","metadata":{"papermill":{"duration":0.006595,"end_time":"2023-02-20T20:20:44.824432","exception":false,"start_time":"2023-02-20T20:20:44.817837","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Token attribute examples\nprint(doc[3].text)\nprint(doc[3].lang_)\nprint(doc[3].__len__())","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.839834Z","iopub.status.busy":"2023-02-20T20:20:44.839426Z","iopub.status.idle":"2023-02-20T20:20:44.84551Z","shell.execute_reply":"2023-02-20T20:20:44.844293Z"},"papermill":{"duration":0.017044,"end_time":"2023-02-20T20:20:44.84835","exception":false,"start_time":"2023-02-20T20:20:44.831306","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can locate the index of each token using the `.i` attribute.","metadata":{"papermill":{"duration":0.006658,"end_time":"2023-02-20T20:20:44.862844","exception":false,"start_time":"2023-02-20T20:20:44.856186","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Locate index of tokens\nfor token in doc[:6]:\n    print(token.text, token.i)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.878994Z","iopub.status.busy":"2023-02-20T20:20:44.877931Z","iopub.status.idle":"2023-02-20T20:20:44.884044Z","shell.execute_reply":"2023-02-20T20:20:44.882784Z"},"papermill":{"duration":0.016713,"end_time":"2023-02-20T20:20:44.88644","exception":false,"start_time":"2023-02-20T20:20:44.869727","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Tokenizing paragraphs</p>\n\nWe can tokenize paragraphs in a very similar way.","metadata":{"papermill":{"duration":0.006798,"end_time":"2023-02-20T20:20:44.90035","exception":false,"start_time":"2023-02-20T20:20:44.893552","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Tokenize multiple sentences\ns = \"Hello there! General Kenobi. You are a bold one.\"\ndoc = nlp(s)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.916473Z","iopub.status.busy":"2023-02-20T20:20:44.915703Z","iopub.status.idle":"2023-02-20T20:20:44.930218Z","shell.execute_reply":"2023-02-20T20:20:44.929104Z"},"papermill":{"duration":0.025624,"end_time":"2023-02-20T20:20:44.933028","exception":false,"start_time":"2023-02-20T20:20:44.907404","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can iterate through the sentences using the `.sents` attribute.","metadata":{"papermill":{"duration":0.006835,"end_time":"2023-02-20T20:20:44.947023","exception":false,"start_time":"2023-02-20T20:20:44.940188","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Print sentences\nlist(doc.sents)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:44.96348Z","iopub.status.busy":"2023-02-20T20:20:44.963059Z","iopub.status.idle":"2023-02-20T20:20:44.971697Z","shell.execute_reply":"2023-02-20T20:20:44.970491Z"},"papermill":{"duration":0.019677,"end_time":"2023-02-20T20:20:44.974143","exception":false,"start_time":"2023-02-20T20:20:44.954466","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that each sentence is a **span object** of the original document. ","metadata":{"papermill":{"duration":0.006872,"end_time":"2023-02-20T20:20:44.988203","exception":false,"start_time":"2023-02-20T20:20:44.981331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Object type\ntype(list(doc.sents)[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:20:45.004308Z","iopub.status.busy":"2023-02-20T20:20:45.003886Z","iopub.status.idle":"2023-02-20T20:20:45.010466Z","shell.execute_reply":"2023-02-20T20:20:45.0097Z"},"papermill":{"duration":0.017075,"end_time":"2023-02-20T20:20:45.012495","exception":false,"start_time":"2023-02-20T20:20:44.99542","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}
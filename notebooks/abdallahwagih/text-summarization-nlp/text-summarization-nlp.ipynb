{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Needed Modules","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom heapq import nlargest  # This module provides an implementation of the heap queue algorithm, also known as the priority queue algorithm.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Acquisition\nYou can find this text in wikipedia\n- **Link**: https://en.wikipedia.org/wiki/Automatic_summarization#:~:text=There%20are%20broadly,redundant%20frames%20captured.","metadata":{}},{"cell_type":"code","source":"text = \"\"\"\nThere are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[13] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load english large model from **SpaCy**","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenization\ndoc = nlp(text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We will get text words frequencies","metadata":{}},{"cell_type":"code","source":"word_frequencies = {}\nfor token in doc:\n    # Remove stopwords and punctuations, and also '\\n'\n    if token.is_stop or token.is_punct or str(token) == '\\n':\n            continue\n    \n    # At the first of each word, the word is not existed in the dict\n    if token.text not in word_frequencies.keys():\n        word_frequencies[token.text] = 1\n        \n    else:\n        word_frequencies[token.text] += 1\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# That's words frequencies\nprint(word_frequencies)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the count of most frequency item\nmax_frequency = max(word_frequencies.values())\nmax_frequency ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We will normalize these frequencies with max frequency item","metadata":{}},{"cell_type":"code","source":"for word in word_frequencies.keys():\n    word_frequencies[word] = word_frequencies[word] / max_frequency","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After normalizing \nprint(word_frequencies)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sentences Tokenization","metadata":{}},{"cell_type":"code","source":"# Store sentences in a list\nsentence_tokens = [sent for sent in doc.sents]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent in sentence_tokens:\n    print(sent)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We will get sentences frequencies","metadata":{}},{"cell_type":"code","source":"sentence_scores = {}\nfor sent in sentence_tokens:\n    for word in sent:\n        # Chech if each word is existed in words list\n        if word.text.lower() in word_frequencies.keys():\n            \n            # At the first of each sentence, the sentence is not existed in the dict\n            if sent not in sentence_scores.keys():\n                sentence_scores[sent] = word_frequencies[word.text.lower()]\n            else:\n                sentence_scores[sent] += word_frequencies[word.text.lower()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k, v in sentence_scores.items():\n    print(f'Sentence: {k} || Frequence: {v}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We want to summarize the full text to a factor (factor)","metadata":{}},{"cell_type":"code","source":"factor = 0.3\nselect_length = int(len(sentence_tokens) * factor)\nselect_length ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlargest function : returns the specified number of largest elements\nlg_sents = nlargest(select_length, sentence_scores, key=sentence_scores.get)\nlg_sents","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lg_sents is list contains 'Span' items\nfor sent in lg_sents:\n    print(type(sent))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert these items to a strings\nsummary_sents = [word.text for word in lg_sents]\nsummary_sents","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The final summary","metadata":{}},{"cell_type":"code","source":"summary = ' '.join(summary_sents)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(summary)","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Import Needed Modules**","metadata":{}},{"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/docs\n!pip install imutils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-04T19:12:02.034477Z","iopub.execute_input":"2023-11-04T19:12:02.034772Z","iopub.status.idle":"2023-11-04T19:12:21.771779Z","shell.execute_reply.started":"2023-11-04T19:12:02.034745Z","shell.execute_reply":"2023-11-04T19:12:21.770708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_docs.vis import embed\nfrom tensorflow import keras\nfrom imutils import paths\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport imageio\nimport cv2\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:15:54.405416Z","iopub.execute_input":"2023-11-04T19:15:54.406376Z","iopub.status.idle":"2023-11-04T19:15:54.660561Z","shell.execute_reply.started":"2023-11-04T19:15:54.406337Z","shell.execute_reply":"2023-11-04T19:15:54.659525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Get Data**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/ucf101-videos/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/ucf101-videos/test.csv\")\n\nprint(f\"Total videos for training: {len(train_df)}\")\nprint(f\"Total videos for testing: {len(test_df)}\")\n\ntrain_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:16:48.901744Z","iopub.execute_input":"2023-11-04T19:16:48.902608Z","iopub.status.idle":"2023-11-04T19:16:48.933493Z","shell.execute_reply.started":"2023-11-04T19:16:48.902574Z","shell.execute_reply":"2023-11-04T19:16:48.932608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Set Hyperparameters**","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 64\nEPOCHS = 10\n\nMAX_SEQ_LENGTH = 20\nNUM_FEATURES = 2048","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:15:57.167311Z","iopub.execute_input":"2023-11-04T19:15:57.168134Z","iopub.status.idle":"2023-11-04T19:15:57.172445Z","shell.execute_reply.started":"2023-11-04T19:15:57.1681Z","shell.execute_reply":"2023-11-04T19:15:57.171474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Create Helper Functions**","metadata":{}},{"cell_type":"code","source":"def crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x // 2) - (min_dim // 2)\n    start_y = (y // 2) - (min_dim // 2)\n    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n\n\ndef load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    return np.array(frames)","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:17:09.909853Z","iopub.execute_input":"2023-11-04T19:17:09.910186Z","iopub.status.idle":"2023-11-04T19:17:09.918986Z","shell.execute_reply.started":"2023-11-04T19:17:09.91016Z","shell.execute_reply":"2023-11-04T19:17:09.918026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_feature_extractor():\n    feature_extractor = keras.applications.InceptionV3(\n        weights=\"imagenet\",\n        include_top=False,\n        pooling=\"avg\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    )\n    preprocess_input = keras.applications.inception_v3.preprocess_input\n\n    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    preprocessed = preprocess_input(inputs)\n\n    outputs = feature_extractor(preprocessed)\n    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n\n\nfeature_extractor = build_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:17:26.783235Z","iopub.execute_input":"2023-11-04T19:17:26.783868Z","iopub.status.idle":"2023-11-04T19:17:33.947068Z","shell.execute_reply.started":"2023-11-04T19:17:26.783838Z","shell.execute_reply":"2023-11-04T19:17:33.946308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n\nprint(label_processor.get_vocabulary())","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:17:47.565676Z","iopub.execute_input":"2023-11-04T19:17:47.566047Z","iopub.status.idle":"2023-11-04T19:17:47.608737Z","shell.execute_reply.started":"2023-11-04T19:17:47.566015Z","shell.execute_reply":"2023-11-04T19:17:47.607864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    video_paths = df[\"video_name\"].values.tolist()\n    labels = df[\"tag\"].values\n    labels = label_processor(labels[..., None]).numpy()\n\n    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n    # masked with padding or not.\n    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n    frame_features = np.zeros(\n        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n    )\n\n    # For each video.\n    for idx, path in enumerate(video_paths):\n        # Gather all its frames and add a batch dimension.\n        frames = load_video(os.path.join(root_dir, path))\n        frames = frames[None, ...]\n\n        # Initialize placeholders to store the masks and features of the current video.\n        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n        temp_frame_features = np.zeros(\n            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n        )\n\n        # Extract features from the frames of the current video.\n        for i, batch in enumerate(frames):\n            video_length = batch.shape[0]\n            length = min(MAX_SEQ_LENGTH, video_length)\n            for j in range(length):\n                temp_frame_features[i, j, :] = feature_extractor.predict(\n                    batch[None, j, :]\n                )\n            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n        frame_features[idx,] = temp_frame_features.squeeze()\n        frame_masks[idx,] = temp_frame_mask.squeeze()\n\n    return (frame_features, frame_masks), labels\n\n\ntrain_data, train_labels = prepare_all_videos(train_df, \"train\")\ntest_data, test_labels = prepare_all_videos(test_df, \"test\")\n\nprint(f\"Frame features in train set: {train_data[0].shape}\")\nprint(f\"Frame masks in train set: {train_data[1].shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:18:13.586167Z","iopub.execute_input":"2023-11-04T19:18:13.586558Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility for our sequence model.\ndef get_sequence_model():\n    class_vocab = label_processor.get_vocabulary()\n\n    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n\n    # Refer to the following tutorial to understand the significance of using `mask`:\n    # https://keras.io/api/layers/recurrent_layers/gru/\n    x = keras.layers.GRU(16, return_sequences=True)(\n        frame_features_input, mask=mask_input\n    )\n    x = keras.layers.GRU(8)(x)\n    x = keras.layers.Dropout(0.4)(x)\n    x = keras.layers.Dense(8, activation=\"relu\")(x)\n    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n\n    rnn_model = keras.Model([frame_features_input, mask_input], output)\n\n    rnn_model.compile(\n        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n    )\n    return rnn_model\n\n\n# Utility for running experiments.\ndef run_experiment():\n    filepath = \"/tmp/video_classifier\"\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        filepath, save_weights_only=True, save_best_only=True, verbose=1\n    )\n\n    seq_model = get_sequence_model()\n    history = seq_model.fit(\n        [train_data[0], train_data[1]],\n        train_labels,\n        validation_split=0.3,\n        epochs=EPOCHS,\n        callbacks=[checkpoint],\n    )\n\n    seq_model.load_weights(filepath)\n    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n\n    return history, seq_model\n\n\n_, sequence_model = run_experiment()","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:44:46.71044Z","iopub.execute_input":"2023-11-04T19:44:46.710834Z","iopub.status.idle":"2023-11-04T19:45:01.892642Z","shell.execute_reply.started":"2023-11-04T19:44:46.710802Z","shell.execute_reply":"2023-11-04T19:45:01.891731Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef prepare_single_video(frames):\n    frames = frames[None, ...]\n    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    for i, batch in enumerate(frames):\n        video_length = batch.shape[0]\n        length = min(MAX_SEQ_LENGTH, video_length)\n        for j in range(length):\n            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n    return frame_features, frame_mask\n\n\ndef sequence_prediction(path):\n    class_vocab = label_processor.get_vocabulary()\n\n    frames = load_video(os.path.join(\"test\", path))\n    frame_features, frame_mask = prepare_single_video(frames)\n    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n\n    for i in np.argsort(probabilities)[::-1]:\n        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n    return frames\n\n\n# This utility is for visualization.\n# Referenced from:\n# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\ndef to_gif(images):\n    converted_images = images.astype(np.uint8)\n    imageio.mimsave(\"animation.gif\", converted_images, duration=100)\n    return embed.embed_file(\"animation.gif\")","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:55:13.431983Z","iopub.execute_input":"2023-11-04T19:55:13.432868Z","iopub.status.idle":"2023-11-04T19:55:13.442755Z","shell.execute_reply.started":"2023-11-04T19:55:13.432833Z","shell.execute_reply":"2023-11-04T19:55:13.44176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Test Video**","metadata":{}},{"cell_type":"code","source":"test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\nprint(f\"Test video path: {test_video}\")\ntest_frames = sequence_prediction(test_video)\nto_gif(test_frames[:MAX_SEQ_LENGTH])","metadata":{"execution":{"iopub.status.busy":"2023-11-04T19:55:51.07127Z","iopub.execute_input":"2023-11-04T19:55:51.07228Z","iopub.status.idle":"2023-11-04T19:55:53.553022Z","shell.execute_reply.started":"2023-11-04T19:55:51.072237Z","shell.execute_reply":"2023-11-04T19:55:53.551981Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
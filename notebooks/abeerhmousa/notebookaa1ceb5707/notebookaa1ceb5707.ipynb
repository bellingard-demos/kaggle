{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-30T15:43:56.152336Z","iopub.execute_input":"2023-08-30T15:43:56.152843Z","iopub.status.idle":"2023-08-30T15:43:56.171183Z","shell.execute_reply.started":"2023-08-30T15:43:56.152805Z","shell.execute_reply":"2023-08-30T15:43:56.170238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Steps\n\n0. Data Visualization and statistics \n\n1. Data Preprocessing : handling missing values, scaling features, and encoding categorical variables\n2. Data Splitting: training, and testing sets (x_train and test, y_train and test)\n3. Feature Selection/Engineering\n4. Model Selection:  machine learning algorithms for fraud detection\n> * Neural Networks\n> * Logistic Regression\n> * Decision Trees\n> * Random Forests\n> * Gradient Boosting (e.g., XGBoost, LightGBM)\n5. Model Training\n6. Model Evaluation:accuracy, precision, recall, F1-score, and ROC-AUC. \n7. Handling Imbalance: oversampling the minority class, undersampling the majority class, Synthetic Minority Over-sampling Technique (SMOTE)\n8. Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# load dataset to pandas dataframe\ndf = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\n# shows the valuse of top rows, which gives more idea on data types. \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:43:56.452977Z","iopub.execute_input":"2023-08-30T15:43:56.454009Z","iopub.status.idle":"2023-08-30T15:44:00.308728Z","shell.execute_reply.started":"2023-08-30T15:43:56.453967Z","shell.execute_reply":"2023-08-30T15:44:00.307253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Visualization**\n\nDoing some basic statistic to visualize the data:\nBasic Stats\n\n>* describe() shows a summary of numerial features.\n>* value_counts()  generate a summary of categorical features.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:00.31161Z","iopub.execute_input":"2023-08-30T15:44:00.311979Z","iopub.status.idle":"2023-08-30T15:44:00.883753Z","shell.execute_reply.started":"2023-08-30T15:44:00.311949Z","shell.execute_reply":"2023-08-30T15:44:00.882036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# histogram\ndf.hist(bins=50, figsize=(15, 15))","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:00.886028Z","iopub.execute_input":"2023-08-30T15:44:00.886922Z","iopub.status.idle":"2023-08-30T15:44:09.77265Z","shell.execute_reply.started":"2023-08-30T15:44:00.886875Z","shell.execute_reply":"2023-08-30T15:44:09.771447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exploring the dataset\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.774149Z","iopub.execute_input":"2023-08-30T15:44:09.774521Z","iopub.status.idle":"2023-08-30T15:44:09.808118Z","shell.execute_reply.started":"2023-08-30T15:44:09.774489Z","shell.execute_reply":"2023-08-30T15:44:09.806828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count null values in each attribute (column)\ndf.isnull().sum()\n# no null values in the date \n# in case there is any null values we can treate it by filling it with the mean value as below \n# df.fillna(df.mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.811351Z","iopub.execute_input":"2023-08-30T15:44:09.811819Z","iopub.status.idle":"2023-08-30T15:44:09.840397Z","shell.execute_reply.started":"2023-08-30T15:44:09.81178Z","shell.execute_reply":"2023-08-30T15:44:09.839138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target is the calss, so lets do some statistcs on it","metadata":{}},{"cell_type":"code","source":"df['Class'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.841944Z","iopub.execute_input":"2023-08-30T15:44:09.842594Z","iopub.status.idle":"2023-08-30T15:44:09.85619Z","shell.execute_reply.started":"2023-08-30T15:44:09.842545Z","shell.execute_reply":"2023-08-30T15:44:09.854489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prc = 492 / 284315\nprint('The % of fraud to normal transaction is:'+ str(prc))\nprint('data is unbalanced')","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.858156Z","iopub.execute_input":"2023-08-30T15:44:09.85855Z","iopub.status.idle":"2023-08-30T15:44:09.868578Z","shell.execute_reply.started":"2023-08-30T15:44:09.858518Z","shell.execute_reply":"2023-08-30T15:44:09.867001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Splitting Data x_train, y_train / x_test, y_test\n\n1. split the features from the target into two data sets x: for features, y: for target\n2. split the data sets into training data set to train the model and test data set to test the model ","metadata":{}},{"cell_type":"code","source":"y = df['Class']\nx = df.drop(['Class'], axis = 1)\ny.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.872822Z","iopub.execute_input":"2023-08-30T15:44:09.87323Z","iopub.status.idle":"2023-08-30T15:44:09.927114Z","shell.execute_reply.started":"2023-08-30T15:44:09.873197Z","shell.execute_reply":"2023-08-30T15:44:09.925797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.928484Z","iopub.execute_input":"2023-08-30T15:44:09.928862Z","iopub.status.idle":"2023-08-30T15:44:09.963328Z","shell.execute_reply.started":"2023-08-30T15:44:09.92883Z","shell.execute_reply":"2023-08-30T15:44:09.961766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\n\n**Correlation Analysis**: in this we will study the correlation between each feature with the target","metadata":{}},{"cell_type":"code","source":"# from pandas.plotting import scatter_matrix\n# attributes = ['Time', 'V1', 'V2', 'V3','V4', 'V5', 'Amount','Class']\n# scatter_matrix(df[attributes], figsize=(12,12)) ","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.965097Z","iopub.execute_input":"2023-08-30T15:44:09.965541Z","iopub.status.idle":"2023-08-30T15:44:09.974432Z","shell.execute_reply.started":"2023-08-30T15:44:09.96549Z","shell.execute_reply":"2023-08-30T15:44:09.973229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.stats\n\npearson_correlations = []\nfor column in x.columns:\n    pearson_corr, _ = scipy.stats.pearsonr(x[column], y)\n    pearson_correlations.append((column, pearson_corr))\n    \nprint(\"Pearson Correlations:\")\nfor feature, corr in pearson_correlations:\n    print(f\"{feature}: {corr:.4f}\")    ","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:09.97591Z","iopub.execute_input":"2023-08-30T15:44:09.976815Z","iopub.status.idle":"2023-08-30T15:44:10.308263Z","shell.execute_reply.started":"2023-08-30T15:44:09.976777Z","shell.execute_reply":"2023-08-30T15:44:10.306747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the following have high correlation with the target: Class:\n\n1. V1: -0.1013\n2. **V3: -0.1930**\n3. V4: 0.1334\n4. **V10: -0.2169**\n5. V11: 0.1549\n6. **V12: -0.2606**\n7. **V14: -0.3025**\n4. **V16: -0.1965**\n5. **V17: -0.3265**\n6. V18: -0.1115\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:10.310539Z","iopub.execute_input":"2023-08-30T15:44:10.311646Z","iopub.status.idle":"2023-08-30T15:44:10.492467Z","shell.execute_reply.started":"2023-08-30T15:44:10.311546Z","shell.execute_reply":"2023-08-30T15:44:10.491074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**t-test - Feature Selection** : in this step, the t-test was applied on each continuous feature with the binary target 'Class' to assess the relationship for each.","metadata":{}},{"cell_type":"code","source":"t_test = []\nfor column in x.columns:\n    fraud_values = x[column][y == 1]\n    non_fraud_values = x[column][y == 0]\n    t_statistic, p_value = scipy.stats.ttest_ind(fraud_values, non_fraud_values)\n    t_test.append((column, t_statistic, p_value))\n\n# Print t-test results\nprint(\"T-Test Results:\")\nfor feature, t_statistic, p_value in t_test:\n    print(f\"{feature}: t-statistic={t_statistic:.4f}, p-value={p_value:.4f}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:10.494753Z","iopub.execute_input":"2023-08-30T15:44:10.495215Z","iopub.status.idle":"2023-08-30T15:44:10.635029Z","shell.execute_reply.started":"2023-08-30T15:44:10.495171Z","shell.execute_reply":"2023-08-30T15:44:10.634143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**p-value explanation:**\nif p_value is less than the significance level (common 0.05) this means that the feature is potentially informative and we can select it in our model\n\n**features to execlude because of high p-value:**\n1. V22: t-statistic=0.4298, **p-value=0.6674**\n2. V23: t-statistic=-1.4330, **p-value=0.1519**\n3. V25: t-statistic=1.7652, **p-value=0.0775**\n ","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing - Scaling Features\n\nThe data has high and low mean so the following step will normalize the data.\n\n**features (column) with high mean can dominate the learning process.  **","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\n# convert from array to panda data fram\nx_train_df = pd.DataFrame(x_train)\nx_train_scaled_df = pd.DataFrame(x_train_scaled)\nx_test_scaled_df = pd.DataFrame(x_test_scaled)\n\nx_train_scaled_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:10.639796Z","iopub.execute_input":"2023-08-30T15:44:10.640458Z","iopub.status.idle":"2023-08-30T15:44:11.182908Z","shell.execute_reply.started":"2023-08-30T15:44:10.6404Z","shell.execute_reply":"2023-08-30T15:44:11.181777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:11.18445Z","iopub.execute_input":"2023-08-30T15:44:11.184807Z","iopub.status.idle":"2023-08-30T15:44:11.58812Z","shell.execute_reply.started":"2023-08-30T15:44:11.184777Z","shell.execute_reply":"2023-08-30T15:44:11.586887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above x_train_df statistics differ from x_train_sacled_df after transforming the data  ","metadata":{}},{"cell_type":"code","source":"x_test_scaled_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:11.589922Z","iopub.execute_input":"2023-08-30T15:44:11.59101Z","iopub.status.idle":"2023-08-30T15:44:11.782224Z","shell.execute_reply.started":"2023-08-30T15:44:11.590958Z","shell.execute_reply":"2023-08-30T15:44:11.78091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Oversampling\n\nIn our data set we have the following counts for the results for our target \"Class\":\n\n0:    **284315**  - non fraud\n\n1:       **492**  - fraud\n\n\nwe will use it in the mdel and test the accuracy\n","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=42)\nx_train_scaled_oversampled, y_train_oversampled = smote.fit_resample(x_train_scaled, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:11.783389Z","iopub.execute_input":"2023-08-30T15:44:11.784024Z","iopub.status.idle":"2023-08-30T15:44:12.160098Z","shell.execute_reply.started":"2023-08-30T15:44:11.78399Z","shell.execute_reply":"2023-08-30T15:44:12.158808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Undersampling\n\nwe will use also undersampling in the model and test the accuracy and take the better model\n","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nunder_sampler = RandomUnderSampler(random_state=42)\nX_train_scaled_undersampled, y_train_undersampled = under_sampler.fit_resample(x_train_scaled, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:12.161662Z","iopub.execute_input":"2023-08-30T15:44:12.162014Z","iopub.status.idle":"2023-08-30T15:44:12.243749Z","shell.execute_reply.started":"2023-08-30T15:44:12.161984Z","shell.execute_reply":"2023-08-30T15:44:12.242223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    # **Neural Network**\n\nhere we used two activation function:\n1. ReLU (Rectified Linear Activation):f(x) = max(0, x) used in hidden layers to address the vanishing gradient problem\n2. Sigmoid Activation: f(x) = 1 / (1 + exp(-x)) for target prediction\n\n** we used all the feature in this model as NN can handle large number of inputs \n","metadata":{}},{"cell_type":"markdown","source":"># Model 1: NN with Oversampling - Using All features","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n# step1: build the model\nmodel_nn_oversampling = Sequential()\nmodel_nn_oversampling.add(Dense(64, activation='relu', input_shape=(x_train_scaled_oversampled.shape[1],)))\nmodel_nn_oversampling.add(Dense(32, activation='relu'))\nmodel_nn_oversampling.add(Dense(1, activation='sigmoid'))\n\n#Step2: compile the model\nmodel_nn_oversampling.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n#Step3: train the model\ntraining_nn_oversampling_history = model_nn_oversampling.fit(x_train_scaled_oversampled, y_train_oversampled, epochs=10, batch_size=32, validation_split=0.1)\n\n#Step4: Evaluate the model\nloss, accuracy = model_nn_oversampling.evaluate(x_test_scaled, y_test)\nprint(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:44:12.245358Z","iopub.execute_input":"2023-08-30T15:44:12.245752Z","iopub.status.idle":"2023-08-30T15:49:22.565584Z","shell.execute_reply.started":"2023-08-30T15:44:12.245718Z","shell.execute_reply":"2023-08-30T15:49:22.56423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this model: the **Test loss value of 0.0179** indicates that the model's predictions are closer to the actual outcomes.\n\nThe **Accuracy of 0.9988** reflect statistics on correctly predicted instances\n\n\n","metadata":{}},{"cell_type":"markdown","source":"> **Plot the loss to detect overfitting**","metadata":{}},{"cell_type":"code","source":"# Extract training and validation loss values from the training_nn_oversampling_history\ntrain_nn1_loss = training_nn_oversampling_history.history['loss']\nval_nn1_loss = training_nn_oversampling_history.history['val_loss']\n\n# Extract training and validation accuracy values from the training_nn_oversampling_history\ntrain_nn1_acc = training_nn_oversampling_history.history['accuracy']\nval_nn1_acc = training_nn_oversampling_history.history['val_accuracy']","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:49:22.567378Z","iopub.execute_input":"2023-08-30T15:49:22.567781Z","iopub.status.idle":"2023-08-30T15:49:22.57434Z","shell.execute_reply.started":"2023-08-30T15:49:22.567747Z","shell.execute_reply":"2023-08-30T15:49:22.572903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Loss\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 11), train_nn1_loss, label='Train Loss')\nplt.plot(range(1, 11), val_nn1_loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Plot accuracy \nplt.subplot(1, 2, 2)\nplt.plot(range(1, 11), train_nn1_acc, label='Train Accuracy')\nplt.plot(range(1, 11), val_nn1_acc, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:49:22.576076Z","iopub.execute_input":"2023-08-30T15:49:22.576405Z","iopub.status.idle":"2023-08-30T15:49:23.318575Z","shell.execute_reply.started":"2023-08-30T15:49:22.576377Z","shell.execute_reply":"2023-08-30T15:49:23.317278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion** \n\nFrom the plot above: \n\n1. Both the training and validation loss are decreasing over the epochs, it indicates that the model is learning effectively. \n\n2. Both training and validation accuracy are increasing, it's a positive sign that the model is learning effectively","metadata":{}},{"cell_type":"code","source":"# step 5: predect values \npredictions = model_nn_oversampling.predict(x_test_scaled)\nprint(predictions)\n\n#convert results to binary\nprint(\"**** As binary Output ******\")\nbinary_predictions = (predictions >= 0.5).astype(int)\nprint(binary_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:49:23.320719Z","iopub.execute_input":"2023-08-30T15:49:23.32182Z","iopub.status.idle":"2023-08-30T15:49:26.304983Z","shell.execute_reply.started":"2023-08-30T15:49:23.321765Z","shell.execute_reply":"2023-08-30T15:49:26.303738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **NN Oversampling Confusion Matrix** ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Calculate confusion matrix\nconf_matrix_nn1 = confusion_matrix(y_test, binary_predictions)\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_nn1, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix - NN Oversampling')\nplt.xticks([0.5, 1.5], ['Non-Fraud', 'Fraud'])\nplt.yticks([0.5, 1.5], ['Non-Fraud', 'Fraud'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T17:02:38.720577Z","iopub.execute_input":"2023-08-30T17:02:38.72239Z","iopub.status.idle":"2023-08-30T17:02:38.955308Z","shell.execute_reply.started":"2023-08-30T17:02:38.722328Z","shell.execute_reply":"2023-08-30T17:02:38.952137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"># **Model 2: NN with Undersampling - Using All features **","metadata":{}},{"cell_type":"code","source":"# step1: build the model\nmodel_nn_undersampling = Sequential()\nmodel_nn_undersampling.add(Dense(64, activation='relu', input_shape=(X_train_scaled_undersampled.shape[1],)))\nmodel_nn_undersampling.add(Dense(32, activation='relu'))\nmodel_nn_undersampling.add(Dense(1, activation='sigmoid'))\n\n#Step2: compile the model\nmodel_nn_undersampling.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n#Step3: train the model\ntraining_nn_undersampling_history = model_nn_undersampling.fit(X_train_scaled_undersampled, y_train_undersampled, epochs=10, batch_size=32, validation_split=0.1)\n\n#Step4: Evaluate the model\nloss, accuracy = model_nn_undersampling.evaluate(x_test_scaled, y_test)\nprint(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:49:26.307526Z","iopub.execute_input":"2023-08-30T15:49:26.308016Z","iopub.status.idle":"2023-08-30T15:49:32.456009Z","shell.execute_reply.started":"2023-08-30T15:49:26.307981Z","shell.execute_reply":"2023-08-30T15:49:32.454592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this model: the Test loss value is 0.1102 which is greater than the first model, this means \n\nThe Accuracy of 0.9702 which is less than the first model\n\n> **First Conclusion**\n\n> *model_nn_oversampling is better than model_nn_undersampling*","metadata":{}},{"cell_type":"code","source":"# step 5: predect values \npredictions_nn_undersampling = model_nn_undersampling.predict(x_test_scaled)\nprint(predictions_nn_undersampling)\n\n#convert results to binary\nprint(\"**** As binary Output ******\")\nbinary_undersampling_predictions = (predictions_nn_undersampling >= 0.5).astype(int)\nprint(binary_undersampling_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T16:59:44.473266Z","iopub.execute_input":"2023-08-30T16:59:44.473803Z","iopub.status.idle":"2023-08-30T16:59:47.669664Z","shell.execute_reply.started":"2023-08-30T16:59:44.473764Z","shell.execute_reply":"2023-08-30T16:59:47.668309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Plot the loss to detect overfitting**","metadata":{}},{"cell_type":"code","source":"# Extract training and validation loss values from the training_nn_oversampling_history\ntrain_nn2_loss = training_nn_undersampling_history.history['loss']\nval_nn2_loss = training_nn_undersampling_history.history['val_loss']\n\n# Extract training and validation accuracy values from the training_nn_oversampling_history\ntrain_nn2_acc = training_nn_undersampling_history.history['accuracy']\nval_nn2_acc = training_nn_undersampling_history.history['val_accuracy']","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:49:32.457539Z","iopub.execute_input":"2023-08-30T15:49:32.457911Z","iopub.status.idle":"2023-08-30T15:49:32.464942Z","shell.execute_reply.started":"2023-08-30T15:49:32.457877Z","shell.execute_reply":"2023-08-30T15:49:32.463381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Loss\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 11), train_nn2_loss, label='Train Loss')\nplt.plot(range(1, 11), val_nn2_loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss - undersampling')\nplt.legend()\n\n# Plot accuracy \nplt.subplot(1, 2, 2)\nplt.plot(range(1, 11), train_nn2_acc, label='Train Accuracy')\nplt.plot(range(1, 11), val_nn2_acc, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy - undersampling')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:49:32.467112Z","iopub.execute_input":"2023-08-30T15:49:32.467525Z","iopub.status.idle":"2023-08-30T15:49:33.221811Z","shell.execute_reply.started":"2023-08-30T15:49:32.467494Z","shell.execute_reply":"2023-08-30T15:49:33.220371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Second Conclusion** \n\nFrom the plot above: \n\n1. Both the training and validation loss are decreasing over the epochs, it indicates that the model is learning effectively. \n\n2. Both training and validation accuracy are increasing, it's a positive sign that the model is learning effectively","metadata":{}},{"cell_type":"markdown","source":"> **NN Undersampling Confusion Matrix** \n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Calculate confusion matrix\nconf_matrix_nn2 = confusion_matrix(y_test, binary_undersampling_predictions)\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_nn2, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.xticks([0.5, 1.5], ['Non-Fraud', 'Fraud'])\nplt.yticks([0.5, 1.5], ['Non-Fraud', 'Fraud'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T16:59:57.775718Z","iopub.execute_input":"2023-08-30T16:59:57.77746Z","iopub.status.idle":"2023-08-30T16:59:58.024742Z","shell.execute_reply.started":"2023-08-30T16:59:57.77737Z","shell.execute_reply":"2023-08-30T16:59:58.023723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forests\n\nIn this part we will test another ML model and compare it with the NN models \n\nSteps below are followed to build and apply Random Forests model:\n** Note (data splitting )\n\n1. Splitting the data without scaling it because random forest algorithm depends on decision trees and relative comparisons between features. \n2. Feature Selection: using scikit-learn's SelectFromModel\n3. Building and Training the Model\n4. Evaluate the model\n\n\n***Note*** no need for oversampling or undersampling in Random Forest model, this model is able to handle unbalanced data.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\nselector = SelectFromModel(rf_selector)\n\nselector.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T15:49:33.223701Z","iopub.execute_input":"2023-08-30T15:49:33.224109Z","iopub.status.idle":"2023-08-30T15:54:25.866545Z","shell.execute_reply.started":"2023-08-30T15:49:33.224075Z","shell.execute_reply":"2023-08-30T15:54:25.865042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mask = selector.get_support()\n# Get the column names of selected features\nselected_columns = x_train.columns[selected_mask]\nprint(selected_columns)\n\n# Transform training and testing data to selected features\nx_train_rf_selected = selector.transform(x_train)\nx_test_rf_selected = selector.transform(x_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T16:24:36.617608Z","iopub.execute_input":"2023-08-30T16:24:36.618131Z","iopub.status.idle":"2023-08-30T16:24:36.721522Z","shell.execute_reply.started":"2023-08-30T16:24:36.618092Z","shell.execute_reply":"2023-08-30T16:24:36.719676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Build the Random Forest model\nrandom_forest = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n\n# Train the model \nrandom_forest.fit(x_train_rf_selected, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T16:27:53.97015Z","iopub.execute_input":"2023-08-30T16:27:53.971452Z","iopub.status.idle":"2023-08-30T16:29:16.310489Z","shell.execute_reply.started":"2023-08-30T16:27:53.971393Z","shell.execute_reply":"2023-08-30T16:29:16.309136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the RF model \ny_pred = random_forest.predict(x_test_rf_selected)\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-30T16:30:56.607462Z","iopub.execute_input":"2023-08-30T16:30:56.607862Z","iopub.status.idle":"2023-08-30T16:30:57.155334Z","shell.execute_reply.started":"2023-08-30T16:30:56.607832Z","shell.execute_reply":"2023-08-30T16:30:57.154031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-30T16:31:05.620045Z","iopub.execute_input":"2023-08-30T16:31:05.620495Z","iopub.status.idle":"2023-08-30T16:31:05.632258Z","shell.execute_reply.started":"2023-08-30T16:31:05.620459Z","shell.execute_reply":"2023-08-30T16:31:05.631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.xticks([0.5, 1.5], ['Non-Fraud', 'Fraud'])\nplt.yticks([0.5, 1.5], ['Non-Fraud', 'Fraud'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T16:31:10.462406Z","iopub.execute_input":"2023-08-30T16:31:10.4633Z","iopub.status.idle":"2023-08-30T16:31:10.683023Z","shell.execute_reply.started":"2023-08-30T16:31:10.463258Z","shell.execute_reply":"2023-08-30T16:31:10.681617Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}
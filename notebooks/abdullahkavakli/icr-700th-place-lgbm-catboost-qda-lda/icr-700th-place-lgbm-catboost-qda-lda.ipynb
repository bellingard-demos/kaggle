{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### What was the biggest pitfall that eveyone did?","metadata":{}},{"cell_type":"markdown","source":"It was data leakage. When I examined the public notebooks, I saw a lot of people did fall into this trap. Me? Don't ask me. When compare the algorithms, I get good results. However, i forgot some algorithms' random state ðŸ˜….\n\nLast but least, i think tuning were not effective because of imbalanced dataset. How do I know? I studied an extremely imbalaned dataset. But I can't say I'm so sure. Can't see the winners notebooks. You can reach out the notebook i mentioned here: https://www.kaggle.com/code/abdullahkavakli/credit-card-fraud-detection","metadata":{}},{"cell_type":"code","source":"\nfrom itertools import combinations\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom scipy.stats import uniform, randint\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, accuracy_score, average_precision_score, make_scorer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, StratifiedKFold, train_test_split, GridSearchCV\nfrom sklearn.preprocessing import RobustScaler, power_transform, OneHotEncoder, PowerTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier, EasyEnsembleClassifier, RUSBoostClassifier, BalancedRandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.semi_supervised import SelfTrainingClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.svm import NuSVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.combine import *\nfrom imblearn.over_sampling import *\nfrom imblearn.under_sampling import *\nimport hyperopt \nimport sys\nimport copy\nfrom hyperopt.pyll import scope\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\nfrom warnings import filterwarnings\n\nfilterwarnings(\"ignore\")\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:33.036562Z","iopub.execute_input":"2023-08-10T22:01:33.037412Z","iopub.status.idle":"2023-08-10T22:01:37.824612Z","shell.execute_reply.started":"2023-08-10T22:01:33.037373Z","shell.execute_reply":"2023-08-10T22:01:37.823499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#submission = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-10T22:01:37.827432Z","iopub.execute_input":"2023-08-10T22:01:37.827937Z","iopub.status.idle":"2023-08-10T22:01:37.8402Z","shell.execute_reply.started":"2023-08-10T22:01:37.827891Z","shell.execute_reply":"2023-08-10T22:01:37.837062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/test.csv\")\ndf_greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")\n\nprint('No. of records for train : {}'.format(df_train.shape))\nprint('No. of records for test : {}'.format(df_test.shape))\nprint('No. of records for greeks : {}'.format(df_greeks.shape))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:37.842019Z","iopub.execute_input":"2023-08-10T22:01:37.842385Z","iopub.status.idle":"2023-08-10T22:01:37.940447Z","shell.execute_reply.started":"2023-08-10T22:01:37.842357Z","shell.execute_reply":"2023-08-10T22:01:37.939563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rand_state=42","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:37.942728Z","iopub.execute_input":"2023-08-10T22:01:37.943462Z","iopub.status.idle":"2023-08-10T22:01:37.948068Z","shell.execute_reply.started":"2023-08-10T22:01:37.943428Z","shell.execute_reply":"2023-08-10T22:01:37.94722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def balanced_logloss(y_true, y_pred):\n    if isinstance(y_true, pd.DataFrame):\n        y_true = y_true.to_numpy().ravel()\n    if len(y_true.shape) == 2:\n        y_true = y_true.ravel()\n    nc = np.bincount(y_true)\n    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:37.949442Z","iopub.execute_input":"2023-08-10T22:01:37.950133Z","iopub.status.idle":"2023-08-10T22:01:37.960369Z","shell.execute_reply.started":"2023-08-10T22:01:37.950097Z","shell.execute_reply":"2023-08-10T22:01:37.959408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def contains_none(data_x):\n    if isinstance(data_x, pd.DataFrame):\n        return data_x.isna().any().any()\n    if isinstance(data_x, np.ndarray):\n        return np.isnan(data_x).any()\n    raise Exception(\"Unknown data type\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:37.961974Z","iopub.execute_input":"2023-08-10T22:01:37.962834Z","iopub.status.idle":"2023-08-10T22:01:37.976938Z","shell.execute_reply.started":"2023-08-10T22:01:37.962787Z","shell.execute_reply":"2023-08-10T22:01:37.975293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def impute_train_test(train, test, impute_method=\"knn\"):\n    if impute_method == \"knn\":\n        imputer = KNNImputer().set_output(transform=\"pandas\")\n        train = imputer.fit_transform(train)\n        test = imputer.transform(test)\n    if impute_method == \"lgbm\":\n        imputer = LGBMImputer()\n        train = imputer.fit_transform(train)\n        test = imputer.transform(test)\n    if contains_none(train) or contains_none(test): #if lgbm didn't impute, failed\n        train, test = impute_train_test(train, test, impute_method=\"knn\")\n    return train,test    ","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:37.978777Z","iopub.execute_input":"2023-08-10T22:01:37.979397Z","iopub.status.idle":"2023-08-10T22:01:37.990079Z","shell.execute_reply.started":"2023-08-10T22:01:37.979353Z","shell.execute_reply":"2023-08-10T22:01:37.98916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_and_scale(train, test, power_transform=False):\n    if power_transform:\n        pt = PowerTransformer().set_output(transform=\"pandas\")\n        train = pt.fit_transform(train)\n        test = pt.transform(test)\n    scaler = RobustScaler().set_output(transform=\"pandas\")\n    train_scaled = scaler.fit_transform(train)\n    test_scaled = scaler.transform(test)\n    return train_scaled, test_scaled","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:37.991791Z","iopub.execute_input":"2023-08-10T22:01:37.992796Z","iopub.status.idle":"2023-08-10T22:01:38.005649Z","shell.execute_reply.started":"2023-08-10T22:01:37.992757Z","shell.execute_reply":"2023-08-10T22:01:38.00435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_train_test(X_train, X_test, impute_method=\"knn\", power_transform=False, one_hot_encode=True):\n        numeric_columns = X_train.select_dtypes([float,int]).columns\n        categorical_columns = X_train.select_dtypes(object).columns\n        \n        X_train.loc[:,numeric_columns], X_test.loc[:,numeric_columns] =\\\n        transform_and_scale(X_train.loc[:,numeric_columns],\n                            X_test.loc[:,numeric_columns], \n                            power_transform=power_transform)\n        if not categorical_columns.empty:\n            le = LabelEncoder()\n            le.fit(X_train.loc[:,categorical_columns])#changed in 5.0.0dev2\n            X_train.loc[:,categorical_columns] = le.transform(X_train.loc[:,categorical_columns])\n            X_test.loc[:,categorical_columns] = le.transform(X_test.loc[:,categorical_columns])\n        if contains_none(X_train) or contains_none(X_test): #changed in 5.0.0dev2\n            X_train, X_test = impute_train_test(X_train, X_test, impute_method=impute_method)      \n        if one_hot_encode and (not categorical_columns.empty):#Ã§Ä±kacak mÄ± sorun bi bak\n            enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform=\"pandas\")\n            X_train = pd.concat([X_train, enc.fit_transform(X_train.loc[:,categorical_columns])] ,axis=1)\n            X_test = pd.concat([X_test, enc.transform(X_test.loc[:,categorical_columns])] ,axis=1)\n            \n        #X_train = pd.get_dummies(X_train, columns=categorical_columns, dtype=int)\n        #X_test = pd.get_dummies(X_test, columns=categorical_columns, dtype=int) \n        return X_train, X_test","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:38.00729Z","iopub.execute_input":"2023-08-10T22:01:38.007742Z","iopub.status.idle":"2023-08-10T22:01:38.02019Z","shell.execute_reply.started":"2023-08-10T22:01:38.007696Z","shell.execute_reply":"2023-08-10T22:01:38.018973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_log_loss_scorer = make_scorer(balanced_logloss, greater_is_better=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:38.02386Z","iopub.execute_input":"2023-08-10T22:01:38.024607Z","iopub.status.idle":"2023-08-10T22:01:38.039045Z","shell.execute_reply.started":"2023-08-10T22:01:38.024546Z","shell.execute_reply":"2023-08-10T22:01:38.037578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_raw = df_train.drop(columns=[\"Id\", \"Class\"]).copy(deep=True)\ny = df_train.loc[:,[\"Class\"]].copy(deep=True)\nX_test_raw = df_test.drop(columns=[\"Id\"])\nX_train, X_test = process_train_test(X_raw, X_test_raw)\n\nX_us, y_us = CondensedNearestNeighbour(random_state=rand_state,n_jobs=-1).fit_resample(X_train, y) \n\nclf_meta = LGBMClassifier(boosting_type='goss', random_state=42, verbose=-1,\n                                           is_unbalance=True, n_jobs=-1)\nclassifiers_stack=[(\"XGB Random Forest Classifier\", xgb.XGBRFClassifier(objective=\"binary:logistic\", eval_metric=b_log_loss_scorer, \n                                                                       tree_method=\"hist\", random_state=rand_state,scale_pos_weight=4.71,\n                                                                       enable_categorical=True)),\n                   (\"Cat Boost\", CatBoostClassifier(verbose=0, objective=\"Logloss\", auto_class_weights='Balanced', thread_count=-1)),\n                   (\"LGBM\", LGBMClassifier(boosting_type='goss', random_state=42, verbose=-1,\n                                           is_unbalance=True, n_jobs=-1)),\n                   (\"QDA\", QuadraticDiscriminantAnalysis(priors=[0.5, 0.5])),\n                   (\"LDA\", LinearDiscriminantAnalysis(priors=[0.5, 0.5])),\n                  ]\nclf_stack = StackingClassifier(estimators=classifiers_stack, \n                               stack_method=\"predict_proba\",\n                               passthrough=False,\n                               final_estimator=clf_meta)\nmodel =clf_stack \nmodel.fit(X_us, y_us.to_numpy())\n\nprob_all = model.predict_proba(X_test)\ncls_0 = prob_all[:,0]\ncls_1 = prob_all[:,1]\nsubmission = df_test.loc[:,[\"Id\"]].copy(deep=True)\nsubmission[\"class_0\"] = cls_0\nsubmission[\"class_1\"] = cls_1\nprint(prob_all)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:01:38.041124Z","iopub.execute_input":"2023-08-10T22:01:38.041657Z","iopub.status.idle":"2023-08-10T22:02:11.219275Z","shell.execute_reply.started":"2023-08-10T22:01:38.041599Z","shell.execute_reply":"2023-08-10T22:02:11.218218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:02:11.22061Z","iopub.execute_input":"2023-08-10T22:02:11.220973Z","iopub.status.idle":"2023-08-10T22:02:11.244023Z","shell.execute_reply.started":"2023-08-10T22:02:11.220944Z","shell.execute_reply":"2023-08-10T22:02:11.242763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:02:11.245438Z","iopub.execute_input":"2023-08-10T22:02:11.245979Z","iopub.status.idle":"2023-08-10T22:02:11.260319Z","shell.execute_reply.started":"2023-08-10T22:02:11.245932Z","shell.execute_reply":"2023-08-10T22:02:11.25892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
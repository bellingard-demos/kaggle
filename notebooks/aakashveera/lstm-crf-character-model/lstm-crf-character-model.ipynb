{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall keras -y\n!pip install keras==2.2.4\n\n!pip uninstall tensorflow -y\n!pip install tensorflow==1.14.0\n\n!pip install git+https://www.github.com/keras-team/keras-contrib.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport nltk\nfrom nltk.tokenize import WordPunctTokenizer\nnltk.download('punkt')\n\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1 = pd.read_csv(\"/kaggle/input/entity-annotated-corpus/ner_dataset.csv\", encoding=\"latin1\")\ndf_1 = df_1.fillna(method=\"ffill\")\n\ndf_1['Sentence #'] = df_1['Sentence #'].apply(lambda x: x.split(': ')[1]).astype(int)\ndf_1.drop(['POS','Tag'],axis=1,inplace=True)\n\ndf_1.columns = ['Sentence','Word']\n\ndf_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def annotater(arg):\n    caps = first = small = misc = 0\n    n = len(arg)\n    \n    for i in range(n):\n        if 65<=ord(arg[i])<=90:\n            if i==0:\n                first = 1\n            caps += 1\n            \n        elif 97<=ord(arg[i])<=122:\n            small += 1\n            \n        else:\n            misc += 1\n            \n    if small==n:\n        return 's'\n    elif caps==n:\n        return 'c'\n    elif first + small==n:\n        return 'f'\n    elif misc>0:\n        return 'm'\n    else:\n        return 'f'\n    \ndf_1['Target'] = df_1['Word'].apply(annotater)\ndf_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reader(filename):\n    file =  open(filename)\n    lines = file.readlines()\n    text = ''\n    for i in lines:\n        text += i\n    return text\n\nenglish = reader(\"/kaggle/input/english/english.txt\")\nenglish_sentences = english.split('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(arg):\n    arg += '.'\n    tokenizer = WordPunctTokenizer() \n    return tokenizer.tokenize(arg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []\nnumbers = []\nfor n,i in enumerate(english_sentences):\n    tokens = tokenizer(i)\n    for i in tokens:\n        sentences.append(i)\n    for i in range(len(tokens)):\n        numbers.append(47960+n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame(numbers,columns=['Sentence'])\ndf2['Word'] = sentences\ndf2['Target'] = df2['Word'].apply(annotater)\n\ndf = pd.concat([df_1,df2],axis=0)\ndf['Word'] = df['Word'].apply(lambda x: x.lower())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {}\n\nfor n,i in enumerate(df['Word'].unique()):\n    word2idx[i] = n+2\n\nword2idx[\"PAD\"]= 0\nword2idx[\"UNK\"]=1\n   \nidx2word = {v:k for k,v in word2idx.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2tag = {\"P\":0,'f':1,'c':2,'s':3,'m':4}\n\ntag2text = {0:\"P\",1:'f',2:'c',3:'s',4:'m'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(arg):\n    if arg=='f':\n        return 1\n    elif arg=='c':\n        return 2\n    elif arg=='s':\n        return 3\n    elif arg=='m':\n        return 4\n\ndf['Target'] = df['Target'].apply(encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_next(n):\n    values = df[df['Sentence']==n][['Word','Target']].values\n    return values[:,0],values[:,1]\n\nX_word = []\ny_word = []\n\nfor i in tqdm(range(df['Sentence'].values[-1])):\n    if i+1==22480:\n        continue\n    x_,y_ = get_next(i+1)\n    X_word.append(np.array([word2idx[i] for i in x_]))\n    y_word.append(np.array(y_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 80\nMAX_LEN_CHAR = 10\nEMBEDDING = 300\nn_words = len(word2idx)\nBATCH_SIZE = 32\nEPOCHS = 5\n\nX_word_pad = pad_sequences(X_word,MAX_LEN,padding=\"post\", value=0)\ny_word_pad = pad_sequences(y_word,MAX_LEN,padding=\"post\", value=0)\ny_word_pad = [to_categorical(i, num_classes=4+1) for i in y_word_pad]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chars = set([char for word in list(word2idx.keys()) for char in word])\n\nn_chars = len(chars)\n\nprint(n_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char2idx = {c: i + 2 for i, c in enumerate(chars)}\nchar2idx[\"UNK\"] = 1\nchar2idx[\"PAD\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_char = []\nfor sentence in tqdm(X_word):\n    sent_seq = []\n    for i in range(MAX_LEN):\n        word_seq = []\n        for j in range(MAX_LEN_CHAR):\n            try:\n                word_seq.append(char2idx.get(idx2word[sentence[i]][j]))\n            except:\n                word_seq.append(char2idx.get(\"PAD\"))\n        sent_seq.append(word_seq)\n    X_char.append(np.array(sent_seq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_char, X_test_char, y_train, y_test = train_test_split(X_char, y_word_pad, test_size=0.2,random_state=42)\n\nX_train_word, X_test_word, _, _ = train_test_split(X_word_pad, y_word_pad, test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense,concatenate, SpatialDropout1D, TimeDistributed, Dropout, Bidirectional\nfrom keras_contrib.layers import CRF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"word_in = Input(shape=(MAX_LEN,))\n\nemb_word = Embedding(input_dim=n_words, \n                  output_dim=EMBEDDING, \n                  input_length=MAX_LEN,\n                  mask_zero=True)(word_in)\n\nchar_in = Input(shape=(MAX_LEN,MAX_LEN_CHAR))\n\nemb_char = TimeDistributed(Embedding(input_dim=len(char2idx), \n                  output_dim=20, \n                  input_length=MAX_LEN_CHAR,\n                  mask_zero=True))(char_in)\n\nchar_enc = TimeDistributed(LSTM(units=80, return_sequences=False,\n                                recurrent_dropout=0.3))(emb_char)\n\nx = concatenate([emb_word, char_enc])\nx = SpatialDropout1D(0.2)(x)\n\nmodel = Bidirectional(LSTM(units=50, \n                           return_sequences=True, \n                           recurrent_dropout=0.2))(x)\n\nmodel = TimeDistributed(Dense(50, activation=\"relu\"))(model)\ncrf = CRF(5)\nout = crf(model)\n\nmodel = Model([word_in, char_in], out)\nmodel.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([X_train_word,\n                    np.array(X_train_char)],\n                    np.array(y_train),\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['crf_viterbi_accuracy'])\nplt.plot(history.history['val_crf_viterbi_accuracy'])\n\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict([X_test_word,\n                    np.array(X_test_char)])\npred = np.argmax(pred, axis=-1)\ny_true = np.argmax(y_test, -1)\n\ny_true = [[tag2text[j] for j in i] for i in y_true]\npred = [[tag2text[j] for j in i] for i in pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sklearn-crfsuite","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn_crfsuite.metrics import flat_classification_report\n\nprint(flat_classification_report(y_pred=pred, y_true=y_true))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decoder(labels,tokens):\n    decoded = []\n    for i in range(len(tokens)):\n        if tag2text[labels[0][i]]=='f':\n            tmp = list(tokens[i])\n            tmp[0] = tmp[0].upper()\n            decoded.append(''.join(tmp))\n        elif tag2text[labels[0][i]]=='s':\n            decoded.append(tokens[i])\n        elif tag2text[labels[0][i]]=='c':\n            decoded.append(tokens[i].upper())\n        else:\n            decoded.append(tokens[i])\n    \n    text = []\n    for token in decoded:\n        \n        if token=='(':\n            text.append(token)\n        \n        elif token not in '!\"#$%&\\'*)+-./:,;<=>?@[\\\\]^_`{|}~':\n            text.append(token + ' ')\n        else:\n            if text[-1][-1]==' ':\n                tmp = text.pop()\n                text.append(tmp[:-1])\n            \n            text.append(token+' ')\n            \n    \n    return ''.join(text)\n\ndef encode(text):\n\n    tokens = WordPunctTokenizer().tokenize(text)\n    \n    encoded = []\n\n    for token in tokens:\n        if token in word2idx.keys():\n            encoded.append(word2idx[token])\n        else:\n            encoded.append(1)\n\n    encoded = encoded + [0 for x in range(80-len(encoded))]\n    \n    sent_seq = []\n    for i in range(MAX_LEN):\n        word_seq = []\n        for j in range(MAX_LEN_CHAR):\n            try:\n                word_seq.append(char2idx.get(idx2word[encoded[i]][j]))\n            except:\n                word_seq.append(char2idx.get(\"PAD\"))\n        sent_seq.append(np.array(word_seq))        \n    \n    return np.array(encoded),np.array(sent_seq),tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text = input(\"Enter the Text for case conversion: \").lower()\n\ntext = 'uSIng mATPLotlib and SeaBORN LIBraries is one OF THE best IDea to do Eda.'.lower()\n\nencoded_word,encoded_char,tokens = encode(text)\n\nlabels = np.argmax(model.predict([encoded_word.reshape(1,80),encoded_char.reshape(1,80,10)]),axis=-1)\ntext = decoder(labels,tokens)\nprint()\nprint(\"The converted text is: \",text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text = input(\"Enter the Text for case conversion: \").lower()\n\ntext = 'people in india started using english as thier primary language next to hindi.'\n\nencoded_word,encoded_char,tokens = encode(text)\n\nlabels = np.argmax(model.predict([encoded_word.reshape(1,80),encoded_char.reshape(1,80,10)]),axis=-1)\ntext = decoder(labels,tokens)\nprint()\nprint(\"The converted text is: \",text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip uninstall keras -y\n!pip install keras==2.2.4\n\n!pip uninstall tensorflow -y\n!pip install tensorflow==1.14.0\n\n!pip install git+https://www.github.com/keras-team/keras-contrib.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport nltk\nfrom nltk.tokenize import WordPunctTokenizer\nnltk.download('punkt')\n\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1 = pd.read_csv(\"/kaggle/input/entity-annotated-corpus/ner_dataset.csv\", encoding=\"latin1\")\ndf_1 = df_1.fillna(method=\"ffill\")\n\ndf_1['Sentence #'] = df_1['Sentence #'].apply(lambda x: x.split(': ')[1]).astype(int)\ndf_1.drop(['POS','Tag'],axis=1,inplace=True)\n\ndf_1.columns = ['Sentence','Word']\n\ndf_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def annotater(arg):\n    caps = first = small = misc = 0\n    n = len(arg)\n    \n    for i in range(n):\n        if 65<=ord(arg[i])<=90:\n            if i==0:\n                first = 1\n            caps += 1\n            \n        elif 97<=ord(arg[i])<=122:\n            small += 1\n            \n        else:\n            misc += 1\n            \n    if small==n:\n        return 's'\n    elif caps==n:\n        return 'c'\n    elif first + small==n:\n        return 'f'\n    elif misc>0:\n        return 'm'\n    else:\n        return 'f'\n    \ndf_1['Target'] = df_1['Word'].apply(annotater)\ndf_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reader(filename):\n    file =  open(filename)\n    lines = file.readlines()\n    text = ''\n    for i in lines:\n        text += i\n    return text\n\nenglish = reader(\"/kaggle/input/english/english.txt\")\nenglish_sentences = english.split('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(arg):\n    arg += '.'\n    tokenizer = WordPunctTokenizer() \n    return tokenizer.tokenize(arg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []\nnumbers = []\nfor n,i in enumerate(english_sentences):\n    tokens = tokenizer(i)\n    for i in tokens:\n        sentences.append(i)\n    for i in range(len(tokens)):\n        numbers.append(47960+n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame(numbers,columns=['Sentence'])\ndf2['Word'] = sentences\ndf2['Target'] = df2['Word'].apply(annotater)\n\ndf = pd.concat([df_1,df2],axis=0)\ndf['Word'] = df['Word'].apply(lambda x: x.lower())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {}\n\nfor n,i in enumerate(df['Word'].unique()):\n    word2idx[i] = n+2\n\nword2idx[\"PAD\"]= 0\nword2idx[\"UNK\"]=1\n   \nidx2word = {v:k for k,v in word2idx.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2tag = {\"P\":0,'f':1,'c':2,'s':3,'m':4}\n\ntag2text = {0:\"P\",1:'f',2:'c',3:'s',4:'m'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(arg):\n    if arg=='f':\n        return 1\n    elif arg=='c':\n        return 2\n    elif arg=='s':\n        return 3\n    elif arg=='m':\n        return 4\n\ndf['Target'] = df['Target'].apply(encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_next(n):\n    values = df[df['Sentence']==n][['Word','Target']].values\n    return values[:,0],values[:,1]\n\nX = []\ny = []\n\nfor i in tqdm(range(df['Sentence'].values[-1])):\n    if i+1==22480:\n        continue\n    x_,y_ = get_next(i+1)\n    X.append(np.array([word2idx[i] for i in x_]))\n    y.append(np.array(y_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 80\nEMBEDDING = 300\nn_words = len(word2idx)\nBATCH_SIZE = 32\nEPOCHS = 5\n\nX = pad_sequences(X,MAX_LEN,padding=\"post\", value=0)\ny = pad_sequences(y,MAX_LEN,padding=\"post\", value=0)\n\ny = [to_categorical(i, num_classes=4+1) for i in y]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom keras_contrib.layers import CRF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape=(MAX_LEN,))\n\nmodel = Embedding(input_dim=n_words, \n                  output_dim=EMBEDDING, \n                  input_length=80,\n                  mask_zero=True)(inputs)\n\nmodel = Bidirectional(LSTM(units=50, \n                           return_sequences=True, \n                           recurrent_dropout=0.2))(model)\n\nmodel = TimeDistributed(Dense(50, activation=\"relu\"))(model)\ncrf = CRF(5)\nout = crf(model)\n\nmodel = Model(inputs, out)\nmodel.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, np.array(y_train), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['crf_viterbi_accuracy'])\nplt.plot(history.history['val_crf_viterbi_accuracy'])\n\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test)\npred = np.argmax(pred, axis=-1)\ny_true = np.argmax(y_test, -1)\n\ny_true = [[tag2text[j] for j in i] for i in y_true]\npred = [[tag2text[j] for j in i] for i in pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sklearn-crfsuite","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn_crfsuite.metrics import flat_classification_report\n\nprint(flat_classification_report(y_pred=pred, y_true=y_true))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decoder(labels,tokens):\n    decoded = []\n    for i in range(len(tokens)):\n        if tag2text[labels[0][i]]=='f':\n            tmp = list(tokens[i])\n            tmp[0] = tmp[0].upper()\n            decoded.append(''.join(tmp))\n        elif tag2text[labels[0][i]]=='s':\n            decoded.append(tokens[i])\n        elif tag2text[labels[0][i]]=='c':\n            decoded.append(tokens[i].upper())\n        else:\n            decoded.append(tokens[i])\n    \n    text = []\n    for token in decoded:\n        \n        if token=='(':\n            text.append(token)\n        \n        elif token not in '!\"#$%&\\'*)+-./:,;<=>?@[\\\\]^_`{|}~':\n            text.append(token + ' ')\n        else:\n            if text[-1][-1]==' ':\n                tmp = text.pop()\n                text.append(tmp[:-1])\n            \n            text.append(token+' ')\n            \n    \n    return ''.join(text)\n\ndef encode(text):\n\n    tokens = WordPunctTokenizer().tokenize(text)\n    \n    encoded = []\n\n    for token in tokens:\n        if token in word2idx.keys():\n            encoded.append(word2idx[token])\n        else:\n            encoded.append(1)\n\n    encoded = encoded + [0 for x in range(80-len(encoded))]\n    \n    return encoded,tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text = input(\"Enter the Text for case conversion: \").lower()\n\ntext = 'uSIng mATPLotlib and SeaBORN LIBraries is one OF THE best IDea to do Eda.'.lower()\n\nencoded,tokens = encode(text)\n\nlabels = np.argmax(model.predict(np.array(encoded).reshape(1,80)),axis=-1)\n\ntext = decoder(labels,tokens)\nprint()\nprint(\"The converted text is: \",text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text = input(\"Enter the Text for case conversion: \").lower()\n\ntext = 'people in india started using english as thier primary language next to hindi.'\n\nencoded,tokens = encode(text)\n\nlabels = np.argmax(model.predict(np.array(encoded).reshape(1,80)),axis=-1)\n\ntext = decoder(labels,tokens)\nprint()\nprint(\"The converted text is: \",text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
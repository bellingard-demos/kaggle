{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"*Last edit by DLao - 2019*\n\n\n\n\n\n<br>\n<br>\n\n\n![](http://bathvenuefinder.co.uk/web/wp-content/uploads/2016/02/The-Porter-fine-food.jpg)\n\n# Amazon fine food review - Sentiment analysis\n\nThe analysis is to study Amazon food review from customers, and try to predict whether a review is positive or negative. The dataset contains **more than 500K reviews** with number of upvotes & total votes to those comments.\n\n## Table of Content\n\n* Data preparation\n\n* Score prediction\n  - Logistic regression model on word count\n  - Logistic regression model on TFIDF\n  - Logistic regression model on TFIDF + ngram\n  \n* Upvote prediction\n  - Data preview\n  - Resampling due to imbalanced data\n  - Logistic regression model on word count\n  - Logistic regression model on TFIDF + ngram\n  - Study on non-context features\n  \n* In-depth study on user behaviour *(Edit on 2017/10/18)*\n  \n<br>\nI will keep updating this notebook. Feel free to fork and **Upvote** if you find it useful in some ways!","metadata":{"_uuid":"f458204a207f4d96fe334270ee778e079a1014a2","_cell_guid":"dd1ad847-c6ae-44a4-bbda-ea78d0769190"}},{"cell_type":"markdown","source":"## Data preparation","metadata":{"_uuid":"d5fc32bfdd6b8f35291ddc358df228a064cc1c16","_cell_guid":"b875d60b-7d37-4845-bc56-1527988765c6"}},{"cell_type":"markdown","source":"Let's load some library needed throughout the analysis:","metadata":{"_uuid":"22b3e2fd24784e7097d12c87f5817bf9bbc6f41c","_cell_guid":"d1c22d63-35ce-4a7d-b15b-345e6a3e11f4"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom string import punctuation\nfrom sklearn import svm\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk import ngrams\nfrom itertools import chain\nfrom wordcloud import WordCloud\n","metadata":{"_uuid":"12e6c6dedd58c81029e7ac240d8dbc9c4375e7e6","_cell_guid":"a77dbc40-11a9-42b1-8cbe-5e47757f0ee7","execution":{"iopub.status.busy":"2022-03-15T05:56:46.367119Z","iopub.execute_input":"2022-03-15T05:56:46.36747Z","iopub.status.idle":"2022-03-15T05:56:48.487837Z","shell.execute_reply.started":"2022-03-15T05:56:46.367388Z","shell.execute_reply":"2022-03-15T05:56:48.48696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we load in the dataset and add some columns for upvote metrics:","metadata":{"_uuid":"34654f16cb6e425ed285dca51aea0f9a026c7c14","_cell_guid":"0e97c861-5ea5-47bc-8b0a-e2fad750b51a"}},{"cell_type":"code","source":"odf = pd.read_csv('../input/Reviews.csv')\nodf['Helpful %'] = np.where(odf['HelpfulnessDenominator'] > 0, odf['HelpfulnessNumerator'] / odf['HelpfulnessDenominator'], -1)\nodf['% Upvote'] = pd.cut(odf['Helpful %'], bins = [-1, 0, 0.2, 0.4, 0.6, 0.8, 1.0], labels = ['Empty', '0-20%', '20-40%', '40-60%', '60-80%', '80-100%'], include_lowest = True)\nodf.head()","metadata":{"_uuid":"f176f9a43baa485feca92e643b24abbea7094a4a","_cell_guid":"9e9eb800-a0b7-4026-944e-a0fbce39a959","execution":{"iopub.status.busy":"2022-03-15T05:56:55.544299Z","iopub.execute_input":"2022-03-15T05:56:55.544887Z","iopub.status.idle":"2022-03-15T05:57:03.422939Z","shell.execute_reply.started":"2022-03-15T05:56:55.544824Z","shell.execute_reply":"2022-03-15T05:57:03.422183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_s = odf.groupby(['Score', '% Upvote']).agg({'Id': 'count'})\ndf_s = df_s.unstack()\ndf_s.columns = df_s.columns.get_level_values(1)\nfig = plt.figure(figsize=(15,10))\n\nsns.heatmap(df_s[df_s.columns[::-1]].T, cmap = 'YlGnBu', linewidths=.5, annot = True, fmt = 'd', cbar_kws={'label': '# reviews'})\nplt.yticks(rotation=0)\nplt.title('How helpful users find among user scores')","metadata":{"_uuid":"382316d23c2d4856431204abcdbcefbc6844764d","_cell_guid":"20e9de30-7e63-4d77-8f31-4df4080bfadf","execution":{"iopub.status.busy":"2022-03-04T05:08:46.453082Z","iopub.execute_input":"2022-03-04T05:08:46.453547Z","iopub.status.idle":"2022-03-04T05:08:47.018496Z","shell.execute_reply.started":"2022-03-04T05:08:46.453508Z","shell.execute_reply":"2022-03-04T05:08:47.017855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Key message from above:\n* Reviews are skewed towards positive\n* More than half of the reviews are with zero votes\n* Many people agree with score 5 reviews","metadata":{"_uuid":"c99b3bd4843231d6e2c0cfb5f499e229a7274b72","_cell_guid":"7ed3c36d-c141-43c0-bacb-da3019895a3c"}},{"cell_type":"markdown","source":"Then we get rid of score 3 reviews (neutral), and separate the remaining reviews into binary class (1 = positive, 0 = negative):","metadata":{"_uuid":"912f4b1a7239a19099f77559e91965928eb551d9","_cell_guid":"6d540e4d-4d44-4d12-8ecb-5c96b19e16f5"}},{"cell_type":"code","source":"df = odf[odf['Score'] != 3]\nX = df['Text']\ny_dict = {1:0, 2:0, 4:1, 5:1}\ny = df['Score'].map(y_dict)","metadata":{"_uuid":"75303d78f841c0a0bc26259b82889ee80294e777","_cell_guid":"d535a0cd-0ec3-4fd9-8cbd-2aeba7072153","execution":{"iopub.status.busy":"2022-03-04T05:08:51.384331Z","iopub.execute_input":"2022-03-04T05:08:51.38496Z","iopub.status.idle":"2022-03-04T05:08:51.469241Z","shell.execute_reply.started":"2022-03-04T05:08:51.384904Z","shell.execute_reply":"2022-03-04T05:08:51.46854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score prediction\n### Logistic regression model on word count","metadata":{"_uuid":"a9162a034912652c3a3e471ad8a29fdb24e3c89f","_cell_guid":"c42b5b60-7746-41dc-9662-19bc690b9121"}},{"cell_type":"code","source":"c = CountVectorizer(stop_words = 'english')\n\ndef text_fit(X, y, model,clf_model,coef_show=1):\n    \n    X_c = model.fit_transform(X)\n    print('# features: {}'.format(X_c.shape[1]))\n    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0)\n    print('# train records: {}'.format(X_train.shape[0]))\n    print('# test records: {}'.format(X_test.shape[0]))\n    clf = clf_model.fit(X_train, y_train)\n    acc = clf.score(X_test, y_test)\n    print ('Model Accuracy: {}'.format(acc))\n    \n    if coef_show == 1: \n        w = model.get_feature_names()\n        coef = clf.coef_.tolist()[0]\n        coeff_df = pd.DataFrame({'Word' : w, 'Coefficient' : coef})\n        coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n        print('')\n        print('-Top 20 positive-')\n        print(coeff_df.head(20).to_string(index=False))\n        print('')\n        print('-Top 20 negative-')        \n        print(coeff_df.tail(20).to_string(index=False))\n    \n    \ntext_fit(X, y, c, LogisticRegression())","metadata":{"_uuid":"7a4ac414ac566bae7bb3e5550da895c7f07e7569","_cell_guid":"577d8073-3b61-4d81-88ed-839742b9e799","execution":{"iopub.status.busy":"2022-03-04T05:08:57.028745Z","iopub.execute_input":"2022-03-04T05:08:57.029403Z","iopub.status.idle":"2022-03-04T05:11:30.550378Z","shell.execute_reply.started":"2022-03-04T05:08:57.029348Z","shell.execute_reply":"2022-03-04T05:11:30.548693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy is around 93.9% - not bad. However we notice that some of those significant coefficients are not meaningful, e.g. 280mg.\n\nLet's also look at the base line accuracy (predicting with majority class, in this case positive class):","metadata":{"_uuid":"26b218af8cd7c0c14d0d6ca5eec459a27b3fbdc7","_cell_guid":"edcc6ec5-e764-4acc-ae37-1a69f23fa297"}},{"cell_type":"code","source":"text_fit(X, y, c, DummyClassifier(),0)","metadata":{"_uuid":"da62f498dd485b62cd7129804d36ab234b10904c","_cell_guid":"fd6edba5-9dc8-47e5-a6bb-b65d6ce1f5e8","execution":{"iopub.status.busy":"2022-03-04T05:14:44.340542Z","iopub.execute_input":"2022-03-04T05:14:44.340896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic regression model on TFIDF","metadata":{"_uuid":"154cca9f52d1e77649f2e12140c6f912abcd0117","_cell_guid":"b106ed3d-779e-4ee6-b1ee-4d64455d4d37"}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(stop_words = 'english')\ntext_fit(X, y, tfidf, LogisticRegression())\n","metadata":{"_uuid":"54023303417b1b6bcc32ebfa2dd973ec96c9a0a1","_cell_guid":"cb526e31-a125-41d2-943a-7a3613f75624","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accurancy is roughly the same - 93.5%. However we notice that the significant words make much more sense now, with higher coefficient magnitude as well!","metadata":{"_uuid":"871fe6d0df70d4825ca6b7bb24d391e1c5ab7368","_cell_guid":"fee2b5d6-d330-421f-9a33-a72fa51d72d2"}},{"cell_type":"markdown","source":"### Logistic regression model on TFIDF + ngram","metadata":{"_uuid":"192a21df420525cac90d288aabe64e74d4829aff","_cell_guid":"4e3d29af-8979-48c9-b072-d94964e4afe3"}},{"cell_type":"code","source":"tfidf_n = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english')\ntext_fit(X, y, tfidf_n, LogisticRegression())","metadata":{"_uuid":"72916d30dca26b6d5af7cec81bf6cd8a10ab64ee","_cell_guid":"4a6f8305-8426-4fb6-97ed-7ff639f27d42","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding ngram parameter, we are able to understand phrase like \"not good\". Therefore the accuracy goes up a bit to 94.5%. We can see \"highly recommend\" is on 20th place of positive words","metadata":{"_uuid":"16d86e008a261cf09370ea25feb6944f9a9ccc32","_cell_guid":"9e374d81-ee7a-433a-8ed1-5d5aeea9f279"}},{"cell_type":"markdown","source":"## Upvote prediction\n### Data preview","metadata":{"_uuid":"22af55dec11ef18c2eb3b94ba53c7639e09324c4","_cell_guid":"ddb94e95-3d7a-4a99-b525-8bbef54a8bbe"}},{"cell_type":"markdown","source":"This study is to see whether there is pattern found for unpopular (downvote) comments by users (might be used to boost sales).\nWe will be focusing on score 5 reviews, and get rid of comments with neutral votes:","metadata":{"_uuid":"31f220dd27e37bd087bc501d9e56b728010a39b1","_cell_guid":"b568e9ad-6495-42ee-a377-3979a8e96cc9"}},{"cell_type":"code","source":"df = df[df['Score'] == 5]\ndf = df[df['% Upvote'].isin(['0-20%', '20-40%', '60-80%', '80-100%'])]\ndf.shape\n\nX = df['Text']\ny_dict = {'0-20%': 0, '20-40%': 0, '60-80%': 1, '80-100%': 1}\ny = df['% Upvote'].map(y_dict)\n\nprint('Class distribution:')\nprint(y.value_counts())","metadata":{"_uuid":"5ca846ee71a6ea947d7874b1737f2e87f32aa3b7","_cell_guid":"4159fa98-0cde-4934-ad74-046131e44608","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target class is highly skewed to positive (upvotes). Let's resample the data to get balanced data:","metadata":{"_uuid":"71ac359288e1b2610ddc42236b1253e16a0d3a1c","_cell_guid":"8dce7aef-6c66-4e8e-bfca-540383bdd709"}},{"cell_type":"code","source":"df_s = pd.DataFrame(data = [X,y]).T\n\nDownvote_records = len(df_s[df_s['% Upvote'] == 0])\nDownvote_indices = np.array(df_s[df_s['% Upvote'] == 0].index)\n\nUpvote_indices = df_s[df_s['% Upvote'] == 1].index\n\nrandom_upvote_indices = np.random.choice(Upvote_indices, Downvote_records, replace = False)\nrandom_upvote_indices = np.array(random_upvote_indices)\n\nunder_sample_indices = np.concatenate([Downvote_indices,random_upvote_indices])\n\nunder_sample_data = df_s.ix[under_sample_indices, :]\nX_u = under_sample_data['Text']\nunder_sample_data['% Upvote'] = under_sample_data['% Upvote'].astype(int)\ny_u = under_sample_data['% Upvote']\n\n\nprint(\"Percentage of upvote transactions: \", len(under_sample_data[under_sample_data['% Upvote'] == 1])/len(under_sample_data))\nprint(\"Percentage of downvote transactions: \", len(under_sample_data[under_sample_data['% Upvote'] == 0])/len(under_sample_data))\nprint(\"Total number of records in resampled data: \", len(under_sample_data))","metadata":{"_uuid":"3696b4f918c20e88319b8acb1be1153c92f66c3b","_cell_guid":"33b320e7-80c0-4f04-899f-1f2c1e81e563","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic regression model on word count","metadata":{"_uuid":"80f9dad3271fd1ec9da5777e3c8b6f974242915e","_cell_guid":"30ef9791-cb76-436b-95b1-8768664d8ba4"}},{"cell_type":"code","source":"c = CountVectorizer(stop_words = 'english')\n\ntext_fit(X_u, y_u, c, LogisticRegression())","metadata":{"_uuid":"38d940aee1abb2d87cb10c79516f27f0e62e7b18","_cell_guid":"65ffcfb6-635d-437d-9f08-0eadb9a6957c","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The words look no sense at all, and the coefficients are very small.","metadata":{"_uuid":"2a1b0573fa2844772b47244fc0ebdc161ab0a0b6","_cell_guid":"20296e55-ff5a-46c2-84fa-d14db01a7b8a"}},{"cell_type":"markdown","source":"### Logistic regression model on TFIDF + ngram","metadata":{"_uuid":"b2eff77a4b50bfe114df381ca1bba7a0a42fd4ab","_cell_guid":"c4ed4015-6e67-4073-b0c9-1a56bcea6098"}},{"cell_type":"code","source":"tfidf_n = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english')\n\ntext_fit(X_u, y_u, tfidf_n, LogisticRegression())","metadata":{"_uuid":"b9bb5e65c4f20416739d68800b91d546feb0a7e4","_cell_guid":"d72dcc7d-74b0-4c5d-92a3-87e4034fb0ce","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar to count metric, the accuracy isn't very high and words are not meaningful.","metadata":{"_uuid":"98a02ac427ac71302f8d9ffd134bd3c2670afd99","_cell_guid":"8151e536-ef85-4d64-8ebf-10fbeb8c76cc"}},{"cell_type":"markdown","source":"### Study on non-context features","metadata":{"_uuid":"7254306b3a79653a783012031a28452fa610409b","_cell_guid":"8177abd0-2cfd-4c26-b82f-2bf2edc199f8"}},{"cell_type":"markdown","source":"Let's look at some examples of upvote and downvote comments:","metadata":{"_uuid":"b19f5f3dd9f233d39c6e1cdb44a514637dde6841","_cell_guid":"c1b58b3a-43aa-4215-8a5b-cd5261cee2e8"}},{"cell_type":"code","source":"#pd.set_option('display.max_colwidth', -1)\nprint('Downvote score 5 comments examples:')\nprint(under_sample_data[under_sample_data['% Upvote']==0]['Text'].iloc[:100:20])\nprint('Upvote score 5 comments examples')\nprint(under_sample_data[under_sample_data['% Upvote']==1]['Text'].iloc[:100:20])","metadata":{"_uuid":"a1b7fed1cff92a13db1289a018900a585574c21f","_cell_guid":"b056c0a1-8976-4571-ad47-3bc2c45d6fa2","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time to extract some possible features:","metadata":{"_uuid":"f4d7310c83e84a1846df1f9f84214e15c0b42151","_cell_guid":"5841a2fe-e967-485f-ade4-3d16e4d8d383"}},{"cell_type":"code","source":"under_sample_data['word_count'] = under_sample_data['Text'].apply(lambda x: len(x.split()))\nunder_sample_data['capital_count'] = under_sample_data['Text'].apply(lambda x: sum(1 for c in x if c.isupper()))\nunder_sample_data['question_mark'] = under_sample_data['Text'].apply(lambda x: sum(1 for c in x if c == '?'))\nunder_sample_data['exclamation_mark'] = under_sample_data['Text'].apply(lambda x: sum(1 for c in x if c == '!'))\nunder_sample_data['punctuation'] = under_sample_data['Text'].apply(lambda x: sum(1 for c in x if c in punctuation))\n\nprint(under_sample_data.groupby('% Upvote').agg({'word_count': 'mean', 'capital_count': 'mean', 'question_mark': 'mean', 'exclamation_mark': 'mean', 'punctuation': 'mean'}).T)\n\nX_num = under_sample_data[under_sample_data.columns.difference(['% Upvote', 'Text'])]\ny_num = under_sample_data['% Upvote']\n","metadata":{"_uuid":"d1e242d3b9fc260e8246598bf94473894ea2a9bd","_cell_guid":"fc10f1df-c999-4580-8b3e-ece62a9f4763","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we train the model to predict:","metadata":{"_uuid":"e817b1ba10decfec707f02a69764174f0782efbe","_cell_guid":"a273f109-5c2e-40d0-8dcb-51f75783dcf9"}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_num, y_num, random_state=0)\n\nclf_lr = LogisticRegression().fit(X_train, y_train)\nacc_lr = clf_lr.score(X_test, y_test)\nprint('Logistic Regression accuracy: {}'.format(acc_lr))\n\nclf_svm = svm.SVC().fit(X_train, y_train)\nacc_svm = clf_svm.score(X_test, y_test)\nprint('SVM accuracy: {}'.format(acc_svm))","metadata":{"_uuid":"a3c3cd460cf154127fbe7577b174f31b48227b53","_cell_guid":"0aada9b1-e851-4001-aa6f-b2932f82986a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy is lower than context features we tried above, meaning the feature is not good enough to predict our target.","metadata":{"_uuid":"a781da0f073df2998107e36e66752ed5b5ebf33a","_cell_guid":"4fd116b9-fe41-433b-9d79-00a28f9ea86c"}},{"cell_type":"markdown","source":"## In-depth study on user behaviour","metadata":{"_uuid":"7fd8db193bc024e92266d5e883a73152366f5609","_cell_guid":"2f458f6a-806f-4b8c-891a-bead3abb248c"}},{"cell_type":"markdown","source":"This analysis will be carried out to focus on one specific user, on what he / she likes in terms of fine food, based on the reviews he / she had given in the past. This can be expanded to all users later on. \n\nFirst let's look at how many reviews each user gave in the past:","metadata":{"_uuid":"ac6f97ed677b2581b9da5d9102980fb0e988323b","_cell_guid":"36924dc3-12d8-431d-8da8-fc92f6893d5b"}},{"cell_type":"code","source":"df_user = odf.groupby(['UserId', 'ProfileName']).agg({'Score':['count', 'mean']})\ndf_user.columns = df_user.columns.get_level_values(1)\ndf_user.columns = ['Score count', 'Score mean']\ndf_user = df_user.sort_values(by = 'Score count', ascending = False)\nprint(df_user.head(10))\n","metadata":{"_uuid":"290c5adaf8560bddb9de09f8209beba77013d4c3","_cell_guid":"c698ccae-4423-4586-832c-07675ae4643d","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The user with most frequent reviews are \"CFH\" with 448 reviews and average score 4.54. Let's look at his review distribution:","metadata":{"_uuid":"152f88e34f56968f92d060854070f28ab8d69a84","_cell_guid":"76a4827d-9284-4b7d-a7db-6bca1c7195f6"}},{"cell_type":"code","source":"def plot_user(UserId):\n    df_1user = odf[odf['UserId'] == UserId]['Score']\n    df_1user_plot = df_1user.value_counts(sort=False)\n    ax = df_1user_plot.plot(kind = 'bar', figsize = (15,10), title = 'Score distribution of user {} review'.format(odf[odf['UserId'] == UserId]['ProfileName'].iloc[0]))\n\nplot_user('A3OXHLG6DIBRW8')","metadata":{"_uuid":"32353458bc0d3075a945f3717b29cf321932fd7d","_cell_guid":"2ea99724-28d9-48ac-b841-e8dbdb3875f6","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks user \"CFH\" prefer to give high score a lot. I would rather analyze a more neutral user, let's dig into that:","metadata":{"_uuid":"2c9caf2d64eae4f89e8f3c0e969d28e89ec871e9","_cell_guid":"2c4749bf-8af8-4af7-b96f-f224cada9615"}},{"cell_type":"code","source":"print(df_user[(df_user['Score mean']<3.5) & (df_user['Score mean']>2.5)].head())","metadata":{"_uuid":"d99e5622ab3ccbe804c8bb233e98dd82a643b9e9","_cell_guid":"2e09a303-8c2f-4632-bc3c-bc86827ce4cf","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A. Bennett seems a good target:","metadata":{"_uuid":"e85e3bcfec7b42bc6f0b90351a1a18e3ecc08444","_cell_guid":"bd9e9ea9-4b40-4424-8b25-40ae32c04daa"}},{"cell_type":"code","source":"plot_user('A2M9D9BDHONV3Y')","metadata":{"_uuid":"89514c32c39edad69e70fc564a29078cb5b60761","_cell_guid":"a7c92ba7-0259-447f-a13d-8b586f9fbb40","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"He seems like a better analysis target! Now let's dig deep into his reviews:","metadata":{"_uuid":"94993660cefaacbc71ffbf2e38a690c1fc23b859","_cell_guid":"ba36510e-0c03-472c-9e47-b37b1f4ad2b3"}},{"cell_type":"code","source":"def get_token_ngram(score, benchmark, userid='all'):\n\n    if userid != 'all':\n        df = odf[(odf['UserId'] == userid) & (odf['Score'] == score)]['Text']\n    else:\n        df = odf[odf['Score'] == score]['Text']\n        \n    count = len(df)\n    total_text = ' '.join(df)\n    total_text = total_text.lower()\n    stop = set(stopwords.words('english'))\n    total_text = nltk.word_tokenize(total_text)\n    total_text = [word for word in total_text if word not in stop and len(word) >= 3]\n    lemmatizer = WordNetLemmatizer()\n    total_text = [lemmatizer.lemmatize(w,'v') for w in total_text]\n    bigrams = ngrams(total_text,2)\n    trigrams = ngrams(total_text, 3)\n\n    # look at 2-gram and 3-gram together\n    combine = chain(bigrams, trigrams)\n    text = nltk.Text(combine)\n    fdist = nltk.FreqDist(text)\n    \n    # return only phrase occurs more than benchmark of his reviews\n    return sorted([(w,fdist[w],str(round(fdist[w]/count*100,2))+'%') for w in set(text) if fdist[w] >= count*benchmark], key=lambda x: -x[1])\n\n# score 1-5 reviews with this user\nindex = ['Phrase', 'Count', 'Occur %']\n\nfor j in range(1,6):\n    test = pd.DataFrame()\n    d = get_token_ngram(j, 0.25, 'A2M9D9BDHONV3Y')\n    print('score {} reviews most popular 2-gram / 3-gram:'.format(j))\n    for i in d:\n        test = test.append(pd.Series(i, index = index), ignore_index = True)\n    test = test.sort_values('Count', ascending=False)\n    print(test)\n\n","metadata":{"_uuid":"bdff6fb6209a93fa2e95468df38c77a8b1d487a1","_cell_guid":"2de54019-90b9-48db-819c-6c18a0350c7d","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like A. Bennett likes to use something like \"give it a try\" for score 2 reviews and \"highly recommend\" for score 5 reviews (without other frequent phrases). He likes using the phrase \"...\" a lot as well. Sample size is very small though.\n\nLet's look at full picture now instead of just one user:","metadata":{"_uuid":"226ae03b35efa71db8446f4652295c7a557d8656","_cell_guid":"0bbcac4b-78a6-4e8d-adf8-3a6a1ce04f9e"}},{"cell_type":"code","source":"# score 1-5 reviews with all users\nindex = ['Phrase', 'Count', 'Occur %']\n\nfor j in range(1,6):\n    test = pd.DataFrame()\n    # easier benchmark since we have many different users here, thus different phrase\n    d = get_token_ngram(j, 0.03)\n    print('score {} reviews most popular 2-gram / 3-gram:'.format(j))\n    for i in d:\n        test = test.append(pd.Series(i, index = index), ignore_index = True)\n    test = test.sort_values('Count', ascending=False)\n    print(test)","metadata":{"_uuid":"8290839463e5ede1665dfaf143ffb45e469b4888","_cell_guid":"33174a83-6b0a-4525-ae2c-894c2753a7b2","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Funny that people are more likely to attach HTML links (href=) for more positive reviews (3 and 4).\n<br>\nOther than that, not much useful discoveries except positive / negative tones, since people are very different in texting style. \n<br>\nMaybe we should focus on popular single **adjective** word people used for different score. First we modify the function a bit for easier implementation:","metadata":{"_uuid":"549ee31225e30abd609b6f7dc6e7f0e46b6e09fa","_cell_guid":"8c1cad58-69da-4e0d-b6fb-83259be25759"}},{"cell_type":"code","source":"def get_token_adj(score, benchmark, userid='all'):\n    \n    if userid != 'all':\n        df = odf[(odf['UserId'] == userid) & (odf['Score'] == score)]['Text']\n    else:\n        df = odf[odf['Score'] == score]['Text']\n        \n    count = len(df)\n    total_text = ' '.join(df)\n    total_text = total_text.lower()\n    stop = set(stopwords.words('english'))\n    total_text = nltk.word_tokenize(total_text)\n    total_text = [word for word in total_text if word not in stop and len(word) >= 3]\n    lemmatizer = WordNetLemmatizer()\n    total_text = [lemmatizer.lemmatize(w,'a') for w in total_text]\n    # get adjective only\n    total_text = [word for word, form in nltk.pos_tag(total_text) if form == 'JJ']\n    \n    text = nltk.Text(total_text)\n    fdist = nltk.FreqDist(text)\n    \n    # return only phrase occurs more than benchmark of his reviews\n    return sorted([(w,fdist[w],str(round(fdist[w]/count*100,2))+'%') for w in set(text) if fdist[w] >= count*benchmark], key=lambda x: -x[1])","metadata":{"_uuid":"09ae2694002b63c623bb03bca4c17eaf24f9c571","_cell_guid":"83300b5e-17fa-4aca-be29-14f97202c2be","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at user A. Bennett again:","metadata":{"_uuid":"66274784e2b763b90e3bc8688fb0770ba1dc14c3","_cell_guid":"0610e903-5df8-4b7c-b55c-9545e6067615"}},{"cell_type":"code","source":"# score 1-5 reviews with this user\nindex = ['Phrase', 'Count', 'Occur %']\n\nfor j in range(1,6):\n    test = pd.DataFrame()\n    d = get_token_adj(j, 0.25, 'A2M9D9BDHONV3Y')\n    print('score {} reviews most popular adjectives word:'.format(j))\n    for i in d:\n        test = test.append(pd.Series(i, index = index), ignore_index = True)\n    test = test.sort_values('Count', ascending=False)\n    print(test)","metadata":{"_uuid":"43b9c581046a482e31152d40ef7d8614fe286442","_cell_guid":"feabf506-2256-4f25-980f-816023fb1e98","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obviously much more useful information here, e.g. A. Bennett hates food that are too dry and expensive\n<br>\nLet's also look at big picture:","metadata":{"_uuid":"a46a240772c569cb4c78eb80bb37f9b65f13faf8","_cell_guid":"d9fba032-011a-4b82-a23c-bdd8c32497da"}},{"cell_type":"code","source":"# score 1-5 reviews with all users\nindex = ['Phrase', 'Count', 'Occur %']\n\nfor j in range(1,6):\n    test = pd.DataFrame()\n    d = get_token_adj(j, 0.05)\n    print('score {} reviews most popular adjectives word:'.format(j))\n    for i in d:\n        test = test.append(pd.Series(i, index = index), ignore_index = True)\n    test = test.sort_values('Count', ascending=False)\n    print(test)","metadata":{"_uuid":"83735d321f39d96314d1983f3ca754ecb7ba3d5e","_cell_guid":"ccbedb9b-4f0f-4735-bc27-d0d95374b6d4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Appreciate any comments!\n\nHope it is a fun read!","metadata":{"_uuid":"e441b552e976917b3762fe4ed17c6f1e8903d726","_cell_guid":"b78887ad-822f-4b3a-985c-91479747ee39"}}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q feature_engine autoviz dataprep","metadata":{"execution":{"iopub.status.busy":"2023-08-16T03:06:55.706363Z","iopub.execute_input":"2023-08-16T03:06:55.706827Z","iopub.status.idle":"2023-08-16T03:07:56.891619Z","shell.execute_reply.started":"2023-08-16T03:06:55.706781Z","shell.execute_reply":"2023-08-16T03:07:56.890226Z"},"_kg_hide-input":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom autoviz import AutoViz_Class\nfrom dataprep.datasets import load_dataset\nfrom dataprep.eda import create_report\n\nimport shap\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.encoding import RareLabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T03:08:14.974644Z","iopub.execute_input":"2023-08-16T03:08:14.975063Z","iopub.status.idle":"2023-08-16T03:08:28.670087Z","shell.execute_reply.started":"2023-08-16T03:08:14.975034Z","shell.execute_reply":"2023-08-16T03:08:28.668477Z"},"_kg_hide-input":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/global-daily-climate-data/weather.csv').drop_duplicates()\ndf_countries = pd.read_csv(\"/kaggle/input/global-daily-climate-data/countries.csv\").drop_duplicates()\ndf = df.set_index(\"country\").join(df_countries.set_index(\"country\")[['region']], how='left')\ndf = df[df['avg_temp_c']>-273]\nprint(df.shape)\ndf.sample(5).T","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:06.847913Z","iopub.execute_input":"2023-08-16T05:25:06.84903Z","iopub.status.idle":"2023-08-16T05:25:29.639519Z","shell.execute_reply.started":"2023-08-16T05:25:06.848979Z","shell.execute_reply":"2023-08-16T05:25:29.638016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:29.6418Z","iopub.execute_input":"2023-08-16T05:25:29.642175Z","iopub.status.idle":"2023-08-16T05:25:29.668814Z","shell.execute_reply.started":"2023-08-16T05:25:29.642144Z","shell.execute_reply":"2023-08-16T05:25:29.667167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:29.670838Z","iopub.execute_input":"2023-08-16T05:25:29.671259Z","iopub.status.idle":"2023-08-16T05:25:29.680132Z","shell.execute_reply.started":"2023-08-16T05:25:29.671227Z","shell.execute_reply":"2023-08-16T05:25:29.678639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = df.columns\ncols","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:29.684014Z","iopub.execute_input":"2023-08-16T05:25:29.685401Z","iopub.status.idle":"2023-08-16T05:25:29.696803Z","shell.execute_reply.started":"2023-08-16T05:25:29.685352Z","shell.execute_reply":"2023-08-16T05:25:29.695288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:29.698527Z","iopub.execute_input":"2023-08-16T05:25:29.699069Z","iopub.status.idle":"2023-08-16T05:25:29.717018Z","shell.execute_reply.started":"2023-08-16T05:25:29.699027Z","shell.execute_reply":"2023-08-16T05:25:29.715163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:29.719293Z","iopub.execute_input":"2023-08-16T05:25:29.719706Z","iopub.status.idle":"2023-08-16T05:25:31.254386Z","shell.execute_reply.started":"2023-08-16T05:25:29.719655Z","shell.execute_reply":"2023-08-16T05:25:31.253034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:31.255814Z","iopub.execute_input":"2023-08-16T05:25:31.25617Z","iopub.status.idle":"2023-08-16T05:25:31.269244Z","shell.execute_reply.started":"2023-08-16T05:25:31.256139Z","shell.execute_reply":"2023-08-16T05:25:31.267833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:31.270601Z","iopub.execute_input":"2023-08-16T05:25:31.27096Z","iopub.status.idle":"2023-08-16T05:25:32.541257Z","shell.execute_reply.started":"2023-08-16T05:25:31.27093Z","shell.execute_reply":"2023-08-16T05:25:32.540028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data visualisation","metadata":{}},{"cell_type":"code","source":"AV = AutoViz_Class()\nfilename = \"\"\ntarget_variable = 'avg_temp_c'\ncustom_plot_dir = \"custom_plot_directory\"\n\ndft = AV.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=target_variable,\n    dfte=df,\n    header=0,\n    verbose=1,\n    lowess=False,\n    chart_format=\"html\",\n    max_rows_analyzed=min([df.shape[0], 10**3]),\n    max_cols_analyzed=min([df.shape[1], 50]),\n    save_plot_dir=custom_plot_dir\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T03:19:07.017423Z","iopub.execute_input":"2023-08-16T03:19:07.017897Z","iopub.status.idle":"2023-08-16T03:19:30.04206Z","shell.execute_reply.started":"2023-08-16T03:19:07.017862Z","shell.execute_reply":"2023-08-16T03:19:30.039987Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\n# Define the list of file names\nfrom pathlib import Path\nfile_names = []\nfor file in Path(f'/kaggle/working/{custom_plot_dir}/{target_variable}/').glob('*.html'):\n    filename = str(file).split('/')[-1]\n    file_names.append(filename)\n\n# Loop through the list and display each HTML file\nfor file_name in file_names:\n    file_path = f'/kaggle/working/{custom_plot_dir}/{target_variable}/{file_name}'\n    with open(file_path, 'r') as file:\n        html_content = file.read()\n        display(HTML(html_content))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T03:19:33.274195Z","iopub.execute_input":"2023-08-16T03:19:33.274633Z","iopub.status.idle":"2023-08-16T03:19:33.471841Z","shell.execute_reply.started":"2023-08-16T03:19:33.274601Z","shell.execute_reply":"2023-08-16T03:19:33.470622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_report(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T03:20:05.922449Z","iopub.execute_input":"2023-08-16T03:20:05.922947Z","iopub.status.idle":"2023-08-16T03:21:57.933847Z","shell.execute_reply.started":"2023-08-16T03:20:05.922901Z","shell.execute_reply":"2023-08-16T03:21:57.931836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# null values ","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:32.54339Z","iopub.execute_input":"2023-08-16T05:25:32.543936Z","iopub.status.idle":"2023-08-16T05:25:35.801563Z","shell.execute_reply.started":"2023-08-16T05:25:32.543887Z","shell.execute_reply":"2023-08-16T05:25:35.800236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum().plot(kind ='bar')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df.drop(columns=['snow_depth_mm', 'avg_wind_dir_deg','avg_wind_speed_kmh',\n                  'peak_wind_gust_kmh', 'avg_sea_level_pres_hpa','sunshine_total_min'] ,inplace=True )","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:35.805978Z","iopub.execute_input":"2023-08-16T05:25:35.806398Z","iopub.status.idle":"2023-08-16T05:25:36.008288Z","shell.execute_reply.started":"2023-08-16T05:25:35.806365Z","shell.execute_reply":"2023-08-16T05:25:36.006993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill missing values in 'min_temp_c' column with the mean of non-missing values\ndf['min_temp_c'] = df['min_temp_c'].fillna(df['min_temp_c'].mean())\n\n# Fill missing values in 'max_temp_c' column with the median of non-missing values\ndf['max_temp_c'] = df['max_temp_c'].fillna(df['max_temp_c'].median())\n\n# Drop rows where the 'region' column has missing values\ndf = df.dropna(subset=['region','precipitation_mm'])\n\n# Check the count of missing values in each column\nmissing_value_counts = df.isna().sum()\n\n# Print the counts of missing values\nprint(\"Count of missing values in each column:\")\nprint(missing_value_counts)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:25:45.383333Z","iopub.execute_input":"2023-08-16T05:25:45.384281Z","iopub.status.idle":"2023-08-16T05:25:48.995364Z","shell.execute_reply.started":"2023-08-16T05:25:45.384232Z","shell.execute_reply":"2023-08-16T05:25:48.994058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:26:26.668867Z","iopub.execute_input":"2023-08-16T05:26:26.669314Z","iopub.status.idle":"2023-08-16T05:26:26.675256Z","shell.execute_reply.started":"2023-08-16T05:26:26.669281Z","shell.execute_reply":"2023-08-16T05:26:26.674057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:26:00.045357Z","iopub.execute_input":"2023-08-16T05:26:00.045854Z","iopub.status.idle":"2023-08-16T05:26:00.054437Z","shell.execute_reply.started":"2023-08-16T05:26:00.045815Z","shell.execute_reply":"2023-08-16T05:26:00.053491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode the 'season' column\ndf = pd.get_dummies(df, columns=['season'], drop_first=True)\n\n# Display the first few rows of the encoded dataset\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:26:08.078115Z","iopub.execute_input":"2023-08-16T05:26:08.078599Z","iopub.status.idle":"2023-08-16T05:26:08.961864Z","shell.execute_reply.started":"2023-08-16T05:26:08.078564Z","shell.execute_reply":"2023-08-16T05:26:08.960237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform the 'region' column using label encoding\ndf['region'] = le.fit_transform(df['region'])\n\n# Transform the 'capital' column using label encoding\ndf['capital'] = le.fit_transform(df['capital'])\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:26:22.876441Z","iopub.execute_input":"2023-08-16T05:26:22.876902Z","iopub.status.idle":"2023-08-16T05:26:24.147855Z","shell.execute_reply.started":"2023-08-16T05:26:22.876869Z","shell.execute_reply":"2023-08-16T05:26:24.14674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:26:30.977055Z","iopub.execute_input":"2023-08-16T05:26:30.977762Z","iopub.status.idle":"2023-08-16T05:26:30.984541Z","shell.execute_reply.started":"2023-08-16T05:26:30.977725Z","shell.execute_reply":"2023-08-16T05:26:30.982909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sampling the dataset¶\n","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=['avg_temp_c',\"date\"])\ny = df['avg_temp_c']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Display the shapes of the resulting datasets\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:26:36.001718Z","iopub.execute_input":"2023-08-16T05:26:36.003057Z","iopub.status.idle":"2023-08-16T05:26:37.149274Z","shell.execute_reply.started":"2023-08-16T05:26:36.003011Z","shell.execute_reply":"2023-08-16T05:26:37.147915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building and Analysis","metadata":{}},{"cell_type":"code","source":"models = {\n    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n}\nbest_model = None\nbest_r2 = 0\n\nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred= model.predict(X_test)\n\n    # Evaluate the model\n    r2 = r2_score(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    submit = pd.DataFrame()\n    submit['Actual avg_temp_c'] = y_test\n    submit['Predict_avg_temp_c'] = y_pred\n    submit = submit.reset_index()\n    r2 = r2_score(y_test, y_pred)\n    if r2 > best_r2:\n        best_r2 = r2\n        best_model = model.__class__.__name__\n\n    print(f'{model_name}:')\n    print(f'R2 Score: {r2:.2f}')\n    print(submit.head(5))\n\n    print('----------------------------------------')\nprint(f\"The best performing model is: {best_model} with accuracy: {best_r2:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:26:44.660939Z","iopub.execute_input":"2023-08-16T05:26:44.661375Z","iopub.status.idle":"2023-08-16T05:30:54.076778Z","shell.execute_reply.started":"2023-08-16T05:26:44.661343Z","shell.execute_reply":"2023-08-16T05:30:54.075445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# feature_importances","metadata":{}},{"cell_type":"code","source":"importances = model.feature_importances_\n\nfeature_names = X.columns\n\nfeature_importance_dict = dict(zip(feature_names, importances))\n\nsorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n\nfor feature, importance in sorted_feature_importance:\n    print(f\"{feature}: {importance:.2f}\")\n\nplt.figure(figsize=(12, 7))\nplt.barh(*zip(*sorted_feature_importance), alpha=0.9, color='teal')\nplt.title('Feature Importance', fontsize=15)\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T05:32:03.447062Z","iopub.execute_input":"2023-08-16T05:32:03.44753Z","iopub.status.idle":"2023-08-16T05:32:03.60943Z","shell.execute_reply.started":"2023-08-16T05:32:03.447498Z","shell.execute_reply":"2023-08-16T05:32:03.607671Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Movie Recommendation System\n\n## Introduction\nIn this notebook, we will build a movie recommendation system using the TMDB dataset. The goal is to provide personalized movie recommendations to users based on their preferences and movie attributes.","metadata":{}},{"cell_type":"code","source":"# Import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:19.33745Z","iopub.execute_input":"2023-06-28T17:52:19.337891Z","iopub.status.idle":"2023-06-28T17:52:20.304432Z","shell.execute_reply.started":"2023-06-28T17:52:19.337856Z","shell.execute_reply":"2023-06-28T17:52:20.302977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/updatedtmdb-movies-dataset/tmdb_movies_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.305927Z","iopub.execute_input":"2023-06-28T17:52:20.306332Z","iopub.status.idle":"2023-06-28T17:52:20.401394Z","shell.execute_reply.started":"2023-06-28T17:52:20.3063Z","shell.execute_reply":"2023-06-28T17:52:20.400134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Dataset Shape:\", df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.403156Z","iopub.execute_input":"2023-06-28T17:52:20.403654Z","iopub.status.idle":"2023-06-28T17:52:20.410241Z","shell.execute_reply.started":"2023-06-28T17:52:20.403607Z","shell.execute_reply":"2023-06-28T17:52:20.408798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.415153Z","iopub.execute_input":"2023-06-28T17:52:20.415795Z","iopub.status.idle":"2023-06-28T17:52:20.441155Z","shell.execute_reply.started":"2023-06-28T17:52:20.415742Z","shell.execute_reply":"2023-06-28T17:52:20.438975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop('Unnamed: 0', axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.443034Z","iopub.execute_input":"2023-06-28T17:52:20.443523Z","iopub.status.idle":"2023-06-28T17:52:20.462912Z","shell.execute_reply.started":"2023-06-28T17:52:20.443482Z","shell.execute_reply":"2023-06-28T17:52:20.461585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.465496Z","iopub.execute_input":"2023-06-28T17:52:20.465935Z","iopub.status.idle":"2023-06-28T17:52:20.50136Z","shell.execute_reply.started":"2023-06-28T17:52:20.465899Z","shell.execute_reply":"2023-06-28T17:52:20.49973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop rows with missing values\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.503229Z","iopub.execute_input":"2023-06-28T17:52:20.504361Z","iopub.status.idle":"2023-06-28T17:52:20.525586Z","shell.execute_reply.started":"2023-06-28T17:52:20.50431Z","shell.execute_reply":"2023-06-28T17:52:20.524156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"# Extract release year from the release_date column\ndf['release_year'] = pd.to_datetime(df['release_date']).dt.year\n\n# Plot the distribution of movies based on release year\nplt.figure(figsize=(10, 6))\nsns.histplot(df['release_year'], bins=30)\nplt.title('Distribution of Movies by Release Year')\nplt.xlabel('Release Year')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.527365Z","iopub.execute_input":"2023-06-28T17:52:20.527796Z","iopub.status.idle":"2023-06-28T17:52:20.983229Z","shell.execute_reply.started":"2023-06-28T17:52:20.527754Z","shell.execute_reply":"2023-06-28T17:52:20.981768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot of vote count vs. vote average\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='vote_count', y='vote_average')\nplt.title('Vote Count vs. Vote Average')\nplt.xlabel('Vote Count')\nplt.ylabel('Vote Average')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:20.984921Z","iopub.execute_input":"2023-06-28T17:52:20.98589Z","iopub.status.idle":"2023-06-28T17:52:21.383644Z","shell.execute_reply.started":"2023-06-28T17:52:20.985851Z","shell.execute_reply":"2023-06-28T17:52:21.38253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute correlation matrix\ncorr_matrix = df[['popularity', 'vote_average', 'vote_count']].corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:21.384999Z","iopub.execute_input":"2023-06-28T17:52:21.385661Z","iopub.status.idle":"2023-06-28T17:52:21.74153Z","shell.execute_reply.started":"2023-06-28T17:52:21.385618Z","shell.execute_reply":"2023-06-28T17:52:21.740183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine movie overviews into a single string\ntext = ' '.join(df['overview'].dropna().tolist())\n\n# Generate word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n# Plot the word cloud\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Movie Overviews')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:21.743836Z","iopub.execute_input":"2023-06-28T17:52:21.744439Z","iopub.status.idle":"2023-06-28T17:52:25.652044Z","shell.execute_reply.started":"2023-06-28T17:52:21.744397Z","shell.execute_reply":"2023-06-28T17:52:25.650394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort movies by vote average in descending order\ntop_rated_movies = df.sort_values('vote_average', ascending=False).head(10)\n\n# Plot a bar chart of top-rated movies\nplt.figure(figsize=(10, 6))\nsns.barplot(x='vote_average', y='title', data=top_rated_movies)\nplt.title('Top-Rated Movies')\nplt.xlabel('Vote Average')\nplt.ylabel('Movie Title')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:25.65403Z","iopub.execute_input":"2023-06-28T17:52:25.654463Z","iopub.status.idle":"2023-06-28T17:52:26.038189Z","shell.execute_reply.started":"2023-06-28T17:52:25.654424Z","shell.execute_reply":"2023-06-28T17:52:26.036689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the month from the release_date column\ndf['release_month'] = pd.to_datetime(df['release_date']).dt.month\n\n# Count the number of movies released each month\nrelease_counts = df['release_month'].value_counts().sort_index()\n\n# Plot the monthly release distribution\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=release_counts.index, y=release_counts.values)\nplt.title('Monthly Movie Release Distribution')\nplt.xlabel('Month')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:26.044548Z","iopub.execute_input":"2023-06-28T17:52:26.045036Z","iopub.status.idle":"2023-06-28T17:52:26.406714Z","shell.execute_reply.started":"2023-06-28T17:52:26.044996Z","shell.execute_reply":"2023-06-28T17:52:26.40514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of vote_average\nplt.figure(figsize=(8, 6))\nsns.histplot(df['vote_average'], bins=20)\nplt.title('Distribution of Vote Average')\nplt.xlabel('Vote Average')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:26.408752Z","iopub.execute_input":"2023-06-28T17:52:26.409567Z","iopub.status.idle":"2023-06-28T17:52:26.841073Z","shell.execute_reply.started":"2023-06-28T17:52:26.409517Z","shell.execute_reply":"2023-06-28T17:52:26.839707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Content-Based Filtering","metadata":{}},{"cell_type":"code","source":"# Select relevant columns for content-based filtering\nfeatures = ['id', 'title', 'overview', 'release_date', 'popularity']","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:26.84321Z","iopub.execute_input":"2023-06-28T17:52:26.84368Z","iopub.status.idle":"2023-06-28T17:52:26.850053Z","shell.execute_reply.started":"2023-06-28T17:52:26.843642Z","shell.execute_reply":"2023-06-28T17:52:26.848627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a copy of the dataset with the selected features\ndata = df[features].copy()\n\n# Fill missing values in the overview column with an empty string\ndata['overview'] = data['overview'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:26.851713Z","iopub.execute_input":"2023-06-28T17:52:26.852203Z","iopub.status.idle":"2023-06-28T17:52:26.871531Z","shell.execute_reply.started":"2023-06-28T17:52:26.852166Z","shell.execute_reply":"2023-06-28T17:52:26.869906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a TF-IDF vectorizer\ntfidf = TfidfVectorizer(stop_words='english')\n\n# Compute TF-IDF matrix\ntfidf_matrix = tfidf.fit_transform(data['overview'])\n\n# Compute cosine similarity matrix\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\n# Function to get recommendations based on movie title\ndef get_recommendations(title, cosine_similarities, data):\n    # Get the index of the movie with the given title\n    idx = data[data['title'] == title].index[0]\n    \n    # Get the similarity scores of all movies with the given movie\n    similarity_scores = list(enumerate(cosine_similarities[idx]))\n    \n    # Sort the movies based on similarity scores\n    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n    \n    # Get the top 5 most similar movies (excluding the given movie itself)\n    top_similar_movies = similarity_scores[1:6]\n    \n    # Get the indices of the top similar movies\n    movie_indices = [i[0] for i in top_similar_movies]\n    \n    # Return the titles of the top similar movies\n    return data['title'].iloc[movie_indices]","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:26.873497Z","iopub.execute_input":"2023-06-28T17:52:26.874088Z","iopub.status.idle":"2023-06-28T17:52:29.27503Z","shell.execute_reply.started":"2023-06-28T17:52:26.874038Z","shell.execute_reply":"2023-06-28T17:52:29.273822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get recommendations for a movie\nmovie_title = 'Dilwale Dulhania Le Jayenge'\nrecommendations = get_recommendations(movie_title, cosine_similarities, data)\nprint(f\"Recommended movies for '{movie_title}':\")\nprint(recommendations)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:29.276681Z","iopub.execute_input":"2023-06-28T17:52:29.27707Z","iopub.status.idle":"2023-06-28T17:52:29.303124Z","shell.execute_reply.started":"2023-06-28T17:52:29.277038Z","shell.execute_reply":"2023-06-28T17:52:29.301583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get recommendations for a movie\nmovie_title2 = 'My Hero Academia: Heroes Rising'\nrecommendations2 = get_recommendations(movie_title2, cosine_similarities, data)\nprint(f\"Recommended movies for '{movie_title2}':\")\nprint(recommendations2)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:29.304748Z","iopub.execute_input":"2023-06-28T17:52:29.305153Z","iopub.status.idle":"2023-06-28T17:52:29.328054Z","shell.execute_reply.started":"2023-06-28T17:52:29.30512Z","shell.execute_reply":"2023-06-28T17:52:29.326769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a copy of the dataset\noriginal_data = df.copy()\n\n# Select the features to be scaled\nfeatures_to_scale = ['popularity', 'vote_count', 'release_year', 'release_month']\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit the scaler on the selected features of the original dataset\nscaler.fit(original_data[features_to_scale])\n\n# Scale the selected features of the original dataset\noriginal_data_scaled = scaler.transform(original_data[features_to_scale])\n\n# Create a new DataFrame with the scaled features\noriginal_data_scaled_df = pd.DataFrame(original_data_scaled, columns=features_to_scale)\n\n# Concatenate the scaled features with the remaining columns of the original dataset\noriginal_data_scaled_df = pd.concat([original_data_scaled_df, original_data.drop(columns=features_to_scale)], axis=1)\n\ndf = original_data_scaled_df\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:29.329488Z","iopub.execute_input":"2023-06-28T17:52:29.329864Z","iopub.status.idle":"2023-06-28T17:52:29.369095Z","shell.execute_reply.started":"2023-06-28T17:52:29.329831Z","shell.execute_reply":"2023-06-28T17:52:29.367828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and Evaluating the Models","metadata":{}},{"cell_type":"code","source":"# Select numeric features for training\nnumeric_features = ['popularity', 'vote_count', 'release_year', 'release_month']\n\n# Split the dataset into training and validation sets\ntrain_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Prepare the features and target variables\nX_train = train_data[numeric_features].astype(float)\ny_train = train_data['vote_average'].astype(float)\n\n# Check and handle missing values in y_train\nmissing_y_train = np.isnan(y_train)\nX_train = X_train[~missing_y_train]\ny_train = y_train[~missing_y_train]\n\nX_val = val_data[numeric_features].astype(float)\ny_val = val_data['vote_average'].astype(float)\n\n# Check and handle missing values in y_val\nmissing_y_val = np.isnan(y_val)\nX_val = X_val[~missing_y_val]\ny_val = y_val[~missing_y_val]\n\n# Create a pipeline with imputation and regression\nlinear_pipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    LinearRegression()\n)\n\nrf_pipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    RandomForestRegressor()\n)\n\ngb_pipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    GradientBoostingRegressor()\n)\n\nnn_pipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    MLPRegressor()\n)\n\n# Fit the models and make predictions\nlinear_pipeline.fit(X_train, y_train)\nlinear_predictions = linear_pipeline.predict(X_val)\nlinear_rmse = mean_squared_error(y_val, linear_predictions, squared=False)\nprint(\"Linear Regression MSE:\", linear_rmse)\n\nrf_pipeline.fit(X_train, y_train)\nrf_predictions = rf_pipeline.predict(X_val)\nrf_rmse = mean_squared_error(y_val, rf_predictions, squared=False)\nprint(\"Random Forest Regression MSE:\", rf_rmse)\n\ngb_pipeline.fit(X_train, y_train)\ngb_predictions = gb_pipeline.predict(X_val)\ngb_rmse = mean_squared_error(y_val, gb_predictions, squared=False)\nprint(\"Gradient Boosting Regression MSE:\", gb_rmse)\n\nnn_pipeline.fit(X_train, y_train)\nnn_predictions = nn_pipeline.predict(X_val)\nnn_rmse = mean_squared_error(y_val, nn_predictions, squared=False)\nprint(\"Neural Networks MSE:\", nn_rmse)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:29.371132Z","iopub.execute_input":"2023-06-28T17:52:29.371791Z","iopub.status.idle":"2023-06-28T17:52:41.140724Z","shell.execute_reply.started":"2023-06-28T17:52:29.371753Z","shell.execute_reply":"2023-06-28T17:52:41.139427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We find that Gradient Boosting Regression has the lowest mean squared error so we will use it for prediction.","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter Tuning","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:41:28.772915Z","iopub.execute_input":"2023-06-28T17:41:28.773412Z","iopub.status.idle":"2023-06-28T17:41:28.777845Z","shell.execute_reply.started":"2023-06-28T17:41:28.773375Z","shell.execute_reply":"2023-06-28T17:41:28.776883Z"}}},{"cell_type":"code","source":"# Determine the data\nX = df[numeric_features].dropna()\ny = df['vote_average'].dropna()\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of SimpleImputer with strategy='mean'\nimputer = SimpleImputer(strategy='mean')\n\n# Fit the imputer on X_train\nimputer.fit(X_train)\n\n# Transform X_train and X_test by replacing the missing values with the mean\nX_train_imputed = imputer.transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.1, 0.05, 0.01]\n}\n\n# Create a GradientBoostingRegressor instance\ngb_model = GradientBoostingRegressor()\n\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_root_mean_squared_error')\n\n# Fit the GridSearchCV object to the data\ngrid_search.fit(X_train_imputed, y_train)\n\n# Get the best hyperparameters and the best RMSE score\nbest_params = grid_search.best_params_\nbest_score = np.sqrt(-grid_search.best_score_)\n\n# Train a new model with the best hyperparameters\nbest_model = GradientBoostingRegressor(**best_params)\n\nbest_model.fit(X_train_imputed, y_train)\n\n# Predict on the test set\ny_pred = best_model.predict(X_test_imputed)\n\n# Calculate RMSE on the test set\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best RMSE:\", best_score)\nprint(\"Test RMSE:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:52:41.142488Z","iopub.execute_input":"2023-06-28T17:52:41.143385Z","iopub.status.idle":"2023-06-28T17:55:23.870596Z","shell.execute_reply.started":"2023-06-28T17:52:41.143333Z","shell.execute_reply":"2023-06-28T17:55:23.8688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Movie Recommendations","metadata":{}},{"cell_type":"code","source":"# Generate movie recommendations using the trained model\ntop_k = 5  # Number of recommendations to generate\n\n# Assuming you have a new_data DataFrame containing new movie data\nnew_data = pd.DataFrame({'popularity': [68.94], 'vote_count': [15806], 'release_year': [2001], 'release_month': [9]})\n\n# Perform the same preprocessing steps as the original dataset\nnew_data_scaled = scaler.transform(new_data)  # Scale the new data using the fitted scaler\nnew_predictions = best_model.predict(new_data_scaled)  # Predict the ratings for the new data\n\n# Get the indices of the top-k movies based on the predicted ratings\ntop_indices = np.argsort(new_predictions)[::-1][:top_k]\n\n# Get the details of the top-k recommended movies\ntop_movies = df.iloc[top_indices]['title']\n\n# Print the top-k recommended movies\nprint(\"Top\", top_k, \"Recommended Movies:\")\nfor movie in top_movies:\n    print(movie)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:55:23.872786Z","iopub.execute_input":"2023-06-28T17:55:23.873341Z","iopub.status.idle":"2023-06-28T17:55:23.887726Z","shell.execute_reply.started":"2023-06-28T17:55:23.873305Z","shell.execute_reply":"2023-06-28T17:55:23.885955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\n🎥🔎 In this project, we explored a movie dataset consisting of columns such as 'id', 'title', 'overview', 'release_date', 'popularity', 'vote_average', and 'vote_count'. Our goal was to build a recommendation system based on the movie attributes and user preferences. We followed several steps to achieve this.\n\n📊🔍 First, we performed exploratory data analysis (EDA) to gain insights into the dataset. We visualized the distribution of movie attributes, analyzed their relationships, and identified potential features for our recommendation model.\n\n🔎🎥 Next, We implemented content-based filtering using TF-IDF vectorization and cosine similarity to provide movie recommendations.\n\n🎯✨ After that, we selected the 'vote_average' column as our target variable, which represents the movie rating. We prepared the features and target variables by splitting the dataset into training and validation sets.\n\n⚙️🧪 We trained and evaluated four different regression models: Linear Regression, Random Forest Regression, Gradient Boosting Regression, and Neural Networks. The models were evaluated using the root mean squared error (RMSE) metric on the validation set. Among the models, the Gradient Boosting Regression model achieved the lowest RMSE value of **0.6989676144474153**, indicating better performance in predicting movie ratings.\n\n🌟📈 After identifying the best model, we proceeded to generate movie recommendations. We provided feature values for new movies and used the trained model to predict their ratings. The top-k recommended movies were selected based on the predicted ratings.\n\n\n## Conclusion\n🌍🎞️ In conclusion, we successfully built a movie recommendation system based on regression models. By analyzing the dataset, training different models, and evaluating their performance, we identified the best-performing model for predicting movie ratings. The Gradient Boosting Regression model demonstrated the lowest RMSE on the validation set.\n\n🔍💡 The generated movie recommendations can provide users with a curated list of movies based on their preferences. However, there are still opportunities for further enhancements, including feature engineering, collaborative filtering, fine-tuning models, and exploring ensemble methods.\n\n🚀🌟 Overall, this project serves as a foundation for developing a more advanced and personalized movie recommendation system, allowing users to discover movies tailored to their interests and preferences.","metadata":{}}]}
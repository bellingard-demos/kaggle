{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Logistic Regression with Scikit-Learn: Practical ML #2\n\nIn this notebook, we will cover logistic regression using Scikit-Learn. The dataset being used is [Diabetes Dataset](https://www.kaggle.com/kandij/diabetes-dataset), where we will predict a person will have diabetes based on their blood pressure, BMI, Glucose using **Logistic Regression**.\n\n\n**Link to Part 1:** [Regression with Scikit-Learn: Practical ML #1](https://www.kaggle.com/aadhavvignesh/regression-with-scikit-learn-practical-ml-1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Inspecting Data\n\nLet us load the dataset and save it in a DataFrame:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/diabetes-dataset/diabetes2.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see our dataframe contains the count of values and their datatypes using `.info()`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunately, we do not have any missing values in our dataset. We can now proceed with visualizations and making predictions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Data\n\nLet us visualize the dataset using various plots.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of Ages\n\nLet us plot the **distribution of ages** of the patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.Age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that most patients have the age of 20 - 30 years. \n\nHence, we can say that adults having age in the range of 20-30 years are more prone to diabetes due to lack of exercise, unhealthy diet, etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Plot relation of blood sugar levels and age\n\nThe distribution clearly shows that most patients lie in the range of 20-30 years with blood sugar levels being approximately equal to 100 mg/dL.\n\n**Note:** Normal sugar levels lie in the range of 60 - 90 mg/dL. For more info, check out the [link.](https://www.webmd.com/diabetes/how-sugar-affects-diabetes)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(df.Age, df['Glucose'], kind = 'kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pair plot\n\nLet us plot pairwise relationships in a dataset using `seaborn`'s `pairplot` function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue = 'Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create a helper function to evaluate predictions and calculate accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef evaluate(true, pred):\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    print(\"Confusion Matrix:\\n\", confusion_matrix(true, pred))\n    cm = pd.crosstab(true, pred)\n    sns.heatmap(cm, annot=True)\n    print(\"Accuracy Score:\", accuracy_score(true, pred))\n    print(\"Precision:\", precision)\n    print(\"Recall:\", recall)\n    print(\"F1 Score:\", f1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us prepare the data by dropping the target variable, `Outcome`, and setting it as the target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Outcome'], axis = 1)\ny = df['Outcome']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now split our data into training and test data sets. We use the training set in order to fit the model and the test set to evaluate our model's predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n\nLogistic regression is a **classification algorithm** used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n\nIt is used when the target variable is binary categorical (0 or 1).\n\n![Linear vs Logisitc Regression](https://www.machinelearningplus.com/wp-content/uploads/2017/09/linear_vs_logistic_regression.jpg)\n\n### Sigmoid Function:\n\nThe sigmoid function is represented by:\n\n$\\large S(z) = \\frac{1} {1 + e^{-z}}$\n\nwhere, \n$\\large S(z)$ = output between 0 and 1 (probability estimate),\n\n$\\large z$ = input to the function (your algorithmâ€™s prediction e.g. mx + b),\n\n$\\large e$ = base of natural log","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We'll create a logistic regression model using scikit-learn's `LogisticRegression`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = 'liblinear')\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Measuring Performance\n\n### 1. Confusion Matrix\n\nThis is what a confusion matrix looks like:\n![Confusion Matrix](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n\nNow, let us understand what TP, TN, FP, FN denote in this matrix:\n\n\n- **True Positives (TP):** These are cases in which we predicted yes (they have the disease), and they do have the disease.\n- **True Negatives (TN):** We predicted no, and they don't have the disease.\n- **False Positives (FP):** We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n- **False Negatives (FN):** We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n\n\n### 2. Precision\n\nPrecision is defined as the number of true positives (TP) over the number of true positives plus the number of false positives (FP).\n\n![Precision](https://miro.medium.com/max/948/1*HGd3_eAJ3-PlDQvn-xDRdg.png)\n\n### 3. Recall\n\nRecall is defined as the number of true positives (TP) over the number of true positives plus the number of false negatives (FN).\n\n![Recall](https://miro.medium.com/max/836/1*dXkDleGhA-jjZmZ1BlYKXg.png)\n\n### 4. F1-Score\n\nF1-score is the harmonic mean of precision and recall.\n\n![F1_Score](https://miro.medium.com/max/564/1*T6kVUKxG_Z4V5Fm1UXhEIw.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## PR Curve\n\nA PR curve is simply a graph with Precision values on the y-axis and Recall values on the x-axis. \n\nIt is desired that the algorithm should have both high precision, and high recall. However, most machine learning algorithms often involve a trade-off between the two. **A good PR curve has greater AUC (area under curve). **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions/recalls tradeoff\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC Curve\n\nROC curve plots the true positive rate (another name for recall) against the false positive rate. The false positive rate (FPR) is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, which is the ratio of negative instances that are correctly classified as negative.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.axis([0, 1, 0, 1])\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning\n\nHyperparamter tuning is choosing the set of optimal hyperparameters for our models. The tuning works by changing several parameters like loss function, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\npenalty = ['l1', 'l2']\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\nsolver = ['liblinear', 'saga']\n\nparam_grid = dict(penalty=penalty, C=C, class_weight=class_weight, solver=solver)\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc',\n                    verbose=1, n_jobs=-1, cv=10, iid=True)\ngrid_result = grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = grid_result.predict(X_test)\n\nevaluate(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary:\n\nIn this notebook, you got to learn about:\n\n- Inspecting Data\n- Visualizing Data\n- Splitting Data into Training and Test Sets\n- Logistic Regression\n- Measuring Performance\n- PR, ROC Curve\n- Hyperparameter Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Link to Part 1: [Regression with Scikit-Learn: Practical ML #1](https://www.kaggle.com/aadhavvignesh/regression-with-scikit-learn-practical-ml-1)\n\n## Part 3 coming soon!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
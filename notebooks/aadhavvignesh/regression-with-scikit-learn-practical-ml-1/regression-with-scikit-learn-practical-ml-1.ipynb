{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regression with Scikit-Learn\n\nThe notebook covers the following algorithms:\n- Linear Regression\n- Robust Regression\n- Ridge Regression\n- Lasso Regression\n- ElasticNet\n\nThe dataset being used is [Housing Price in Beijing](https://www.kaggle.com/ruiqurm/lianjia), where in the house prices are continuous in nature, hence, we are going to predict house prices using the listed regression algorithms.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Inspecting Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us load the data and save it in a DataFrame:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/lianjia/new.csv\", encoding='iso-8859-1', low_memory = False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`.info()` helps us to see how many values are existing in each column and their data type. Here, we can see many columns have missing values which we need to fill with some sensible values. Handling missing data is important as many machine learning algorithms do not support data with missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`.describe()` is used to view some basic statistical details like percentile, mean, std, etc. of a DataFrame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`.corr()` is used to find the pairwise correlation of all columns in the dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we select the most significant features and correlated features from the correlation matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['totalPrice', 'square', 'renovationCondition', 'communityAverage']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing with Missing Data\n\nOops! It seems like we're about to perform training on a dataframe consisting of NaN (Not a Number) values. Here are some strategies to handle NaN values:\n\n- **Remove rows with NaN values**: This is generally not preferred as substantial data can be lost, and could also lead to improper distribution of values.\n- **Filling NaN values with scalar values**: This is helpful to some extent, but it can also lead to improper distribution of data.\n- **Filling NaN values with mean**: Filling with mean helps to maintain the distribution of values in the dataframe and hence is more preferred than other methods.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us fill NaN values using mean.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna(df.mean())\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us set `totalPrice` as the target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('totalPrice', axis = 1)\ny = df['totalPrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting Data into Training and Test Sets\nThe data we use is usually split into training data and test data using `train_test_split`. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our modelâ€™s prediction on this subset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics for testing model's performance\n\nThe MSE, MAE, RMSE, and R-Squared are mainly used metrics to evaluate the prediction error rates and model performance in regression analysis.\n\n- **MAE (Mean absolute error)** represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n- **MSE (Mean Squared Error)** represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n- **RMSE (Root Mean Squared Error)** is the error rate by the square root of MSE.\n- **R-squared (Coefficient of determination)** represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we created helper functions to append the metrics to a DataFrame containing the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nresults_df = pd.DataFrame()\ncolumns = [\"Model\", \"Cross Val Score\", \"MAE\", \"MSE\", \"RMSE\", \"R2\"]\n\ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square\n\ndef append_results(model_name, model, results_df, y_test, pred):\n    results_append_df = pd.DataFrame(data=[[model_name, *evaluate(y_test, pred) , cross_val_score(model, X, y, cv=10).mean()]], columns=columns)\n    results_df = results_df.append(results_append_df, ignore_index = True)\n    return results_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression\n\nLinear regression is an algorithm which tries to learn the correlation between a dependent variable and one or more independent features.\n\n![Linear Regression](https://cdn-images-1.medium.com/max/1200/1*LEmBCYAttxS6uI6rEyPLMQ.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)\n\npred = lin_reg.predict(X_test)\n\nresults_df = append_results(\"Linear Regression\",  LinearRegression(), results_df, y_test, pred)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Robust Regression\n\nRobust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. A common situation in which robust estimation is used occurs when the data contain outliers. One of the most popular approaches to outlier detection is **RANSAC or Random Sample Consesus**.\n\nRandom sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RANSACRegressor\n\nmodel = RANSACRegressor()\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\nresults_df = append_results(\"Robust Regression\",  RANSACRegressor(), results_df, y_test, pred)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge Regression\n\nRidge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:\n\n$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$\n\nThe complexity parameter $\\alpha \\geq 0$ controls the amount of shrinkage: the larger the value of , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel = Ridge()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nresults_df = append_results(\"Ridge Regression\",  Ridge(), results_df, y_test, pred)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso Regression\n\nLasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nmodel = Lasso()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nresults_df = append_results(\"Lasso Regression\",  Lasso(), results_df, y_test, pred)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ElasticNet\n\nElasticNet combines L1 norms (LASSO) and L2 norms (ridge regression) into a penalized model for generalized linear regression. This gives it sparsity (L1) and robustness (L2) properties.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nresults_df = append_results(\"ElasticNet Regression\",  ElasticNet(), results_df, y_test, pred)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary:\n\nIn this notebook, you got to learn about:\n\n- Inspecting Data\n- Dealing with Missing Values\n- Splitting Data into Training and Test Sets\n- Common Linear Regression Algorithms (Linear, Ridge, Lasso, etc.)\n- Comparison of all algorithms","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## If you found this notebook useful, show your appreciation with an upvote!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
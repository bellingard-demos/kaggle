{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:12:20.830358Z","iopub.execute_input":"2022-08-23T07:12:20.830717Z","iopub.status.idle":"2022-08-23T07:12:22.688809Z","shell.execute_reply.started":"2022-08-23T07:12:20.830688Z","shell.execute_reply":"2022-08-23T07:12:22.687827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-18T03:32:30.545375Z","iopub.execute_input":"2022-08-18T03:32:30.545787Z","iopub.status.idle":"2022-08-18T03:32:30.601905Z","shell.execute_reply.started":"2022-08-18T03:32:30.545744Z","shell.execute_reply":"2022-08-18T03:32:30.60051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nsns.barplot(x = list(df.columns[2:]), y= list(df.iloc[:,2:].sum()))\nplt.title('Total comments in each category')\nplt.xlabel('Number of comments')\nplt.ylabel('Types of comments')","metadata":{"execution":{"iopub.status.busy":"2022-08-18T03:33:08.550013Z","iopub.execute_input":"2022-08-18T03:33:08.550542Z","iopub.status.idle":"2022-08-18T03:33:09.535845Z","shell.execute_reply.started":"2022-08-18T03:33:08.550495Z","shell.execute_reply":"2022-08-18T03:33:09.534823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor i in range(len(df)):\n    n = df.iloc[i,2:].sum()\n    if n > 1:\n        count = count + 1\n    \nprint('Total multi-labeled comments:',count)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T03:33:33.039511Z","iopub.execute_input":"2022-08-18T03:33:33.039978Z","iopub.status.idle":"2022-08-18T03:34:29.367994Z","shell.execute_reply.started":"2022-08-18T03:33:33.039938Z","shell.execute_reply":"2022-08-18T03:34:29.366774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_count = df.iloc[:,2:].sum(axis=1).value_counts()\n\nplt.figure(figsize=(10,6))\nsns.barplot(x = multi_count.index[1:], y = multi_count[1:])\nplt.title('Multi-Labelled comments')\nplt.ylabel('No. of multi-label comments')\nplt.xlabel('number of labels')","metadata":{"execution":{"iopub.status.busy":"2022-08-18T03:34:47.109499Z","iopub.execute_input":"2022-08-18T03:34:47.10993Z","iopub.status.idle":"2022-08-18T03:34:47.30149Z","shell.execute_reply.started":"2022-08-18T03:34:47.109897Z","shell.execute_reply":"2022-08-18T03:34:47.300403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\nplt.figure(figsize=(10,8))\ntext = df.comment_text.values\n\ncloud = WordCloud(stopwords=STOPWORDS, \n                  background_color='black', \n                  width=2500, \n                  height=1800).generate(''.join(text))\nplt.imshow(cloud)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T03:43:41.065463Z","iopub.execute_input":"2022-08-18T03:43:41.065863Z","iopub.status.idle":"2022-08-18T03:44:33.352585Z","shell.execute_reply.started":"2022-08-18T03:43:41.06583Z","shell.execute_reply":"2022-08-18T03:44:33.350977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the text data:","metadata":{}},{"cell_type":"code","source":"import re\nimport string\nfrom nltk.corpus import stopwords\n\ndef clean_text(x):\n    '''\n    The first two lines replace characters to spaces.\n    Than we remove punctuations from text using string\n    and finally all the stopwords in the text.\n    '''\n    text = re.sub(r\"\\'\", r'', x) \n    text = re.sub(r'\\n', r' ', text)\n    nopunc = [i for i in text if i not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    stop_word = [j for j in nopunc.split() if j.lower() not in stopwords.words('english')]\n    return ' '.join(stop_word)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:12:28.426091Z","iopub.execute_input":"2022-08-23T07:12:28.426774Z","iopub.status.idle":"2022-08-23T07:12:28.844391Z","shell.execute_reply.started":"2022-08-23T07:12:28.426742Z","shell.execute_reply":"2022-08-23T07:12:28.843556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nbefore = datetime.datetime.now()\ndf['clean_comments'] = df['comment_text'].apply(clean_text)\nafter = datetime.datetime.now()\nprint('Time consumed by text cleaning operation: ', after - before)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:12:32.924778Z","iopub.execute_input":"2022-08-23T07:12:32.925553Z","iopub.status.idle":"2022-08-23T07:30:47.691759Z","shell.execute_reply.started":"2022-08-23T07:12:32.92552Z","shell.execute_reply":"2022-08-23T07:30:47.690489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\n\ndef stemm(sentence):\n    '''\n    Function first splits the sentence to words\n    than stem each word to base form and finally\n    concat it to string \\'stem_word\\' '''\n    stem_word = ''\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stem_word += stem\n        stem_word += ' '\n    return stem_word","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:30:54.952779Z","iopub.execute_input":"2022-08-23T07:30:54.953135Z","iopub.status.idle":"2022-08-23T07:30:54.958499Z","shell.execute_reply.started":"2022-08-23T07:30:54.953101Z","shell.execute_reply":"2022-08-23T07:30:54.957689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nbefore = datetime.datetime.now()\n\ndf['clean_comments'] = df['clean_comments'].apply(stemm)\n\nafter = datetime.datetime.now()\nprint('Time consumed by stemming operation: ', after - before)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:56:23.644129Z","iopub.execute_input":"2022-08-23T07:56:23.644516Z","iopub.status.idle":"2022-08-23T07:56:26.634619Z","shell.execute_reply.started":"2022-08-23T07:56:23.644485Z","shell.execute_reply":"2022-08-23T07:56:26.633435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text before and after applying stemming and cleaning text:","metadata":{}},{"cell_type":"code","source":"print(df['comment_text'][0])\nprint('-'*40)\nprint(df['clean_comments'][0])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:58:32.383941Z","iopub.execute_input":"2022-08-23T07:58:32.384308Z","iopub.status.idle":"2022-08-23T07:58:32.390863Z","shell.execute_reply.started":"2022-08-23T07:58:32.384279Z","shell.execute_reply":"2022-08-23T07:58:32.389363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF of a word gives a product of how frequent this word is in the document multiplied by how unique the word is w.r.t. the entire corpus of documents.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectrizer = TfidfVectorizer(analyzer='word',ngram_range=(1,3))\nX = vectrizer.fit_transform(df['clean_comments'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    df.drop(['id','comment_text','clean_comments'],axis=1),\n                                                    test_size=0.33, \n                                                    random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:36:52.923688Z","iopub.execute_input":"2022-08-23T07:36:52.924057Z","iopub.status.idle":"2022-08-23T07:37:49.24674Z","shell.execute_reply.started":"2022-08-23T07:36:52.924027Z","shell.execute_reply":"2022-08-23T07:37:49.245757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In an “one-to-rest” strategy, we build multiple independent classifiers and, choose the class for which the confidence is maximized.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import hamming_loss, accuracy_score\n\nmodel = OneVsRestClassifier(estimator=LogisticRegression())\nmodel.fit(X_train, y_train)\n\nprediction = model.predict(X_test)\nprint('Accuracy Score: ', accuracy_score(y_test, prediction))\nprint('hamming loss : ', hamming_loss(y_test, prediction))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:38:00.845442Z","iopub.execute_input":"2022-08-23T07:38:00.845853Z","iopub.status.idle":"2022-08-23T07:44:24.074791Z","shell.execute_reply.started":"2022-08-23T07:38:00.845825Z","shell.execute_reply":"2022-08-23T07:44:24.07385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
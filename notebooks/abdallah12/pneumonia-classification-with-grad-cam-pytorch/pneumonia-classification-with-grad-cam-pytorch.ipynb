{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1) Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport torch\nimport warnings\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport albumentations as A\nimport seaborn as sns\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nimport torch.optim.lr_scheduler as scheduler\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.pytorch import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:01.234372Z","iopub.execute_input":"2023-12-03T03:17:01.234968Z","iopub.status.idle":"2023-12-03T03:17:06.809311Z","shell.execute_reply.started":"2023-12-03T03:17:01.234938Z","shell.execute_reply":"2023-12-03T03:17:06.808339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:06.810963Z","iopub.execute_input":"2023-12-03T03:17:06.81136Z","iopub.status.idle":"2023-12-03T03:17:06.815609Z","shell.execute_reply.started":"2023-12-03T03:17:06.811335Z","shell.execute_reply":"2023-12-03T03:17:06.814732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nsns.set_palette(\"RdBu\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:06.816736Z","iopub.execute_input":"2023-12-03T03:17:06.817086Z","iopub.status.idle":"2023-12-03T03:17:06.826444Z","shell.execute_reply.started":"2023-12-03T03:17:06.817054Z","shell.execute_reply":"2023-12-03T03:17:06.825553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2) Basic Config and Functions","metadata":{}},{"cell_type":"code","source":"def generate_df(directory: str, categories: list[str]) -> pd.DataFrame:\n    data = []\n    \n    for category in categories:\n        for path in os.listdir(os.path.join(directory, category)):        \n            data.append([path, category])\n        \n    return pd.DataFrame(data, columns=['path', 'label'])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:06.82837Z","iopub.execute_input":"2023-12-03T03:17:06.828647Z","iopub.status.idle":"2023-12-03T03:17:06.83561Z","shell.execute_reply.started":"2023-12-03T03:17:06.828613Z","shell.execute_reply":"2023-12-03T03:17:06.834717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_img(path: str, size: tuple[int, int]):\n    img = cv2.imread(img_path)\n    \n    return cv2.resize(img, size)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:54.053513Z","iopub.execute_input":"2023-12-03T03:17:54.053951Z","iopub.status.idle":"2023-12-03T03:17:54.059204Z","shell.execute_reply.started":"2023-12-03T03:17:54.053905Z","shell.execute_reply":"2023-12-03T03:17:54.0583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 30\nLEARNING_RATE = 3.5e-4\nBATCH_SIZE = 32\nLR_DECAY_EPOCH = [15, 30]\nLR_DECAY = 0.1\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nIMAGE_SIZE = (224, 224)\n\nCATEGORIES = {\n    'PNEUMONIA': 1,\n    'NORMAL': 0\n}\n\nBASE_PATH = '/kaggle/input/chest-xray-pneumonia/chest_xray'\n\nTRAIN_PATH = f'{BASE_PATH}/train'\nTEST_PATH = f'{BASE_PATH}/test'\nVALID_PATH = f'{BASE_PATH}/valid'\n\ntrain_df = generate_df(TRAIN_PATH, CATEGORIES.keys())\n\nbasic_transforms = A.Compose([\n    A.Resize(\n        height=IMAGE_SIZE[0],\n        width=IMAGE_SIZE[1]\n    ),\n    # A.CLAHE(),\n    A.Normalize(\n        mean=(0.5,),\n        std=(0.5,)\n    ),\n    ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:55.355136Z","iopub.execute_input":"2023-12-03T03:17:55.355947Z","iopub.status.idle":"2023-12-03T03:17:55.375179Z","shell.execute_reply.started":"2023-12-03T03:17:55.355905Z","shell.execute_reply":"2023-12-03T03:17:55.374324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4) Load data","metadata":{}},{"cell_type":"code","source":"class ChestXRay(Dataset):\n    def __init__(self, root, categories, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        self.images = []\n        \n        for category in categories:\n            for img in os.listdir(os.path.join(self.root, category)):\n                self.images.append({\n                    'path': os.path.join(self.root, category, img),\n                    'label': category\n                })\n                \n    def __getitem__(self, item):\n        item = self.images[item]\n        \n        path = item['path']\n        label = item['label']\n        \n        img = np.array(Image.open(path).convert('L'))\n        \n        img = np.expand_dims(img, axis=-1)\n        img = img.repeat(3, axis=-1)\n        \n        if self.transforms is not None:\n            img = self.transforms(image=img)['image']\n            \n        return img.float(), torch.tensor(CATEGORIES[label], dtype=torch.int64)\n            \n    def __len__(self):\n        return len(self.images)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:55.775844Z","iopub.execute_input":"2023-12-03T03:17:55.776181Z","iopub.status.idle":"2023-12-03T03:17:55.785397Z","shell.execute_reply.started":"2023-12-03T03:17:55.776153Z","shell.execute_reply":"2023-12-03T03:17:55.78425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ChestXRay(\n    root=TRAIN_PATH,\n    categories=CATEGORIES,\n    transforms=basic_transforms\n)\n\ntest_dataset = ChestXRay(\n    root=TEST_PATH,\n    categories=CATEGORIES,\n    transforms=basic_transforms\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:56.098098Z","iopub.execute_input":"2023-12-03T03:17:56.098458Z","iopub.status.idle":"2023-12-03T03:17:56.129519Z","shell.execute_reply.started":"2023-12-03T03:17:56.098429Z","shell.execute_reply":"2023-12-03T03:17:56.128771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Train*: Found **5216** images belonging to 2 classes.\n\n*Test*: Found **624** images belonging to 2 classes.","metadata":{}},{"cell_type":"markdown","source":"## 5) EDA","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:57.391818Z","iopub.execute_input":"2023-12-03T03:17:57.392161Z","iopub.status.idle":"2023-12-03T03:17:57.40137Z","shell.execute_reply.started":"2023-12-03T03:17:57.392136Z","shell.execute_reply":"2023-12-03T03:17:57.400445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_imgs = 16\n\nnormal_path = f'{TRAIN_PATH}/NORMAL'\npneumonia_path = f'{TRAIN_PATH}/PNEUMONIA'\n\nsample_normal_imgs = os.listdir(normal_path)[:count_imgs]\nsample_pneumonia_imgs = os.listdir(pneumonia_path)[:count_imgs]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:57.991737Z","iopub.execute_input":"2023-12-03T03:17:57.992089Z","iopub.status.idle":"2023-12-03T03:17:57.999496Z","shell.execute_reply.started":"2023-12-03T03:17:57.992061Z","shell.execute_reply":"2023-12-03T03:17:57.99862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\n\nnormal_imgs_path = [normal_path + '/' + i for i in sample_normal_imgs]\npneumonia_imgs_path = [pneumonia_path + '/' + j for j in sample_pneumonia_imgs]\n\nall_imgs = normal_imgs_path + pneumonia_imgs_path\n\nrandom.shuffle(all_imgs)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:58.744651Z","iopub.execute_input":"2023-12-03T03:17:58.745006Z","iopub.status.idle":"2023-12-03T03:17:58.750766Z","shell.execute_reply.started":"2023-12-03T03:17:58.744977Z","shell.execute_reply":"2023-12-03T03:17:58.74978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(28, 10))\n\nfor img_path in all_imgs:\n    plt.subplot(4, 8, counter + 1)\n    \n    img = read_img(img_path, IMAGE_SIZE)\n    \n    label = img_path[len(TRAIN_PATH) + 1: img_path.rfind('/')]\n    \n    plt.imshow(img)\n    plt.title(label)\n    plt.axis('off')\n    \n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:59.274626Z","iopub.execute_input":"2023-12-03T03:17:59.274992Z","iopub.status.idle":"2023-12-03T03:18:05.228671Z","shell.execute_reply.started":"2023-12-03T03:17:59.274962Z","shell.execute_reply":"2023-12-03T03:18:05.227737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\n\nplt.figure(figsize=(28, 20))\n\nfor img_path in all_imgs[:8]:\n    plt.subplot(4, 2, counter + 1)\n    \n    img = read_img(img_path, IMAGE_SIZE)    \n    \n    plt.hist(img.ravel()) \n    plt.title(counter + 1)\n    plt.axis('off')\n    \n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:05.230287Z","iopub.execute_input":"2023-12-03T03:18:05.230589Z","iopub.status.idle":"2023-12-03T03:18:07.376174Z","shell.execute_reply.started":"2023-12-03T03:18:05.230564Z","shell.execute_reply":"2023-12-03T03:18:07.375148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,6))\n\nfig = sns.countplot(train_df, x='label')\n\nplt.xticks(rotation=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:07.377241Z","iopub.execute_input":"2023-12-03T03:18:07.377505Z","iopub.status.idle":"2023-12-03T03:18:07.569666Z","shell.execute_reply.started":"2023-12-03T03:18:07.377482Z","shell.execute_reply":"2023-12-03T03:18:07.568823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we see?\n\n1. The data is imbalanced\n2. Not enough data to train CNN","metadata":{}},{"cell_type":"markdown","source":"### 5.1) Applying CLAHE Filter","metadata":{}},{"cell_type":"code","source":"sample1 = train_df.iloc[0]\n\nsample1_label = sample1['label']\nsample1_path = sample1['path']\n\nsample2 = train_df.iloc[0]\n\nsample2_label = sample2['label']\nsample2_path = sample2['path']\n\nsample1_img = read_img(f'{TRAIN_PATH}/{sample1_label}/{sample1_path}', IMAGE_SIZE)\nsample2_img = read_img(f'{TRAIN_PATH}/{sample2_label}/{sample2_path}', IMAGE_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:07.571859Z","iopub.execute_input":"2023-12-03T03:18:07.57212Z","iopub.status.idle":"2023-12-03T03:18:07.628043Z","shell.execute_reply.started":"2023-12-03T03:18:07.572097Z","shell.execute_reply":"2023-12-03T03:18:07.627333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(28, 10))\n\nplt.subplot(1, 3, 1)\n\nplt.imshow(sample1_img)\nplt.title('Without Filter')\n\nplt.subplot(1, 3, 2)\n\nsample2_img_bw = cv2.cvtColor(sample2_img, cv2.COLOR_BGR2GRAY)\n\nclahe = cv2.createCLAHE(clipLimit=5)\nsample2_img = clahe.apply(sample2_img_bw)\n\nplt.imshow(sample2_img)\nplt.title('With Filter')\n\nplt.subplot(1, 3, 3)\n\n_, ordinary_img = cv2.threshold(sample2_img_bw, 155, 255, cv2.THRESH_BINARY)\n\nplt.imshow(ordinary_img)\n\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:07.629044Z","iopub.execute_input":"2023-12-03T03:18:07.629315Z","iopub.status.idle":"2023-12-03T03:18:08.934838Z","shell.execute_reply.started":"2023-12-03T03:18:07.62929Z","shell.execute_reply":"2023-12-03T03:18:08.933942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the contract of the image is being enchanced, as I think this filter would reduce the noise in the image","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(28, 10))\n\nplt.subplot(1, 2, 1)\nplt.hist(sample1_img.ravel())\n\nplt.subplot(1, 2, 2)\nplt.hist(sample2_img.ravel())\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:08.936032Z","iopub.execute_input":"2023-12-03T03:18:08.936329Z","iopub.status.idle":"2023-12-03T03:18:09.548874Z","shell.execute_reply.started":"2023-12-03T03:18:08.936297Z","shell.execute_reply":"2023-12-03T03:18:09.547982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6) CNN Model","metadata":{}},{"cell_type":"code","source":"class VGG16(nn.Module):\n    def __init__(self):\n        super(VGG16, self).__init__()\n\n        self.model = models.vgg16(weights='DEFAULT')\n        self.features = self.model.features\n        \n        self.features1 = self.features[:6]\n        self.features2 = self.features[6:10]\n        self.features3 = self.features[10:17]\n        self.features4 = self.features[17:30]\n\n\n    def forward(self, images):\n        with torch.no_grad():\n            f1 = self.features1(images)\n            f2 = self.features2(f1)\n            f3 = self.features3(f2)\n            f4 = self.features4(f3)\n\n        return f2, f3, f4","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:12.606335Z","iopub.execute_input":"2023-12-03T03:18:12.606693Z","iopub.status.idle":"2023-12-03T03:18:12.614082Z","shell.execute_reply.started":"2023-12-03T03:18:12.606662Z","shell.execute_reply":"2023-12-03T03:18:12.61309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super(Classifier, self).__init__()\n\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(256, 128, kernel_size=(1, 1), padding=0),\n            nn.ReLU(),\n        )\n\n        self.fc = nn.Linear(128, self.num_classes)\n        \n    def forward(self, f2, f3, f4):\n        features = self.conv1(f4)\n        features = self.conv2(features)\n        \n        features = F.interpolate(features, scale_factor=2, mode='bilinear') + f3\n        \n        features = self.conv3(features)\n        \n        features = F.interpolate(features, scale_factor=2, mode='bilinear') + f2\n        \n        fc = torch.flatten(F.adaptive_avg_pool2d(features, 1), start_dim=1)\n        scores = F.softmax(self.fc(fc), dim=1)\n        \n        # Compute CAMS\n        with torch.no_grad():\n            batch, composition, height, width = features.shape\n            \n            features = features.permute(0, 1, 2, 3)\n            features = features.view(batch, composition, height * width)\n            \n            w = self.fc.weight.data.unsqueeze(0).repeat(batch, 1, 1)\n            \n            cams = torch.matmul(w, features)\n            \n            cams = self._normalize_cams(cams)\n            cams = cams.view(batch, self.num_classes, height, width)\n\n        return fc, cams\n    \n    def _normalize_cams(self, cam):\n        cam = cam - cam.min(dim=-1)[0].unsqueeze(-1)\n        cam = cam / cam.max(dim=-1)[0].unsqueeze(-1)\n\n        return cam","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:13.430143Z","iopub.execute_input":"2023-12-03T03:18:13.430514Z","iopub.status.idle":"2023-12-03T03:18:13.443794Z","shell.execute_reply.started":"2023-12-03T03:18:13.430481Z","shell.execute_reply":"2023-12-03T03:18:13.442943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1) Functions","metadata":{}},{"cell_type":"code","source":"def draw_heatmap(image, vgg16, model):\n    model.eval()\n    vgg16.eval()\n    \n    f2, f3, f4 = vgg16(image.unsqueeze(0))\n    _, cams = model(f2, f3, f4)\n    \n    plt.matshow(heatmap.squeeze().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:14.266184Z","iopub.execute_input":"2023-12-03T03:18:14.266564Z","iopub.status.idle":"2023-12-03T03:18:14.272492Z","shell.execute_reply.started":"2023-12-03T03:18:14.266534Z","shell.execute_reply":"2023-12-03T03:18:14.271445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2) Training","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    pin_memory=False,\n    drop_last=False,\n    shuffle=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:15.667303Z","iopub.execute_input":"2023-12-03T03:18:15.667639Z","iopub.status.idle":"2023-12-03T03:18:15.672361Z","shell.execute_reply.started":"2023-12-03T03:18:15.667611Z","shell.execute_reply":"2023-12-03T03:18:15.671505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16_model = VGG16().to(DEVICE)\nclassifier_model = Classifier(num_classes=2).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:16.284876Z","iopub.execute_input":"2023-12-03T03:18:16.285202Z","iopub.status.idle":"2023-12-03T03:18:31.680888Z","shell.execute_reply.started":"2023-12-03T03:18:16.285177Z","shell.execute_reply":"2023-12-03T03:18:31.680055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(loader, vgg16, model, optimizer, loss_fn, epoch, lr_scheduler=None):\n    model.train()\n    vgg16.eval()\n    \n    losses = []\n    \n    for images, labels in loader:\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        f2, f3, f4 = vgg16(images)\n        scores, _ = model(f2, f3, f4)\n\n        loss = loss_fn(scores, labels)\n        \n        losses.append(loss.item())\n        \n        optimizer.zero_grad()\n        loss.backward()        \n        optimizer.step()\n        \n        if lr_scheduler is not None:\n            lr_scheduler.step()\n        \n        print(f'-> Batch Loss[{loss.item()}]')\n        \n    print(f'|| Epoch[{epoch}]: Loss[{np.mean(losses)}]')\n    \n    return np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:31.682396Z","iopub.execute_input":"2023-12-03T03:18:31.682681Z","iopub.status.idle":"2023-12-03T03:18:31.690213Z","shell.execute_reply.started":"2023-12-03T03:18:31.682655Z","shell.execute_reply":"2023-12-03T03:18:31.689193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_accuracy(loader, vgg16, model, epoch):\n    model.eval()\n    \n    accuracies = []\n    \n    for images, labels in loader:\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        f2, f3, f4 = vgg16(images)\n        predicted, _ = model(f2, f3, f4)\n        \n        accuracies.append((torch.mean((predicted.argmax(1) == labels).float()) * 100).item())\n        \n    print(f'=> Epoch[{epoch}]: Accuracy[{np.mean(accuracies)}]')\n    \n    return np.mean(accuracies)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:31.691208Z","iopub.execute_input":"2023-12-03T03:18:31.691468Z","iopub.status.idle":"2023-12-03T03:18:31.699948Z","shell.execute_reply.started":"2023-12-03T03:18:31.691445Z","shell.execute_reply":"2023-12-03T03:18:31.699173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(classifier_model.parameters(), lr=LEARNING_RATE)\nloss_fn = nn.CrossEntropyLoss()\nlr_scheduler = scheduler.MultiStepLR(optimizer=optimizer, milestones=LR_DECAY_EPOCH, gamma=LR_DECAY)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:43.637974Z","iopub.execute_input":"2023-12-03T03:18:43.638952Z","iopub.status.idle":"2023-12-03T03:18:43.645163Z","shell.execute_reply.started":"2023-12-03T03:18:43.638907Z","shell.execute_reply":"2023-12-03T03:18:43.643887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"starting_epoch = 1","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:44.96867Z","iopub.execute_input":"2023-12-03T03:18:44.969571Z","iopub.status.idle":"2023-12-03T03:18:44.974976Z","shell.execute_reply.started":"2023-12-03T03:18:44.969537Z","shell.execute_reply":"2023-12-03T03:18:44.973823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(starting_epoch, EPOCHS):\n    train_epoch(\n        train_loader,\n        vgg16_model,\n        classifier_model,\n        optimizer,\n        loss_fn,\n        epoch,\n        lr_scheduler\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:18:52.972411Z","iopub.execute_input":"2023-12-03T03:18:52.973129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_heatmap(train_dataset[5][0], vgg16_model, classifier_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T03:17:08.379682Z","iopub.status.idle":"2023-12-03T03:17:08.380021Z","shell.execute_reply.started":"2023-12-03T03:17:08.379863Z","shell.execute_reply":"2023-12-03T03:17:08.379879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
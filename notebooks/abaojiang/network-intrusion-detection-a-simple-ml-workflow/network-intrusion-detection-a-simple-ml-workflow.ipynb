{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Network Intrusion Detection\nThis notebook is a simple **ML workflow** of using classifiers to solve **intrusion detection**, which can be modeled as a **binary classification** problem. That is, the goal is to determine whether the network traffic is **an abnormal behavior or not**.\n\n## Dataset\n* `Train_data.csv`: Training data containing 41 features and 1 column of groundtruths.\n* `Test_data.csv`: Testing data containing 41 features and no labels.\n\n<div class=\"alert alert-blocks alert-warning\" style=\"font-size: 15px;\">\n   <p>In order to test the <b>generalization ability</b> of the classifers, I'll use <code>Train_data.csv</code> only. That is, <code>Test_data.csv</code> won't be used.</p>\n</div>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Import packages\nimport os \nimport warnings\nimport gc\n\nimport pandas as pd \nimport numpy as np \nfrom plotly.subplots import make_subplots\nimport plotly.express as px \nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\n\n# Configuration\nwarnings.simplefilter('ignore')\npd.set_option('max_columns', 50)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:11:17.120564Z","iopub.execute_input":"2021-06-14T09:11:17.120846Z","iopub.status.idle":"2021-06-14T09:11:17.126192Z","shell.execute_reply.started":"2021-06-14T09:11:17.120821Z","shell.execute_reply":"2021-06-14T09:11:17.125078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variable definitions\nDATA_PATH = \"../input/network-intrusion-detection\"","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:13:39.878499Z","iopub.execute_input":"2021-06-14T09:13:39.878811Z","iopub.status.idle":"2021-06-14T09:13:39.881929Z","shell.execute_reply.started":"2021-06-14T09:13:39.878785Z","shell.execute_reply":"2021-06-14T09:13:39.880977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility functions \ndef describe(df, stats):\n    '''Describe the basic information of the raw dataframe.\n    \n    Parameters:\n        df: pd.DataFrame, raw dataframe to be analyzed\n        stats: boolean, whether to get descriptive statistics \n    \n    Return:\n        None\n    '''\n    df_ = df.copy(deep=True)   # Copy of the raw dataframe\n    n_features = df_.shape[1]\n    if n_features > pd.get_option(\"max_columns\"):\n        # If the feature (column) number is greater than max number of columns displayed\n        warnings.warn(\"Please reset the display-related options max_columns \\\n                      to enable the complete display.\", \n                      UserWarning) \n    print(\"=====Basic information=====\")\n    display(df_.info())\n    get_nan_ratios(df_)\n    if stats:\n        print(\"=====Description=====\")\n        numeric_col_num = df_.select_dtypes(include=np.number).shape[1]   # Number of cols in numeric type\n        if numeric_col_num != 0:\n            display(df_.describe())\n        else:\n            print(\"There's no description of numeric data to display!\")\n    del df_\n    gc.collect()\n\ndef get_nan_ratios(df):\n    '''Get NaN ratios of columns with NaN values.\n    \n    Parameters:\n        df: pd.DataFrame, raw dataframe to be analyzed\n        \n    Return:\n        None\n    '''\n    df_ = df.copy()   # Copy of the raw dataframe\n    nan_ratios = df_.isnull().sum() / df_.shape[0] * 100   # Ratios of value nan in each column\n    nan_ratios = pd.DataFrame([df_.columns, nan_ratios]).T   # Take transpose \n    nan_ratios.columns = [\"Columns\", \"NaN ratios\"]\n    nan_ratios = nan_ratios[nan_ratios[\"NaN ratios\"] != 0.0]\n    print(\"=====NaN ratios of columns with NaN values=====\")\n    if len(nan_ratios) == 0:\n        print(\"There isn't any NaN value in the dataset!\")\n    else:\n        display(nan_ratios)\n    del df_\n    gc.collect() ","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:13:40.144824Z","iopub.execute_input":"2021-06-14T09:13:40.145171Z","iopub.status.idle":"2021-06-14T09:13:40.152724Z","shell.execute_reply.started":"2021-06-14T09:13:40.145138Z","shell.execute_reply":"2021-06-14T09:13:40.151833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Split\nBased on the reason I mentioned above in orange block, I'll first do **train-test splitting** on data in `Train_data.csv` to prevent one specific type of data leakage, **Train-Test Contamination**. That is, I first split a **hold-out dataset** to be the **final testing data**. Furthermore, **KFold cross validation** will be implemented on the training dataset to help us determine whether the model has **generalization ability**.\n<div class=\"alert alert-blocks alert-info\" style=\"font-size: 15px;\">\n    <h4>Reference</h4>\n    <p><a href=\"https://medium.com/@eijaz/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f\" style=\"color: orange;\">Hold-out vs. Cross-validation in Machine Learning</a></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Split the training set and testing set (hold-out).\ndf = pd.read_csv(os.path.join(DATA_PATH, \"Train_data.csv\"))\nX, y = df.iloc[:, :-1], df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\nprint(f\"Shape of X_train: {X_train.shape}\\nShape of X_test: {X_test.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\\nShape of y_test: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:13:40.662605Z","iopub.execute_input":"2021-06-14T09:13:40.662927Z","iopub.status.idle":"2021-06-14T09:13:40.824014Z","shell.execute_reply.started":"2021-06-14T09:13:40.662898Z","shell.execute_reply":"2021-06-14T09:13:40.823078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis\nFollowing is the exploratory data analysis of the training set, I'll analyze and display some characteristics of features and also the predicting target. Let's go!\n\n## *2.1 Basic Description*\nBasic information about the training data is shown, including:\n* Data appearance\n* Data types of features\n* NaN ratio of each features\n* Stats of features","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:13:54.664977Z","iopub.execute_input":"2021-06-14T09:13:54.665313Z","iopub.status.idle":"2021-06-14T09:13:54.671005Z","shell.execute_reply.started":"2021-06-14T09:13:54.665285Z","shell.execute_reply":"2021-06-14T09:13:54.669854Z"}}},{"cell_type":"code","source":"print(\"=====DataFrame: X_train=====\")\ndisplay(X_train.head())\ndescribe(X_train, stats=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:14:03.631973Z","iopub.execute_input":"2021-06-14T09:14:03.632317Z","iopub.status.idle":"2021-06-14T09:14:04.042009Z","shell.execute_reply.started":"2021-06-14T09:14:03.632288Z","shell.execute_reply":"2021-06-14T09:14:04.041075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *2.2 Categorical Features* \nThere are three categorical features in the dataset, including `protocol type`, `service` and `flag`. Let's dig deeper into the these features!\n### 2.2.1 Unique Values","metadata":{}},{"cell_type":"code","source":"cat_features = ['protocol_type', 'service', 'flag']\nfor f in cat_features:\n    print(f\"=====Unique values of {f}=====\")\n    unique_vals = X_train[f].unique()\n    print(unique_vals)\n    print(f\"Number of unique values: {len(unique_vals)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:14:17.898272Z","iopub.execute_input":"2021-06-14T09:14:17.898603Z","iopub.status.idle":"2021-06-14T09:14:17.909294Z","shell.execute_reply.started":"2021-06-14T09:14:17.898576Z","shell.execute_reply":"2021-06-14T09:14:17.908181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 Ratio of Each Unique Value","metadata":{}},{"cell_type":"code","source":"for f in cat_features:\n    val_counts = X_train[f].value_counts()\n    fig = go.Figure()\n    fig.add_trace(go.Pie(\n        labels=val_counts.index,\n        values=val_counts\n    ))\n    fig.update_traces(textposition='inside') \n    fig.update_layout(\n        title=f\"Pie Chart of {f}\",\n        uniformtext_minsize=12, \n        uniformtext_mode='hide'\n    )\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:14:30.865643Z","iopub.execute_input":"2021-06-14T09:14:30.865925Z","iopub.status.idle":"2021-06-14T09:14:31.005482Z","shell.execute_reply.started":"2021-06-14T09:14:30.865901Z","shell.execute_reply":"2021-06-14T09:14:31.004674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *2.3 Numeric Features*\n### 2.3.1 Univariate Distribution\nFirst, the univariate histograms of the features are plotted to give us preliminary understanding about the distributions.","metadata":{}},{"cell_type":"code","source":"numeric_features = [col for col in X_train.columns if col not in cat_features]\n\nfig = make_subplots(rows=10, cols=4, subplot_titles=numeric_features)\nfor i in range(1, 11):\n    for j in range(1, 5):\n        feature_idx = 4 * (i-1) + (j-1)\n        if feature_idx == len(numeric_features):\n            break\n        feature = numeric_features[feature_idx]\n        feature_series = X_train[feature]\n        sub_fig = go.Histogram(x=feature_series, name=feature)\n        fig.add_trace(\n            sub_fig,\n            row=i,\n            col=j\n        )\n        \nfig.update_layout(height=1200, title_text=\"Univariate Distribution of Numeric Features\") \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:14:45.552943Z","iopub.execute_input":"2021-06-14T09:14:45.553294Z","iopub.status.idle":"2021-06-14T09:14:46.313836Z","shell.execute_reply.started":"2021-06-14T09:14:45.553261Z","shell.execute_reply":"2021-06-14T09:14:46.311702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.2 Statistical Dispersion and Variation\nBased on the observations of distributions above, some features behave as if they're constant features. Hence, I'll show two measurements to gain a better understanding about the features:\n* The proportion of the value with **the most count** in each feature - from the perspective of **value count**\n* The **variance** of each feature - from the perspective of **value**","metadata":{}},{"cell_type":"code","source":"n_samples = X_train.shape[0]   # Total number of samples\n\n# Get the proportion of the value with the most count in each feature\nmax_proportions = pd.DataFrame()\nfor f in numeric_features:\n    feature_series = X_train[f]\n    max_proportion = np.max(feature_series.value_counts()) / n_samples\n    max_proportions[f] = [max_proportion]\nmax_proportions.index = [\"Max Proportion\"]\n\n# Get the variance of each feature \nvars = pd.DataFrame(X_train.var()).T\nvars.index = [\"Variance\"]\n\ndisp_and_var = max_proportions.append(vars)\nprint(\"=====Statistical dispersion and variation=====\")\ndisplay(disp_and_var)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:15:18.376391Z","iopub.execute_input":"2021-06-14T09:15:18.376694Z","iopub.status.idle":"2021-06-14T09:15:18.456647Z","shell.execute_reply.started":"2021-06-14T09:15:18.376667Z","shell.execute_reply":"2021-06-14T09:15:18.455794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.3 Bivariate Distribution\nAfter simple univariate analysis, let's move on to the bivariate part. First, I'll filter out the features with **high max proportion** or **low variance** based on the pre-defined thresholds to simplify the analysis. The thresholds are defined as follows:\n* Max proportion: 0.99\n* Variance: 0.001\n\n<div class=\"alert alert-blocks alert-warning\">\n    <h4>Notice</h4>\n    <p>In order to make the visualizations more clear, I'll just show some randomly picked joint distributions. And, others can be shown in similar way.</p>\n    <p>Furthermore, the <b>groundtruths</b> are added in to help us find whether there is any <b>clustering</b> property.</p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Filter out features with high \"max proportion\" or low \"variance\"\ndisp_and_var_T = disp_and_var.T   # Take the transpose\nfeatures_remained = disp_and_var_T[(disp_and_var_T['Max Proportion'] < 0.99) & disp_and_var_T['Variance'] > 0.001].index.tolist()\nX_train = X_train.loc[:, features_remained]\nprint(f\"After filtering, there are {len(features_remained)} numeric features remained.\")\n\n# Plot bivariate distributions \nfeatures_picked = features_remained[-5:]\ndf_train = X_train.loc[:, features_picked]\ndf_train['gt'] = y_train\nfig = px.scatter_matrix(df_train, \n                        dimensions=features_picked,\n                        color=\"gt\", \n                        symbol=\"gt\")\nfig.update_traces(diagonal_visible=False)\nfig.update_layout(height=1200, title_text=\"Bivariate Distribution of Numeric Feature Pairs (Randomly Picked)\") \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:15:35.101216Z","iopub.execute_input":"2021-06-14T09:15:35.101522Z","iopub.status.idle":"2021-06-14T09:15:35.958488Z","shell.execute_reply.started":"2021-06-14T09:15:35.101497Z","shell.execute_reply":"2021-06-14T09:15:35.957402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *2.4 Groundtruth Distribution*\nIn the final part of the simple EDA, let's take a look at the distribution of the groundtruths to see if there's a problem of **Class Imbalance**.\n<div class=\"alert alert-blocks alert-info\" style=\"font-size: 15px;\">\n    <h4>Reference</h4>\n    <p><a href=\"https://machinelearningmastery.com/what-is-imbalanced-classification/\" style=\"color: orange;\">A Gentle Introduction to Imbalanced Classification</a></p>\n</div>","metadata":{}},{"cell_type":"code","source":"class_count = pd.DataFrame(y_train).value_counts()\nfig = go.Figure()\nfig.add_trace(go.Pie(\n    labels=class_count.index,\n    values=class_count\n))\nfig.update_traces(textposition='inside') \nfig.update_layout(\n    title=f\"Pie Chart of Groundtruths\",\n    uniformtext_minsize=12, \n    uniformtext_mode='hide'\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:15:54.141164Z","iopub.execute_input":"2021-06-14T09:15:54.141607Z","iopub.status.idle":"2021-06-14T09:15:54.159996Z","shell.execute_reply.started":"2021-06-14T09:15:54.141578Z","shell.execute_reply":"2021-06-14T09:15:54.15918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. KFold Cross Validation with RandomForestClassifier (RFC)\nKFold cross validation is used to measure the **generalization ability** of the model as I mentioned above. In this part, I'll take RandomForestClassifier as the baseline model.","metadata":{}},{"cell_type":"code","source":"# Encode the labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(y_train)\n\n# Evaluate the model performance using KFold CV\nkf = KFold(5, shuffle=True, random_state=42)\nmodels = []   # Trained model record\nfi = []   # Feature importance record\nval_metrics = []   # Evaluation metrics record\nfold = 0\n\nfor train_idx, val_idx in kf.split(X_train):\n    print(f\"=====Evaluation of fold{fold} starts=====\")\n    # Prepare the training and validation sets\n    X_train_, X_val = X_train.iloc[train_idx, :], X_train.iloc[val_idx, :]\n    y_train_, y_val = y_train[train_idx], y_train[val_idx]\n    \n    # Train the classifier (rfc)\n    rfc = RandomForestClassifier(n_estimators=500)\n    rfc.fit(X_train_, y_train_)\n    models.append(rfc)    # Record the trained model\n    fi.append(rfc.feature_importances_)   # Record the feature importance\n    \n    # Predict and evaluate the performance\n    y_val_pred = rfc.predict(X_val)\n    p_r_f1_mac = list(precision_recall_fscore_support(y_val, y_val_pred, average='macro')[:3])\n    p_r_f1_mic = list(precision_recall_fscore_support(y_val, y_val_pred, average='micro')[:3])\n    p_r_f1_wei = list(precision_recall_fscore_support(y_val, y_val_pred, average='weighted')[:3])\n    val_metrics.append([p_r_f1_mac, p_r_f1_mic, p_r_f1_wei])   # Concatenate the evaluation metrics and record\n    print(f\"=====Classification Report=====\\n{classification_report(y_val, y_val_pred)}\")\n    \n    print(f\"=====Evaluation of fold{fold} finishes=====\\n\")\n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:16:13.910555Z","iopub.execute_input":"2021-06-14T09:16:13.910937Z","iopub.status.idle":"2021-06-14T09:16:41.084847Z","shell.execute_reply.started":"2021-06-14T09:16:13.910902Z","shell.execute_reply":"2021-06-14T09:16:41.084058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summarize the avarage performance in KFold CV\navg_metrics = np.mean(val_metrics, axis=0)\nprint(\"=====Average evaluatin metrics over 5 folds=====\")\nfor i, method in enumerate(['Macro', 'Micro', 'Weighted']):\n    print(f\"=====Metrics {method}=====\")\n    print(f\"Precision = {avg_metrics[i][0]} | Recall = {avg_metrics[i][1]} | F1-score = {avg_metrics[i][2]}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:16:41.086997Z","iopub.execute_input":"2021-06-14T09:16:41.087322Z","iopub.status.idle":"2021-06-14T09:16:41.092438Z","shell.execute_reply.started":"2021-06-14T09:16:41.087293Z","shell.execute_reply":"2021-06-14T09:16:41.091694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Feature Importance  \nAfter training the model, we can observe the **feature importance** of each feature to gain a better understanding about which features dominate the decision making process of the random forest.","metadata":{}},{"cell_type":"code","source":"# Sort feature importance\navg_fi = np.mean(fi, axis=0)   # Calculate average feature importance over 5 folds\nfi_dict = {}\nfor feature, feature_importance in zip(features_remained, avg_fi):\n    fi_dict[feature] = feature_importance\nfi_dict = dict(sorted(fi_dict.items(), key=lambda item: item[1], reverse=True))\n\n# Plot feature importance\nfig = go.Figure([go.Bar(x=list(fi_dict.keys()), y=list(fi_dict.values()))])\nfig.update_layout(title=\"Feature Importance\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:16:47.881549Z","iopub.execute_input":"2021-06-14T09:16:47.881986Z","iopub.status.idle":"2021-06-14T09:16:47.892786Z","shell.execute_reply.started":"2021-06-14T09:16:47.881958Z","shell.execute_reply":"2021-06-14T09:16:47.891727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Evaluation on Unseen Data (Testing Set)\nIn the end, I'll evaluate the model performance on the unseen data (i.e. testing set) to see how great the classier could perform. Moreover, the technique, **bagging**, is also implemented. In other words, the **majority voting mechanism** is used to make the prediction more stable.\n<div class=\"alert alert-blocks alert-info\" style=\"font-size: 15px;\">\n    <h4>Reference</h4>\n    <p><a href=\"https://stackabuse.com/ensemble-voting-classification-in-python-with-scikit-learn\" style=\"color: orange;\">Ensemble/Voting Classification in Python with Scikit-Learn</a></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Process the data to meet the model input \nX_test = X_test.loc[:, features_remained]\ny_test = label_encoder.transform(y_test)\n\n# Do inference using trained model from each fold \ny_test_preds = []\nfor rfc in models:\n    y_test_pred = rfc.predict(X_test)\n    y_test_preds.append(y_test_pred)\n\n# Take majority voting\ny_test_pred_voted = np.where(\n    np.mean(y_test_preds, axis=0) >= 0.5, \n    1, \n    0\n)\n\n# Summarize the performance evaluated on testing set\nprint(\"=====Evaluation metrics on testing set=====\")\nfor i, method in enumerate(['Macro', 'Micro', 'Weighted']):\n    print(f\"=====Metrics {method}=====\")\n    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_test_pred_voted, average=method.lower())\n    print(f\"Precision = {precision} | Recall = {recall} | F1-score = {f1_score}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:17:01.493929Z","iopub.execute_input":"2021-06-14T09:17:01.494263Z","iopub.status.idle":"2021-06-14T09:17:02.754487Z","shell.execute_reply.started":"2021-06-14T09:17:01.494235Z","shell.execute_reply":"2021-06-14T09:17:02.75352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Future Work\nThis is just a simple ML workflow from **exploratory data analysis** to final **model evaluation**. There are still many things we can try, which are listed as follows:\n* Do more **exploratory data analysis** to observe the feature interactions.\n* Add in **categorical features** to be the predictors.\n* Implement advanced **feature selection pipeline** to select relevant features.\n* Try to use different **classifiers** to do the model comparison.\n\n<div class=\"alert alert-blocks alert-info\" style=\"font-size: 15px; text-align: center;\">\n    <h4>That's all! Hope this would help!</h4>\n    <h4>Thanks for your attention!</h4>\n</div>","metadata":{}}]}
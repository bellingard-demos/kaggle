{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7022391,"sourceType":"datasetVersion","datasetId":4018622},{"sourceId":7078668,"sourceType":"datasetVersion","datasetId":4072499},{"sourceId":7079043,"sourceType":"datasetVersion","datasetId":4045165}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport torch\nimport pickle\nimport re\nimport numpy as np\nimport tqdm.auto as tqdm\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom transformers import AutoProcessor, BlipForConditionalGeneration\n\n# DO NOT MODIFY THIS LINE\ntorch.manual_seed(217) # seed to ensure consistent random split of data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T05:10:33.271618Z","iopub.execute_input":"2023-11-29T05:10:33.272078Z","iopub.status.idle":"2023-11-29T05:10:58.545277Z","shell.execute_reply.started":"2023-11-29T05:10:33.272038Z","shell.execute_reply":"2023-11-29T05:10:58.544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"def plot_skeleton_with_im(im, keypoints, connect=True):\n    connections = [(0, 1), (1, 2), (3, 4), (4, 5), (10, 11), (11, 12),\n                        (13, 14), (14, 15), (2, 6), (3, 6), (6, 7), (7, 8),\n                        (8, 9), (12, 7), (13, 7), (8, 12), (8, 13)]\n    \n    plt.figure(figsize=(8, 8))  # Adjust the figure size as needed\n    plt.imshow(im)\n    plt.scatter(*zip(*keypoints), s=50, c='b', marker='o')  # Plot keypoints as blue circles\n\n    for i, (x, y) in enumerate(keypoints):\n        plt.text(x, y, str(i), fontsize=12, ha='center', va='bottom', color='b')\n    \n    if connect:\n        for connection in connections:\n            point1, point2 = connection\n            x_values = [keypoints[point1][0], keypoints[point2][0]]\n            y_values = [keypoints[point1][1], keypoints[point2][1]]\n            plt.plot(x_values, y_values, 'r')\n    \n    plt.show()\n\ndef plot_skeleton(keypoints):\n    connections = [(0, 1), (1, 2), (3, 4), (4, 5), (10, 11), (11, 12),\n                        (13, 14), (14, 15), (2, 6), (3, 6), (6, 7), (7, 8),\n                        (8, 9), (12, 7), (13, 7), (8, 12), (8, 13)]\n    \n    plt.figure(figsize=(8, 8))  # Adjust the figure size as needed\n    plt.scatter(*zip(*keypoints), s=50, c='b', marker='o')  # Plot keypoints as blue circles\n\n    for i, (x, y) in enumerate(keypoints):\n        plt.text(x, y, str(i), fontsize=12, ha='center', va='bottom', color='b')\n\n    for connection in connections:\n        point1, point2 = connection\n        x_values = [keypoints[point1][0], keypoints[point2][0]]\n        y_values = [keypoints[point1][1], keypoints[point2][1]]\n        plt.plot(x_values, y_values, 'r')\n    \n    plt.gca().invert_yaxis()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_validation_accuracy(model, val_dataloader, threshold=16):\n    model.eval()\n    correct = 0\n    total = 0\n    total_err = 0\n    missing_label = 0\n    val_loss = 0\n    num_batch = 0\n    \n    progress_bar = tqdm.tqdm(val_dataloader)\n    progress_bar.set_description(f\"Running validation: \")\n    for idx, batch in enumerate(progress_bar):\n        num_batch += 1\n        input_ids = batch.pop(\"input_ids\").to(device)\n        pixel_values = batch.pop(\"pixel_values\").to(device)\n        attention_mask = batch.pop(\"attention_mask\").to(device)\n\n        text = processor.batch_decode(input_ids, skip_special_tokens=True)\n        \n        with torch.no_grad():\n            generated_ids = model.generate(pixel_values=pixel_values, max_length=300)\n            fw_outputs = model(input_ids=input_ids,\n                                pixel_values=pixel_values,\n                                labels=input_ids,\n                                attention_mask=attention_mask)\n\n        val_loss += fw_outputs.loss.item()\n            \n        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n        total += len(generated_caption)\n        \n        for i in range(len(generated_caption)):\n            caption = generated_caption[i]\n            prediction = np.array(re.findall(r'(\\d+), (\\d+)', caption), dtype=float)\n            \n            if prediction.shape[0] != 16:\n                missing_label += 1\n                continue\n            \n            t = text[i]\n            truth = np.array(re.findall(r'(\\d+), (\\d+)', t), dtype=float)\n\n            err = np.abs(prediction - truth)\n            avg_err = np.mean(err)\n            total_err += avg_err\n            \n            if avg_err < threshold:\n                correct += 1\n\n    print(\"Validation accuracy:\", correct / total)\n    print(\"Validation loss:\", val_loss / num_batch)\n    print(\"Total:\", total)\n    print(\"Correct:\", correct)\n    print(\"Missing label:\", missing_label)\n    print(\"Average error:\", total_err / (total - missing_label))\n    \n    model.train()\n    \n    return correct / total, val_loss / num_batch, total_err / (total - missing_label)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:59:04.47477Z","iopub.execute_input":"2023-11-29T04:59:04.475223Z","iopub.status.idle":"2023-11-29T04:59:04.492682Z","shell.execute_reply.started":"2023-11-29T04:59:04.475186Z","shell.execute_reply":"2023-11-29T04:59:04.491673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageCaptioningDataset(Dataset):\n    def __init__(self, data_dict, processor):\n        self.processor = processor\n        self.images = [Image.open(f\"/kaggle/input/28231999/processed_data/{name}\") for name in data_dict['img_name']]\n        self.text = [text for text in data_dict['text']]\n    \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        text = self.text[idx]\n        encoding = self.processor(images=image, text=text, padding=\"max_length\", return_tensors=\"pt\")\n        # remove batch dimension\n        encoding = {k:v.squeeze() for k,v in encoding.items()}\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:59:06.098875Z","iopub.execute_input":"2023-11-29T04:59:06.099847Z","iopub.status.idle":"2023-11-29T04:59:06.108313Z","shell.execute_reply.started":"2023-11-29T04:59:06.099811Z","shell.execute_reply":"2023-11-29T04:59:06.106998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the preprocessed MPII dataset","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/28231999/processed_data/processed_data.pkl', 'rb') as file:\n    # Load the data from the file\n    data = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T03:49:04.468585Z","iopub.execute_input":"2023-11-29T03:49:04.469349Z","iopub.status.idle":"2023-11-29T03:49:04.588396Z","shell.execute_reply.started":"2023-11-29T03:49:04.469314Z","shell.execute_reply":"2023-11-29T03:49:04.587247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove bad points from the dataset","metadata":{}},{"cell_type":"code","source":"joint_coords = np.zeros((len(data['text']), 16, 2))\nbad_groups = []\nfor i, joints in enumerate(data['text']):\n    coords = np.array(re.findall(r'(\\d+),(\\d+)', joints), dtype=int)\n    if coords.shape[0] != 16:\n        print(i)\n        bad_groups.append(i)\n    else:\n        joint_coords[i] = coords\n\n# dict_keys(['img_name', 'joint_points', 'text', 'simple_text', 'visibility_text', 'simple_visibility_text', 'normalized_text', 'normalized_simple_text'])\nprocessed_data = {}\nprocessed_data['img_name'] = [value for i, value in enumerate(data['img_name']) if i not in bad_groups]\nprocessed_data['text'] = [value for i, value in enumerate(data['text']) if i not in bad_groups]\nprocessed_data['joint_points'] = [value for i, value in enumerate(data['joint_points']) if i not in bad_groups]\nprocessed_data['simple_text'] = [value for i, value in enumerate(data['simple_text']) if i not in bad_groups]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T03:49:08.301821Z","iopub.execute_input":"2023-11-29T03:49:08.302172Z","iopub.status.idle":"2023-11-29T03:49:08.390282Z","shell.execute_reply.started":"2023-11-29T03:49:08.302145Z","shell.execute_reply":"2023-11-29T03:49:08.389275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize a data point","metadata":{}},{"cell_type":"code","source":"idx = 0\nimage = Image.open(f\"/kaggle/input/28231999/processed_data/{data['img_name'][idx]}\")\nplt.figure(figsize=(8, 8)) \nplt.imshow(image)\nplt.plot(joint_coords[idx][:, 0], joint_coords[idx][:, 1], 'o')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:59:41.724619Z","iopub.execute_input":"2023-11-29T04:59:41.725313Z","iopub.status.idle":"2023-11-29T04:59:42.283631Z","shell.execute_reply.started":"2023-11-29T04:59:41.725278Z","shell.execute_reply":"2023-11-29T04:59:42.282733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the HuggingFace BLIP Captioning model","metadata":{}},{"cell_type":"code","source":"# Rerun this cell to reload model if you want to fine-tune with a different \n# set of parameters\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:59:47.507567Z","iopub.execute_input":"2023-11-29T04:59:47.507915Z","iopub.status.idle":"2023-11-29T05:00:00.599142Z","shell.execute_reply.started":"2023-11-29T04:59:47.507888Z","shell.execute_reply":"2023-11-29T05:00:00.598061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"batch_size = 4\nlr = 2e-5\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n\nlambda1 = lambda epoch: 0.2 if epoch >= 15 else 1.0\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1, verbose=True)\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, min_lr=1e-5, threshold=0.1, patience=5, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:00:13.654117Z","iopub.execute_input":"2023-11-29T05:00:13.654483Z","iopub.status.idle":"2023-11-29T05:00:13.663595Z","shell.execute_reply.started":"2023-11-29T05:00:13.654456Z","shell.execute_reply":"2023-11-29T05:00:13.662577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize dataloaders","metadata":{}},{"cell_type":"code","source":"dataset = ImageCaptioningDataset(processed_data, processor)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\nsplit_sizes = [train_size, val_size]\n\ntrain_dataset, val_dataset = random_split(dataset, split_sizes)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\nval_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n\nrunning_losses = []\nval_accuracies = []\nval_losses = []\navg_errs = []","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:00:18.679798Z","iopub.execute_input":"2023-11-29T05:00:18.680473Z","iopub.status.idle":"2023-11-29T05:00:21.307883Z","shell.execute_reply.started":"2023-11-29T05:00:18.68044Z","shell.execute_reply":"2023-11-29T05:00:21.306878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"num_epoch = 25\n# checkpoints = [1, 15, 23]\n\nprint(\"Number of epochs:\", num_epoch)\nprint(\"Batch size:\", train_dataloader.batch_size)\nprint(\"Learning rate:\", optimizer.param_groups[0]['lr'])\nprint(\"Optimizer:\", optimizer)\n\nprint(\"Checkpoints will be save at epoch:\", checkpoints)\n\nprint(\"Start training on device:\", device)\nmodel.train()\nfor epoch in range(num_epoch):\n    print(\"Epoch:\", epoch)\n    progress_bar = tqdm.tqdm(train_dataloader)\n    for idx, batch in enumerate(progress_bar):\n        \n        input_ids = batch.pop(\"input_ids\").to(device)\n        pixel_values = batch.pop(\"pixel_values\").to(device)\n        attention_mask = batch.pop(\"attention_mask\").to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(input_ids=input_ids,\n                        pixel_values=pixel_values,\n                        labels=input_ids,\n                        attention_mask=attention_mask)\n\n        loss = outputs.loss\n\n        progress_bar.set_description(f\"Loss: {loss.item()}\")\n        running_lzosses.append(loss.item())\n        \n        loss.backward()\n        optimizer.step()\n        \n        # uncomment to initiate validation during training\n#     if (epoch % 4 == 0 and epoch != 0) or epoch == 1:\n#         val_accuracy, val_loss, avg_err = get_validation_accuracy(model, val_dataloader, 25)\n#         val_accuracies.append(val_accuracy)\n#         val_losses.append(val_loss)\n#         avg_errs.append(avg_err)\n        \n        # uncomment to take checkpoints\n#     if epoch in checkpoints:\n#         torch.save(model.state_dict(), '/kaggle/working/model_early.pt'.format(epoch))\n    \n    # uncomment to use lr scheduler\n#     scheduler.step()\n        \nprint(\"Finish trainig\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T06:51:55.562602Z","iopub.execute_input":"2023-11-28T06:51:55.563272Z","iopub.status.idle":"2023-11-28T07:18:21.233457Z","shell.execute_reply.started":"2023-11-28T06:51:55.56324Z","shell.execute_reply":"2023-11-28T07:18:21.23251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot and save training and validation curves","metadata":{}},{"cell_type":"code","source":"plt.plot(running_losses)\nplt.title('Training Loss')\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.savefig('/kaggle/working/training_loss.jpg')\nplt.show()\n\nplt.plot(val_accuracies)\nplt.title('Validation Accuracy')\nplt.xlabel('Number of iterations')\nplt.ylabel('Accuracy')\nplt.savefig('/kaggle/working/validation_accuracy.jpg')\nplt.show()\n\nplt.plot(val_losses)\nplt.title('Validation Loss')\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.savefig('/kaggle/working/validation_loss.jpg')\nplt.show()\n\nplt.plot(avg_errs)\nplt.title('Average Error')\nplt.xlabel('Number of iterations')\nplt.ylabel('Error')\nplt.savefig('/kaggle/working/avg_err.jpg')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T20:35:42.147117Z","iopub.execute_input":"2023-11-27T20:35:42.147867Z","iopub.status.idle":"2023-11-27T20:35:43.511071Z","shell.execute_reply.started":"2023-11-27T20:35:42.14783Z","shell.execute_reply":"2023-11-27T20:35:43.510127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/training_loss.pkl', 'wb') as file:\n    pickle.dump(running_losses, file)\n\nwith open('/kaggle/working/validation_accuracy.pkl', 'wb') as file:\n    pickle.dump(val_accuracies, file)\n\nwith open('/kaggle/working/validation_loss.pkl', 'wb') as file:\n    pickle.dump(val_losses, file)\n\nwith open('/kaggle/working/avg_err.pkl', 'wb') as file:\n    pickle.dump(avg_errs, file)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:46:43.301305Z","iopub.execute_input":"2023-11-27T22:46:43.302101Z","iopub.status.idle":"2023-11-27T22:46:43.310835Z","shell.execute_reply.started":"2023-11-27T22:46:43.302047Z","shell.execute_reply":"2023-11-27T22:46:43.309945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save a trained model\ntorch.save(model.state_dict(), '/kaggle/working/model_epoch_final.pt')","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:46:43.31268Z","iopub.execute_input":"2023-11-27T22:46:43.313044Z","iopub.status.idle":"2023-11-27T22:46:44.621369Z","shell.execute_reply.started":"2023-11-27T22:46:43.31299Z","shell.execute_reply":"2023-11-27T22:46:44.620342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load a trained model\nmodel.load_state_dict(torch.load('/kaggle/input/mpiistartmodel/start_model/late.pt'))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:17:27.001427Z","iopub.execute_input":"2023-11-29T04:17:27.00244Z","iopub.status.idle":"2023-11-29T04:17:27.599481Z","shell.execute_reply.started":"2023-11-29T04:17:27.002401Z","shell.execute_reply":"2023-11-29T04:17:27.598413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"val_accuracy, val_loss, avg_err = get_validation_accuracy(model, val_dataloader, 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T18:36:46.283699Z","iopub.execute_input":"2023-11-28T18:36:46.284622Z","iopub.status.idle":"2023-11-28T18:39:15.662165Z","shell.execute_reply.started":"2023-11-28T18:36:46.284574Z","shell.execute_reply":"2023-11-28T18:39:15.661235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference on a random sample from the validation set","metadata":{}},{"cell_type":"code","source":"random_val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T07:54:09.641358Z","iopub.execute_input":"2023-11-28T07:54:09.642279Z","iopub.status.idle":"2023-11-28T07:54:09.669632Z","shell.execute_reply.started":"2023-11-28T07:54:09.642245Z","shell.execute_reply":"2023-11-28T07:54:09.66868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nindex = val_dataset.indices[np.random.randint(0, len(val_dataset.indices))]\n\nprint(index)\n\nsample = val_dataset.dataset[index]\n\ninput_ids = sample[\"input_ids\"].to(device)\npixel_values = sample[\"pixel_values\"].to(device)\ntext = processor.decode(input_ids, skip_special_tokens=True)\nprocessed_image = np.transpose(pixel_values.cpu().numpy(), (1, 2, 0))\nimage = val_dataset.dataset.images[index]\nprint(text)\n\nwith torch.no_grad():\n    generated_ids = model.generate(pixel_values=pixel_values.unsqueeze(0), max_length=300)\ngenerated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(generated_caption)\n\n# Convert the pairs to a NumPy array\nprediction = np.array(re.findall(r'(\\d+), (\\d+)', generated_caption), dtype=int)\ntruth = np.array(re.findall(r'(\\d+), (\\d+)', text), dtype=int)\n# plot_skeleton_with_im(processed_image, prediction, False)\nplot_skeleton_with_im(image, prediction, False)\nplot_skeleton_with_im(image, truth, False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:15:56.382649Z","iopub.execute_input":"2023-11-29T04:15:56.383514Z","iopub.status.idle":"2023-11-29T04:15:59.19358Z","shell.execute_reply.started":"2023-11-29T04:15:56.383477Z","shell.execute_reply":"2023-11-29T04:15:59.192674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference on a user input file","metadata":{}},{"cell_type":"code","source":"image = Image.open(f\"FILE_PATH\").resize((384, 384))\n\nmodel.eval()\ninputs = processor(images=image, return_tensors=\"pt\").to(device)\npixel_values = inputs.pixel_values\n# raw_values = transform(image).unsqueeze(0).to(device)\nprocessed_image = np.transpose(pixel_values[0].cpu().numpy(), (1, 2, 0))\nprint()\n\nwith torch.no_grad():\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=300)\ngenerated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(generated_caption)\n\n# Convert the pairs to a NumPy array\nprediction = np.array(re.findall(r'(\\d+), (\\d+)', generated_caption), dtype=int)\nplot_skeleton_with_im(processed_image, prediction, False)\nplot_skeleton_with_im(image, prediction, False)","metadata":{},"execution_count":null,"outputs":[]}]}